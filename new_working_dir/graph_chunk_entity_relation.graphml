<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd"><key id="d9" for="edge" attr.name="file_path" attr.type="string"/>
<key id="d8" for="edge" attr.name="source_id" attr.type="string"/>
<key id="d7" for="edge" attr.name="keywords" attr.type="string"/>
<key id="d6" for="edge" attr.name="description" attr.type="string"/>
<key id="d5" for="edge" attr.name="weight" attr.type="double"/>
<key id="d4" for="node" attr.name="file_path" attr.type="string"/>
<key id="d3" for="node" attr.name="source_id" attr.type="string"/>
<key id="d2" for="node" attr.name="description" attr.type="string"/>
<key id="d1" for="node" attr.name="entity_type" attr.type="string"/>
<key id="d0" for="node" attr.name="entity_id" attr.type="string"/>
<graph edgedefault="undirected"><node id="Takuya Akiba">
  <data key="d0">Takuya Akiba</data>
  <data key="d1">person</data>
  <data key="d2">Takuya Akiba is a researcher involved in the development of evolutionary optimization methods for model merging in machine learning.&lt;SEP&gt;Takuya Akiba is a researcher known for contributions to hyperparameter optimization frameworks and machine learning.&lt;SEP&gt;Takuya Akiba is the initiator of the 'Evolutionary Optimization of Model Merging Recipes' project and has contributed significantly to its design and methodology.&lt;SEP&gt;Takuya Akiba is the initiator of the 'Evolutionary Optimization of Model Merging Recipes' project, contributing significantly to the research.&lt;SEP&gt;Takuya Akiba is one of the authors of a referenced paper on evolutionary optimization of model merging recipes.</data>
  <data key="d3">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b&lt;SEP&gt;chunk-3d9244715091995dd6afa5bd49e8dc52&lt;SEP&gt;chunk-605e26052d578b60e0e6849cb0d635d9&lt;SEP&gt;chunk-72a25142a6a0a013507aa055b73531c4&lt;SEP&gt;chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Makoto Shing">
  <data key="d0">Makoto Shing</data>
  <data key="d1">person</data>
  <data key="d2">Makoto Shing expanded the parameter space model merging to include vision-language models and diffusion models in the research.&lt;SEP&gt;Makoto Shing is a researcher contributing to the study of evolutionary optimization techniques in the context of model merging.&lt;SEP&gt;Makoto Shing is a co-author of a paper discussing the optimization of model merging recipes.</data>
  <data key="d3">chunk-72a25142a6a0a013507aa055b73531c4&lt;SEP&gt;chunk-3d9244715091995dd6afa5bd49e8dc52&lt;SEP&gt;chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Yujin Tang">
  <data key="d0">Yujin Tang</data>
  <data key="d1">person</data>
  <data key="d2">Yujin Tang directed efforts in data flow space model merging and incorporated ideas from neural architecture search.&lt;SEP&gt;Yujin Tang is a researcher focused on evolutionary approaches to combining models for enhanced performance in machine learning.&lt;SEP&gt;Yujin Tang is an author focused on hardware-accelerated neuroevolution.&lt;SEP&gt;Yujin Tang is a contributor to the research on evolutionary optimization of model merging recipes.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f&lt;SEP&gt;chunk-72a25142a6a0a013507aa055b73531c4&lt;SEP&gt;chunk-3d9244715091995dd6afa5bd49e8dc52&lt;SEP&gt;chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Qi Sun">
  <data key="d0">Qi Sun</data>
  <data key="d1">person</data>
  <data key="d2">Qi Sun contributed to the implementation of the parameter space model merging framework and assisted in model evaluation.&lt;SEP&gt;Qi Sun is a researcher participating in the exploration of effective model merging strategies using evolutionary algorithms.&lt;SEP&gt;Qi Sun is an author who has explored the application of transformer layers in computational models.&lt;SEP&gt;Qi Sun is involved in the study of evolutionary optimization of model merging recipes.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f&lt;SEP&gt;chunk-72a25142a6a0a013507aa055b73531c4&lt;SEP&gt;chunk-3d9244715091995dd6afa5bd49e8dc52&lt;SEP&gt;chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="David Ha">
  <data key="d0">David Ha</data>
  <data key="d1">person</data>
  <data key="d2">David Ha is a co-author of the paper on evolutionary optimization of model merging recipes.&lt;SEP&gt;David Ha is a researcher contributing to the field of AI and model merging, particularly through evolutionary methods.&lt;SEP&gt;David Ha is an author known for work in neuroevolution and AI.&lt;SEP&gt;David Ha is an author who has published research on hypernetworks and neural networks, contributing to the field of artificial intelligence.&lt;SEP&gt;David Ha provided guidance and technical insight for the research project, including feedback and writing.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f&lt;SEP&gt;chunk-3d9244715091995dd6afa5bd49e8dc52&lt;SEP&gt;chunk-144f32db3d945848c593404dfe014f4f&lt;SEP&gt;chunk-72a25142a6a0a013507aa055b73531c4&lt;SEP&gt;chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Sakana AI">
  <data key="d0">Sakana AI</data>
  <data key="d1">organization</data>
  <data key="d2">Sakana AI is an organization based in Tokyo, Japan, focused on advancing AI technologies and methodologies.&lt;SEP&gt;Sakana AI is an organization that focuses on developing advanced AI models and provides datasets for research.</data>
  <data key="d3">chunk-72a25142a6a0a013507aa055b73531c4&lt;SEP&gt;chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Tokyo">
  <data key="d0">Tokyo</data>
  <data key="d1">geo</data>
  <data key="d2">Tokyo is mentioned as a location in the context of the document but does not have a direct relevance to the entities identified.&lt;SEP&gt;Tokyo is the capital city of Japan and the location where Sakana AI is based.&lt;SEP&gt;Tokyo is the capital city of Japan, located in the Kanto region, and is known for its modern architecture and historical sites.</data>
  <data key="d3">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a&lt;SEP&gt;chunk-72a25142a6a0a013507aa055b73531c4&lt;SEP&gt;chunk-3ca398087b4d5dd48a0c1fe169e9b7fd</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Evolutionary Model Merge">
  <data key="d0">Evolutionary Model Merge</data>
  <data key="d1">category</data>
  <data key="d2">Evolutionary Model Merge is an approach that evolves models through merging, aiming to create new models capable of handling complex tasks across different domains.&lt;SEP&gt;Evolutionary Model Merge refers to a method developed to automatically discover effective combinations of existing models for creating new foundation models.</data>
  <data key="d3">chunk-72a25142a6a0a013507aa055b73531c4&lt;SEP&gt;chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Japanese LLM">
  <data key="d0">Japanese LLM</data>
  <data key="d1">organization</data>
  <data key="d2">The Japanese LLM is a language model designed for mathematical reasoning and culturally-specific content, achieving state-of-the-art results in benchmarks.&lt;SEP&gt;The Japanese LLM is a language model designed to process and generate text in Japanese, utilized in the model merging experiments.&lt;SEP&gt;The Japanese LLM is a large language model specifically designed to understand and generate Japanese text, showcasing advanced capabilities in various tasks.&lt;SEP&gt;The Japanese LLM refers to a language model specifically designed for understanding and processing the Japanese language.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344&lt;SEP&gt;chunk-72a25142a6a0a013507aa055b73531c4&lt;SEP&gt;chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b&lt;SEP&gt;chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Japanese VLM">
  <data key="d0">Japanese VLM</data>
  <data key="d1">organization</data>
  <data key="d2">The Japanese VLM is a vision-language model that integrates visual and textual information, tailored for understanding Japanese culture-specific content.&lt;SEP&gt;The Japanese VLM is a vision-language model that is culturally aware and performs well on various benchmarks.</data>
  <data key="d3">chunk-72a25142a6a0a013507aa055b73531c4&lt;SEP&gt;chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model Merging">
  <data key="d0">Model Merging</data>
  <data key="d1">category</data>
  <data key="d2">Model Merging is a technique in AI that combines multiple pre-trained models into a single model to leverage their strengths for better performance.&lt;SEP&gt;Model Merging is a technique in machine learning that combines multiple pre-trained models into a single unified model to leverage their strengths across various tasks.&lt;SEP&gt;Model Merging is a technique that combines existing models to create new models, aiming to enhance performance without extensive retraining.&lt;SEP&gt;Model Merging refers to the process of combining multiple models to enhance performance and efficiency in machine learning tasks.&lt;SEP&gt;Model Merging refers to the process of integrating multiple models into a unified architecture, aiming to enhance performance by leveraging strengths of individual models.&lt;SEP&gt;Model merging is a process in machine learning where multiple models are combined to improve performance or efficiency.&lt;SEP&gt;Model merging is a technique in machine learning that combines parameters from multiple models to create a new, optimized model.&lt;SEP&gt;Model Merging is a technique that fuses parameters from multiple fine-tuned LLMs into a single model, enhancing capabilities without additional training costs.&lt;SEP&gt;Model Merging refers to the process of combining parameters from different models to create a unified model capable of performing multiple tasks.&lt;SEP&gt;Model merging is a process that integrates the parameters of multiple models into a unified model, enhancing their combined capabilities.&lt;SEP&gt;Model merging is a technique that integrates parameters from multiple models to create a unified model with enhanced capabilities.&lt;SEP&gt;Model merging is a technique that integrates the parameters of multiple models to create a unified model with enhanced capabilities.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-c8ff785e8102dfde59dbc9309abb6b6a&lt;SEP&gt;chunk-979d654e8dd7d60ff9ad975c80ca86bb&lt;SEP&gt;chunk-689927e59bcb702105a164ca12f7f6b1&lt;SEP&gt;chunk-67c2717229a96dfe2b74a6bceef933f8&lt;SEP&gt;chunk-5fcb50e3e44399e7bec0bc28c47a74f7&lt;SEP&gt;chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7&lt;SEP&gt;chunk-72a25142a6a0a013507aa055b73531c4&lt;SEP&gt;chunk-3df998a85b2dff618feb2e2ce206b62c&lt;SEP&gt;chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Large Language Models">
  <data key="d0">Large Language Models</data>
  <data key="d1">category</data>
  <data key="d2">Large Language Models (LLMs) are advanced AI systems capable of understanding and generating human-like text, often requiring significant computational resources for development.</data>
  <data key="d3">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Open LLM Leaderboard">
  <data key="d0">Open LLM Leaderboard</data>
  <data key="d1">organization</data>
  <data key="d2">The Open LLM Leaderboard is a platform for evaluating and comparing large language models, hosted by HuggingFace.&lt;SEP&gt;The Open LLM Leaderboard is a ranking system that showcases the performance of various large language models, highlighting the effectiveness of model merging.&lt;SEP&gt;The Open LLM Leaderboard is a ranking system that showcases the top language models, many of which are developed through community-driven merging efforts.&lt;SEP&gt;The Open LLM Leaderboard ranks language models based on their performance on various benchmarks, influencing model selection and development.&lt;SEP&gt;The Open LLM Leaderboard is a platform that ranks various language models based on their performance across different benchmarks.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f&lt;SEP&gt;chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b&lt;SEP&gt;chunk-f21dc353d188ca08cba26298157ff09d&lt;SEP&gt;chunk-72a25142a6a0a013507aa055b73531c4&lt;SEP&gt;chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Japanese Math LLM">
  <data key="d0">Japanese Math LLM</data>
  <data key="d1">category</data>
  <data key="d2">The Japanese Math LLM is a specialized large language model designed to understand and perform mathematical reasoning in the Japanese language.</data>
  <data key="d3">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Culturally-Aware Japanese VLM">
  <data key="d0">Culturally-Aware Japanese VLM</data>
  <data key="d1">category</data>
  <data key="d2">The Culturally-Aware Japanese VLM is a vision-language model that integrates visual and textual information, tailored for understanding and generating culturally relevant content in Japanese.</data>
  <data key="d3">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Evolutionary Algorithms">
  <data key="d0">Evolutionary Algorithms</data>
  <data key="d1">category</data>
  <data key="d2">Evolutionary Algorithms are computational methods inspired by natural selection, used to optimize solutions by exploring a vast space of possibilities.&lt;SEP&gt;Evolutionary Algorithms are optimization techniques inspired by natural selection, used to discover effective solutions in complex problem spaces.</data>
  <data key="d3">chunk-72a25142a6a0a013507aa055b73531c4&lt;SEP&gt;chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Foundation Model Development">
  <data key="d0">Foundation Model Development</data>
  <data key="d1">category</data>
  <data key="d2">Foundation Model Development refers to the process of creating robust AI models that serve as the basis for various applications, often requiring innovative approaches like model merging.</data>
  <data key="d3">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Pre-Trained Models">
  <data key="d0">Pre-Trained Models</data>
  <data key="d1">category</data>
  <data key="d2">Pre-Trained Models are machine learning models that have been previously trained on a large dataset and can be fine-tuned for specific tasks.</data>
  <data key="d3">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Transfer Learning">
  <data key="d0">Transfer Learning</data>
  <data key="d1">category</data>
  <data key="d2">Transfer Learning is a machine learning approach where a pre-trained model is further fine-tuned for a new, related task.</data>
  <data key="d3">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Fine-Tuning">
  <data key="d0">Fine-Tuning</data>
  <data key="d1">category</data>
  <data key="d2">Fine-Tuning is the process of training an existing model further on a specialized dataset to improve its performance for a specific task.</data>
  <data key="d3">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Machine Learning Community">
  <data key="d0">Machine Learning Community</data>
  <data key="d1">organization</data>
  <data key="d2">The Machine Learning Community encompasses researchers, developers, and enthusiasts who collaborate on developing and improving machine learning models and techniques.</data>
  <data key="d3">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Open Source Software Development">
  <data key="d0">Open Source Software Development</data>
  <data key="d1">category</data>
  <data key="d2">Open Source Software Development refers to the collaborative approach of creating software where the source code is made freely available for modification and enhancement.</data>
  <data key="d3">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Stable Diffusion">
  <data key="d0">Stable Diffusion</data>
  <data key="d1">organization</data>
  <data key="d2">Stable Diffusion is a generative model that has been widely adopted for creating images and has spawned various specialized fine-tuned versions for different styles.</data>
  <data key="d3">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Task Arithmetic">
  <data key="d0">Task Arithmetic</data>
  <data key="d1">category</data>
  <data key="d2">Task Arithmetic is a merging technique that allows varying merging ratios to improve LLM performance.&lt;SEP&gt;Task Arithmetic is a method for combining language model parameters by adjusting them based on the differences between tasks.&lt;SEP&gt;Task Arithmetic is a method for merging language models by creating task vectors that manipulate model weights to adjust the merged model's behavior.&lt;SEP&gt;Task Arithmetic is a method used in model merging that allows for linear combinations of task-specific models to create a new model.&lt;SEP&gt;Task Arithmetic is a method used in conjunction with M&lt;sup&gt;3&lt;/sup&gt; to improve model performance on specific tasks.&lt;SEP&gt;Task Arithmetic is a method used in model performance evaluation, particularly in the context of model merging.&lt;SEP&gt;Task Arithmetic is a model merging method that utilizes scaling terms to adjust model parameters during merging.&lt;SEP&gt;Task Arithmetic is a model merging technique that combines the parameters of pre-trained and fine-tuned models using a scaling factor and deltas.</data>
  <data key="d3">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-9fbd0c0622e4d5fb84be83781e1c4521&lt;SEP&gt;chunk-689927e59bcb702105a164ca12f7f6b1&lt;SEP&gt;chunk-6c3c559e2fcc2c84ba368ddf5b7db117&lt;SEP&gt;chunk-5fcb50e3e44399e7bec0bc28c47a74f7&lt;SEP&gt;chunk-3df998a85b2dff618feb2e2ce206b62c&lt;SEP&gt;chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Model Soup">
  <data key="d0">Model Soup</data>
  <data key="d1">category</data>
  <data key="d2">Model Soup is a method of merging multiple models by averaging their weights to improve performance in machine learning tasks.</data>
  <data key="d3">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Weighted Model Averaging">
  <data key="d0">Weighted Model Averaging</data>
  <data key="d1">category</data>
  <data key="d2">Weighted Model Averaging is a technique used to combine the weights of multiple models, allowing for a balanced contribution of each model in the merged output.</data>
  <data key="d3">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Latent Diffusion Models">
  <data key="d0">Latent Diffusion Models</data>
  <data key="d1">category</data>
  <data key="d2">Latent Diffusion Models are a type of generative model that operates in a latent space, allowing for efficient image generation and manipulation.</data>
  <data key="d3">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="TIES-Merging">
  <data key="d0">TIES-Merging</data>
  <data key="d1">category</data>
  <data key="d2">TIES-Merging is a merging technique that has shown significant improvements in performance for LLMs.&lt;SEP&gt;TIES-Merging is a method designed to improve model merging performance by addressing parameter interference and information loss during the merging process.&lt;SEP&gt;TIES-Merging is a method used for merging different models in the parameter space to enhance performance.&lt;SEP&gt;TIES-Merging is a method used in the optimization process of models, particularly in merging experiments to enhance performance.&lt;SEP&gt;TIES-Merging is a model merging technique that enhances performance by optimizing configurations for specific tasks in neural networks.&lt;SEP&gt;TIES-Merging is a paper that addresses challenges in merging models and resolving interference in AI systems.&lt;SEP&gt;TIES-Merging is a technique that enhances model merging by analyzing task vectors to optimize the integration of different model parameters.&lt;SEP&gt;TIES-Merging is a method used for merging different model outputs in machine learning, specifically for large language models.&lt;SEP&gt;TIES-Merging is a model merging approach that resolves task conflicts by aligning parameters according to their signs.&lt;SEP&gt;TIES-Merging is a model merging method that facilitates the integration of different models while maintaining their capabilities.&lt;SEP&gt;TIES-Merging is a model merging method that involves scaling terms and a ratio for retaining parameters with the largest-magnitude values.&lt;SEP&gt;TIES-Merging is a technique used to enhance the performance of task-specific language models.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149&lt;SEP&gt;chunk-b650b88d4bebbc45d20b8908b045eff8&lt;SEP&gt;chunk-c8ff785e8102dfde59dbc9309abb6b6a&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-9fbd0c0622e4d5fb84be83781e1c4521&lt;SEP&gt;chunk-689927e59bcb702105a164ca12f7f6b1&lt;SEP&gt;chunk-77432ea4d82a2648008d6d26c50ccdc9&lt;SEP&gt;chunk-67c2717229a96dfe2b74a6bceef933f8&lt;SEP&gt;chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7&lt;SEP&gt;chunk-17c651f0ad506eb37b0f1742513e2650&lt;SEP&gt;chunk-568ea9e09f85a7b95cfe61d48b0ba893&lt;SEP&gt;chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="DARE">
  <data key="d0">DARE</data>
  <data key="d1">category</data>
  <data key="d2">DARE (Drop And REscale) is a model sparsification method designed to reduce redundancy in model parameters while preserving task capabilities.&lt;SEP&gt;DARE is a method employed in the optimization process of machine learning models, particularly in merging experiments.&lt;SEP&gt;DARE is a method that enhances model merging by zeroing out small differences between models while amplifying significant differences, facilitating effective model integration.&lt;SEP&gt;DARE is a method that works in conjunction with TIES-Merging to provide more granular merging capabilities in neural network layers.&lt;SEP&gt;DARE is a specific technique employed in conjunction with TIES-Merging to optimize model parameters during merging experiments.&lt;SEP&gt;DARE is a technique utilized for merging models in the data flow space, aiming to improve model capabilities.&lt;SEP&gt;DARE is a model sparsification method proposed to enhance model merging performance, particularly when combined with M&lt;sup&gt;3&lt;/sup&gt;.&lt;SEP&gt;DARE is a model sparsification method that works in conjunction with M&lt;sup&gt;3&lt;/sup&gt; to improve model merging performance.&lt;SEP&gt;DARE is a sparsification technique that can be combined with model merging methods to improve performance.&lt;SEP&gt;DARE is a sparsification technique used in conjunction with model merging to enhance performance by managing delta parameters.&lt;SEP&gt;DARE is a versatile plug-in designed for merging multiple homologous fine-tuned models while reducing parameter interference, enhancing task performance across benchmarks.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149&lt;SEP&gt;chunk-c8ff785e8102dfde59dbc9309abb6b6a&lt;SEP&gt;chunk-f21dc353d188ca08cba26298157ff09d&lt;SEP&gt;chunk-67c2717229a96dfe2b74a6bceef933f8&lt;SEP&gt;chunk-77432ea4d82a2648008d6d26c50ccdc9&lt;SEP&gt;chunk-47449b4eaab59a26168fa9d821973678&lt;SEP&gt;chunk-568ea9e09f85a7b95cfe61d48b0ba893&lt;SEP&gt;chunk-3df998a85b2dff618feb2e2ce206b62c&lt;SEP&gt;chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="mergekit">
  <data key="d0">mergekit</data>
  <data key="d1">organization</data>
  <data key="d2">Mergekit is a toolkit that provides various recipes for merging language models, making model merging techniques accessible to practitioners.</data>
  <data key="d3">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Mistral">
  <data key="d0">Mistral</data>
  <data key="d1">organization</data>
  <data key="d2">Mistral is a family of models that serves as a popular base for merging techniques in the language model community.</data>
  <data key="d3">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Frankenmerging">
  <data key="d0">Frankenmerging</data>
  <data key="d1">category</data>
  <data key="d2">Frankenmerging is a method that allows users to experiment with stacking layers from different models to create new architectures, distinct from traditional weight merging.&lt;SEP&gt;Frankenmerging is a model merging approach noted for its tendency to produce poor performance outcomes.&lt;SEP&gt;Frankenmerging is another model merging technique that showed decreased performance in the experiments conducted.&lt;SEP&gt;Frankenmerging refers to a specific merging technique used in model building that combines elements from different models to create new architectures.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a&lt;SEP&gt;chunk-d8ed04169bf3a7ed0de2249060ee26fb&lt;SEP&gt;chunk-689927e59bcb702105a164ca12f7f6b1&lt;SEP&gt;chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Parameter Interference">
  <data key="d0">Parameter Interference</data>
  <data key="d1">category</data>
  <data key="d2">Parameter interference refers to the issue of conflicting parameter values and signs across models during the merging process, leading to performance degradation.</data>
  <data key="d3">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="ComPEFT">
  <data key="d0">ComPEFT</data>
  <data key="d1">category</data>
  <data key="d2">ComPEFT is a method that investigates the compression of fine-tuned weight parameter updates in model merging, aiming for improved performance.</data>
  <data key="d3">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Multimodal Model Development">
  <data key="d0">Multimodal Model Development</data>
  <data key="d1">category</data>
  <data key="d2">Multimodal model development involves creating models that can process and integrate multiple types of data, such as text and images.</data>
  <data key="d3">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Image Generation">
  <data key="d0">Image Generation</data>
  <data key="d1">category</data>
  <data key="d2">Image generation refers to the process of creating new images using machine learning models, often enhanced by merging techniques.</data>
  <data key="d3">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="LLM Communities">
  <data key="d0">LLM Communities</data>
  <data key="d1">category</data>
  <data key="d2">LLM communities consist of researchers and engineers focused on developing and merging large language models for various applications.</data>
  <data key="d3">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Evolutionary Neural Architecture Search">
  <data key="d0">Evolutionary Neural Architecture Search</data>
  <data key="d1">category</data>
  <data key="d2">Evolutionary Neural Architecture Search refers to a systematic approach that utilizes evolutionary algorithms to optimize model merging and architecture discovery in deep learning.</data>
  <data key="d3">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Neural Architecture Search (NAS)">
  <data key="d0">Neural Architecture Search (NAS)</data>
  <data key="d1">category</data>
  <data key="d2">Neural Architecture Search (NAS) is a method that employs evolutionary techniques to discover new neural network architectures, often requiring substantial computational resources.</data>
  <data key="d3">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Transformer Blocks">
  <data key="d0">Transformer Blocks</data>
  <data key="d1">category</data>
  <data key="d2">Transformer Blocks are components of neural networks that can be mixed and matched in model architectures, essential for enhancing performance in deep learning tasks.</data>
  <data key="d3">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="NEAT">
  <data key="d0">NEAT</data>
  <data key="d1">organization</data>
  <data key="d2">NEAT (NeuroEvolution of Augmenting Topologies) is an evolutionary algorithm that evolves neural network structures without the need for gradient descent training.</data>
  <data key="d3">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Weight Agnostic Neural Networks">
  <data key="d0">Weight Agnostic Neural Networks</data>
  <data key="d1">category</data>
  <data key="d2">Weight Agnostic Neural Networks are a type of neural network that evolves structures with task-specific biases without training weight parameters.</data>
  <data key="d3">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="SMASH">
  <data key="d0">SMASH</data>
  <data key="d1">category</data>
  <data key="d2">SMASH (Single Model Architecture Search) is a NAS method that avoids costly training through the application of Hypernetworks to estimate weights of architectural candidates.</data>
  <data key="d3">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Deep Learning">
  <data key="d0">Deep Learning</data>
  <data key="d1">category</data>
  <data key="d2">Deep Learning is a subset of machine learning that utilizes neural networks with many layers to analyze various forms of data.</data>
  <data key="d3">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model Building Process">
  <data key="d0">Model Building Process</data>
  <data key="d1">category</data>
  <data key="d2">The Model Building Process encompasses the methodologies and techniques used to create machine learning models, including merging and optimization strategies.</data>
  <data key="d3">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Hypernetwork">
  <data key="d0">Hypernetwork</data>
  <data key="d1">category</data>
  <data key="d2">A Hypernetwork is a neural network that generates weights for another network, enabling efficient training and architecture search.</data>
  <data key="d3">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Parameter Space (PS)">
  <data key="d0">Parameter Space (PS)</data>
  <data key="d1">category</data>
  <data key="d2">Parameter Space refers to the configuration space where model weights are adjusted to create a unified model from multiple sources.</data>
  <data key="d3">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Data Flow Space (DFS)">
  <data key="d0">Data Flow Space (DFS)</data>
  <data key="d1">category</data>
  <data key="d2">Data Flow Space is a configuration space that focuses on how data moves through layers in a neural network architecture.&lt;SEP&gt;Data Flow Space refers to a conceptual space where model merging preserves original weights and optimizes the inference path for tokens in neural networks.</data>
  <data key="d3">chunk-67c2717229a96dfe2b74a6bceef933f8&lt;SEP&gt;chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Merging Configuration Parameters">
  <data key="d0">Merging Configuration Parameters</data>
  <data key="d1">category</data>
  <data key="d2">Merging Configuration Parameters are settings established for sparsification and weight mixing at each layer of a neural network.&lt;SEP&gt;Merging Configuration Parameters are the settings that determine how models are combined, influencing the performance of the resulting architecture.</data>
  <data key="d3">chunk-67c2717229a96dfe2b74a6bceef933f8&lt;SEP&gt;chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="CMA-ES">
  <data key="d0">CMA-ES</data>
  <data key="d1">category</data>
  <data key="d2">CMA-ES is an evolutionary algorithm used to optimize merging configuration parameters in neural networks based on critical task-specific metrics.&lt;SEP&gt;CMA-ES is an optimization algorithm used in the EvoJAX framework for model parameter optimization.&lt;SEP&gt;CMA-ES is an optimization algorithm utilized within the EvoJAX framework to optimize model parameters through evolutionary strategies.&lt;SEP&gt;CMA-ES stands for Covariance Matrix Adaptation Evolution Strategy, a method used for optimization in the context of the models discussed.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149&lt;SEP&gt;chunk-67c2717229a96dfe2b74a6bceef933f8&lt;SEP&gt;chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Evolutionary Search">
  <data key="d0">Evolutionary Search</data>
  <data key="d1">category</data>
  <data key="d2">Evolutionary Search is a method used to explore a large search space of layer arrangements in model merging.</data>
  <data key="d3">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Layer-wise Merging">
  <data key="d0">Layer-wise Merging</data>
  <data key="d1">category</data>
  <data key="d2">Layer-wise Merging is a specific approach within model merging that focuses on optimizing individual layers of neural networks for improved performance.</data>
  <data key="d3">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Input/Output Embedding Layers">
  <data key="d0">Input/Output Embedding Layers</data>
  <data key="d1">category</data>
  <data key="d2">Input/Output Embedding Layers are components of neural networks that transform input data into a format suitable for processing and vice versa.</data>
  <data key="d3">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Transformer Block">
  <data key="d0">Transformer Block</data>
  <data key="d1">category</data>
  <data key="d2">Transformer Block is a building block of transformer models that allows for efficient processing of sequential data in machine learning.</data>
  <data key="d3">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Granular Merging">
  <data key="d0">Granular Merging</data>
  <data key="d1">category</data>
  <data key="d2">Granular Merging refers to the detailed and specific merging of model components to optimize performance for targeted tasks.</data>
  <data key="d3">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Sparsification">
  <data key="d0">Sparsification</data>
  <data key="d1">category</data>
  <data key="d2">Sparsification is a technique used in neural networks to reduce the number of active parameters, improving efficiency and speed.</data>
  <data key="d3">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Weight Mixing">
  <data key="d0">Weight Mixing</data>
  <data key="d1">category</data>
  <data key="d2">Weight Mixing involves combining the weights of different models or layers to create a more robust model.</data>
  <data key="d3">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Inference Path">
  <data key="d0">Inference Path</data>
  <data key="d1">category</data>
  <data key="d2">Inference Path refers to the route that data takes through a neural network during the prediction phase.</data>
  <data key="d3">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Search Space">
  <data key="d0">Search Space</data>
  <data key="d1">category</data>
  <data key="d2">Search Space is the theoretical space of all possible configurations or arrangements of a model that can be explored during optimization.</data>
  <data key="d3">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Layer Indices">
  <data key="d0">Layer Indices</data>
  <data key="d1">category</data>
  <data key="d2">Layer Indices are numerical representations of the positions of layers within a neural network model.</data>
  <data key="d3">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Sequential Order">
  <data key="d0">Sequential Order</data>
  <data key="d1">category</data>
  <data key="d2">Sequential Order refers to the arrangement of layers in a specific sequence in a model merging context.</data>
  <data key="d3">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Indicator Array">
  <data key="d0">Indicator Array</data>
  <data key="d1">category</data>
  <data key="d2">Indicator Array is a data structure used to manage the inclusion or exclusion of layers during model merging.&lt;SEP&gt;The Indicator Array is a data structure initialized within the models, used to track the state of layers and their configurations during processing.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890&lt;SEP&gt;chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Distribution Shift">
  <data key="d0">Distribution Shift</data>
  <data key="d1">category</data>
  <data key="d2">Distribution Shift refers to the changes in data distribution that can occur when inputs are processed by different layers of a model.</data>
  <data key="d3">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="PS Merging">
  <data key="d0">PS Merging</data>
  <data key="d1">category</data>
  <data key="d2">PS Merging is a strategy used within the hybrid model to improve task performance by merging different source models.&lt;SEP&gt;PS Merging is a technique used to combine multiple models in a way that optimizes for various objectives, enhancing the model's performance.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344&lt;SEP&gt;chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="DFS Merging">
  <data key="d0">DFS Merging</data>
  <data key="d1">category</data>
  <data key="d2">DFS Merging is a method applied after PS Merging to further refine the models, often utilizing multi-objective genetic algorithms like NSGA-II.&lt;SEP&gt;DFS Merging is a technique used to combine multiple models while maintaining a modest size for efficient performance on hardware.&lt;SEP&gt;DFS Merging is an approach used in the study to combine different models for improved performance.&lt;SEP&gt;DFS Merging is another strategy utilized in the hybrid model that focuses on enhancing performance through a different integration approach.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344&lt;SEP&gt;chunk-48d7bed100b753d40cd980d00870f149&lt;SEP&gt;chunk-77300cedbd95cdfcbafc5dec2d0ec643&lt;SEP&gt;chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Math LLMs">
  <data key="d0">Math LLMs</data>
  <data key="d1">organization</data>
  <data key="d2">Math LLMs are language models specifically trained to solve mathematical problems, contributing to the merging process for enhanced capabilities.</data>
  <data key="d3">chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="MGSM Dataset">
  <data key="d0">MGSM Dataset</data>
  <data key="d1">organization</data>
  <data key="d2">The MGSM Dataset is a multilingual translation dataset used for evaluating the performance of language models on math problems in Japanese.</data>
  <data key="d3">chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="GSM8k Dataset">
  <data key="d0">GSM8k Dataset</data>
  <data key="d1">organization</data>
  <data key="d2">The GSM8k Dataset is a benchmark dataset for evaluating mathematical reasoning in language models, providing a test set for model accuracy.</data>
  <data key="d3">chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Mistral-7B">
  <data key="d0">Mistral-7B</data>
  <data key="d1">organization</data>
  <data key="d2">Mistral-7B is a foundational model from which other models are fine-tuned, including the Japanese LLM and Math LLMs.</data>
  <data key="d3">chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="CMA-ES Algorithm">
  <data key="d0">CMA-ES Algorithm</data>
  <data key="d1">category</data>
  <data key="d2">CMA-ES is an optimization algorithm used in the PS Merging process to enhance model parameters for better performance.</data>
  <data key="d3">chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Optuna">
  <data key="d0">Optuna</data>
  <data key="d1">organization</data>
  <data key="d2">Optuna is a next-generation hyperparameter optimization framework used in machine learning to enhance model performance.&lt;SEP&gt;Optuna is a next-generation hyperparameter optimization framework used in machine learning.&lt;SEP&gt;Optuna is a software framework used for hyperparameter optimization, particularly in the context of the CMA-ES algorithm.</data>
  <data key="d3">chunk-605e26052d578b60e0e6849cb0d635d9&lt;SEP&gt;chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Section 3.1">
  <data key="d0">Section 3.1</data>
  <data key="d1">event</data>
  <data key="d2">Section 3.1 discusses the experimental setup for evolving a Japanese Math LLM, detailing the models and datasets used.&lt;SEP&gt;Section 3.1 refers to a specific part of the paper that discusses LLM experiments and their methodologies.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52&lt;SEP&gt;chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Section 3.3">
  <data key="d0">Section 3.3</data>
  <data key="d1">event</data>
  <data key="d2">Section 3.3 describes the process of merging a Japanese LLM with an English VLM to create a Japanese VLM.&lt;SEP&gt;Section 3.3 refers to a part of the paper that focuses on VLM experiments and their methodologies.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52&lt;SEP&gt;chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="MGSM">
  <data key="d0">MGSM</data>
  <data key="d1">organization</data>
  <data key="d2">MGSM is an organization that provides a test set used for evaluating language models, particularly in the context of Japanese Math.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="EvoJAX">
  <data key="d0">EvoJAX</data>
  <data key="d1">category</data>
  <data key="d2">EvoJAX is a framework designed for evolutionary optimization in machine learning, specifically for optimizing models through genetic algorithms.&lt;SEP&gt;EvoJAX is a framework utilized for evolutionary optimization in machine learning, particularly in the context of model merging.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model 1">
  <data key="d0">Model 1</data>
  <data key="d1">category</data>
  <data key="d2">Model 1 is a Japanese language model evaluated for its mathematical proficiency, scoring low on the MGSM-JA benchmark.&lt;SEP&gt;Model 1 is a language model evaluated for its performance in Japanese Math tasks, showing limited proficiency in mathematics.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model 2">
  <data key="d0">Model 2</data>
  <data key="d1">category</data>
  <data key="d2">Model 2 is a Math model that demonstrates mathematical proficiency but lacks command of the Japanese language.&lt;SEP&gt;Model 2 is a language model focused on mathematics, demonstrating mathematical proficiency but lacking in Japanese language skills.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model 3">
  <data key="d0">Model 3</data>
  <data key="d1">category</data>
  <data key="d2">Model 3 is another Math model that, while mathematically adept, also scores low in Japanese language capabilities.&lt;SEP&gt;Model 3 is another mathematical language model that showcases mathematical abilities but scores poorly in the Japanese language proficiency.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model 4">
  <data key="d0">Model 4</data>
  <data key="d1">category</data>
  <data key="d2">Model 4 is an optimized merged model that achieved a significant performance score on the MGSM-JA benchmark.&lt;SEP&gt;Model 4 is an optimized merged model that significantly improves performance on the MGSM-JA benchmark compared to the source models.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model 5">
  <data key="d0">Model 5</data>
  <data key="d1">category</data>
  <data key="d2">Model 5 is a DFS-merged model that exhibits performance enhancements over the original source models.&lt;SEP&gt;Model 5 is a DFS-merged model that shows performance enhancement compared to the source models.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model 6">
  <data key="d0">Model 6</data>
  <data key="d1">category</data>
  <data key="d2">Model 6 is a hybrid model that integrates both merging strategies and achieves further performance improvements.&lt;SEP&gt;Model 6 is a hybrid model that integrates both merging strategies, achieving further improvements in performance metrics.&lt;SEP&gt;Model 6 is a hybrid model that integrates multiple merging strategies to enhance performance on mathematical tasks.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149&lt;SEP&gt;chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model 7">
  <data key="d0">Model 7</data>
  <data key="d1">category</data>
  <data key="d2">Model 7 is a general English model evaluated for its performance on Japanese Math tasks.&lt;SEP&gt;Model 7 is an English general model that is evaluated in the context of Japanese Math tasks.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model 8">
  <data key="d0">Model 8</data>
  <data key="d1">category</data>
  <data key="d2">Model 8 is a Japanese general model assessed for its performance on the benchmark tasks.&lt;SEP&gt;Model 8 is a general Japanese model assessed in the context of the benchmark tasks.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model 9">
  <data key="d0">Model 9</data>
  <data key="d1">category</data>
  <data key="d2">Model 9 is a Japanese general model evaluated in the performance comparison.&lt;SEP&gt;Model 9 is another Japanese general model evaluated for its capabilities in the performance comparison.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model 10">
  <data key="d0">Model 10</data>
  <data key="d1">category</data>
  <data key="d2">Model 10 is a commercial model, GPT-3.5, evaluated for its performance on the benchmarks.&lt;SEP&gt;Model 10 is a commercial model, GPT-3.5, that is assessed for its performance on various language tasks.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model 11">
  <data key="d0">Model 11</data>
  <data key="d1">category</data>
  <data key="d2">Model 11 is a commercial model, GPT-4, recognized for its high performance across language tasks.&lt;SEP&gt;Model 11 is another commercial model, GPT-4, known for its high performance on language tasks.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="MGSM's Test Set">
  <data key="d0">MGSM's Test Set</data>
  <data key="d1">organization</data>
  <data key="d2">MGSM's Test Set is a dataset used for evaluating language models, particularly in Japanese Math tasks, providing a benchmark for performance.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="MGSM-JA task">
  <data key="d0">MGSM-JA task</data>
  <data key="d1">event</data>
  <data key="d2">The MGSM-JA task evaluates the performance of models on mathematical problems with a focus on Japanese language understanding.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Japanese Language Model Evaluation Harness (JP-LMEH)">
  <data key="d0">Japanese Language Model Evaluation Harness (JP-LMEH)</data>
  <data key="d1">organization</data>
  <data key="d2">JP-LMEH is a benchmark suite used to evaluate Japanese language proficiency across various tasks.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Shisa Gamma 7b v1">
  <data key="d0">Shisa Gamma 7b v1</data>
  <data key="d1">category</data>
  <data key="d2">Shisa Gamma 7b v1 is a Japanese language model that serves as a baseline for comparison in language proficiency evaluations.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="WizardMath 7B V1.1">
  <data key="d0">WizardMath 7B V1.1</data>
  <data key="d1">category</data>
  <data key="d2">WizardMath 7B V1.1 is another Japanese language model evaluated against the JP-LMEH tasks.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Abel 7B 002">
  <data key="d0">Abel 7B 002</data>
  <data key="d1">category</data>
  <data key="d2">Abel 7B 002 is a Japanese language model included in the evaluation of language proficiency.&lt;SEP&gt;Abel 7B 002 is a source model involved in the performance comparisons of merging methods.</data>
  <data key="d3">chunk-d8ed04169bf3a7ed0de2249060ee26fb&lt;SEP&gt;chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Ours (PS)">
  <data key="d0">Ours (PS)</data>
  <data key="d1">organization</data>
  <data key="d2">Ours (PS) is a merged model that combines different performance metrics from 7B and 10B sources.&lt;SEP&gt;Ours (PS) refers to the model developed by the authors that shows high scores in the JP-LMEH benchmark.</data>
  <data key="d3">chunk-08f49a80ddeee72c7972766e2571c12b&lt;SEP&gt;chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Ours (DFS)">
  <data key="d0">Ours (DFS)</data>
  <data key="d1">organization</data>
  <data key="d2">Ours (DFS) is a merged model that presents performance data from various 7B and 10B sources.&lt;SEP&gt;Ours (DFS) is an organization involved in model merging and optimization, showcasing various performance metrics.&lt;SEP&gt;Ours (DFS) is another model developed by the authors that integrates different merging strategies for evaluation.</data>
  <data key="d3">chunk-08f49a80ddeee72c7972766e2571c12b&lt;SEP&gt;chunk-f96b71f942f04756e590fdfa1786ae98&lt;SEP&gt;chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Hybrid Model">
  <data key="d0">Hybrid Model</data>
  <data key="d1">category</data>
  <data key="d2">The Hybrid Model refers to a model that integrates multiple merging strategies to enhance performance on various tasks, particularly in mathematical problem-solving.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Japanese StableLM 70B">
  <data key="d0">Japanese StableLM 70B</data>
  <data key="d1">category</data>
  <data key="d2">Japanese StableLM 70B is a previous state-of-the-art Japanese language model with 70 billion parameters, serving as a benchmark for comparison.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="MGSM Scores">
  <data key="d0">MGSM Scores</data>
  <data key="d1">category</data>
  <data key="d2">MGSM Scores are performance metrics used to evaluate the effectiveness of models on mathematical tasks, specifically within the MGSM-JA task.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="JSQuAD">
  <data key="d0">JSQuAD</data>
  <data key="d1">category</data>
  <data key="d2">JSQuAD is a task within the JP-LMEH benchmark suite that assesses models' abilities in question answering in Japanese.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="JAQKET">
  <data key="d0">JAQKET</data>
  <data key="d1">category</data>
  <data key="d2">JAQKET is another task in the JP-LMEH benchmark suite, evaluating models on their ability to handle Japanese questions.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="JNLI">
  <data key="d0">JNLI</data>
  <data key="d1">category</data>
  <data key="d2">JNLI is a task included in the JP-LMEH benchmark that tests models on their understanding of natural language inference in Japanese.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="MARC">
  <data key="d0">MARC</data>
  <data key="d1">category</data>
  <data key="d2">MARC is a task in the JP-LMEH framework that evaluates models on their performance in reading comprehension and reasoning.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Appendix A">
  <data key="d0">Appendix A</data>
  <data key="d1">event</data>
  <data key="d2">Appendix A contains additional details and extensive comparisons of different models and their performances.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Appendix C">
  <data key="d0">Appendix C</data>
  <data key="d1">event</data>
  <data key="d2">Appendix C showcases examples demonstrating the utility of the merged models in answering questions related to Japanese culture and math.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Ours (PS+DFS)">
  <data key="d0">Ours (PS+DFS)</data>
  <data key="d1">organization</data>
  <data key="d2">Ours (PS+DFS) is a model that integrates performance metrics from both PS and DFS categories.&lt;SEP&gt;Ours (PS+DFS) is an organization that focuses on merging models with specific parameters for enhanced performance.</data>
  <data key="d3">chunk-08f49a80ddeee72c7972766e2571c12b&lt;SEP&gt;chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Llama 2 70B">
  <data key="d0">Llama 2 70B</data>
  <data key="d1">organization</data>
  <data key="d2">Llama 2 70B refers to a specific model that is part of the analysis, showcasing performance metrics in the context of model evaluation.</data>
  <data key="d3">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Japanese Stable LM 70B">
  <data key="d0">Japanese Stable LM 70B</data>
  <data key="d1">organization</data>
  <data key="d2">Japanese Stable LM 70B is a language model that plays a critical role in the analysis, particularly noted for its contributions.</data>
  <data key="d3">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Swallow 70B">
  <data key="d0">Swallow 70B</data>
  <data key="d1">organization</data>
  <data key="d2">Swallow 70B is another model referenced in the performance analysis, contributing metrics to the overall evaluation.</data>
  <data key="d3">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="CMA-ES Optimization Results">
  <data key="d0">CMA-ES Optimization Results</data>
  <data key="d1">event</data>
  <data key="d2">CMA-ES optimization results refer to the findings from the optimization process that highlight the importance of model contributions in merging.</data>
  <data key="d3">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Parameter Configuration Post PS Merging">
  <data key="d0">Parameter Configuration Post PS Merging</data>
  <data key="d1">event</data>
  <data key="d2">Parameter configuration post PS merging refers to the evolved settings derived from the merging experiments.</data>
  <data key="d3">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Mistral Base Model">
  <data key="d0">Mistral Base Model</data>
  <data key="d1">organization</data>
  <data key="d2">Mistral Base Model is a foundational model that serves as the basis for further fine-tuning in the Japanese language models.</data>
  <data key="d3">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Shisa-Gamma-7B-v1">
  <data key="d0">Shisa-Gamma-7B-v1</data>
  <data key="d1">organization</data>
  <data key="d2">Shisa-Gamma-7B-v1 is a Japanese language model that undergoes continued pretraining and instruction fine-tuning, enhancing its capabilities.</data>
  <data key="d3">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="DARE-TIES">
  <data key="d0">DARE-TIES</data>
  <data key="d1">category</data>
  <data key="d2">DARE-TIES is a model merging technique compared against other methods in the study.&lt;SEP&gt;DARE-TIES is a model that performed relatively better in the experiments, showing slight improvements compared to source models.&lt;SEP&gt;DARE-TIES is another technique for model merging that focuses on creating new models from existing high-performing models.&lt;SEP&gt;DARE-TIES refers to the parameters associated with evolutionary merging in model optimization.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a&lt;SEP&gt;chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b&lt;SEP&gt;chunk-d8ed04169bf3a7ed0de2249060ee26fb&lt;SEP&gt;chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="MGSM-JA">
  <data key="d0">MGSM-JA</data>
  <data key="d1">category</data>
  <data key="d2">MGSM-JA is a scoring metric used to evaluate the performance of models in the experiments.&lt;SEP&gt;MGSM-JA is a scoring metric used to evaluate the performance of the models in the study.&lt;SEP&gt;MGSM-JA is a task or benchmark used to evaluate model performance, particularly in accuracy metrics.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a&lt;SEP&gt;chunk-d8ed04169bf3a7ed0de2249060ee26fb&lt;SEP&gt;chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="JP-LMEH">
  <data key="d0">JP-LMEH</data>
  <data key="d1">category</data>
  <data key="d2">JP-LMEH is another benchmark that measures average performance metrics for model evaluations.&lt;SEP&gt;JP-LMEH is another scoring metric that assesses the performance of models specifically in Japanese language tasks.&lt;SEP&gt;JP-LMEH is another scoring metric used to assess model performance, which was impacted by fine-tuning.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a&lt;SEP&gt;chunk-d8ed04169bf3a7ed0de2249060ee26fb&lt;SEP&gt;chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="TIES-Merge">
  <data key="d0">TIES-Merge</data>
  <data key="d1">organization</data>
  <data key="d2">TIES-Merge is a method used for model merging that was evaluated in the context of performance comparison.&lt;SEP&gt;TIES-Merge is a model merging approach that was tested for performance against other merging methods in the context of machine learning.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a&lt;SEP&gt;chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Shisa Gamma 7B v1">
  <data key="d0">Shisa Gamma 7B v1</data>
  <data key="d1">organization</data>
  <data key="d2">Shisa Gamma 7B v1 is a source model used in the experiments for comparison with the proposed methods.</data>
  <data key="d3">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="WizardMath 7B v1.1">
  <data key="d0">WizardMath 7B v1.1</data>
  <data key="d1">organization</data>
  <data key="d2">WizardMath 7B v1.1 is another source model utilized in the evaluation of model merging techniques.</data>
  <data key="d3">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Performance Comparison Table">
  <data key="d0">Performance Comparison Table</data>
  <data key="d1">event</data>
  <data key="d2">The Performance Comparison Table presents results of various model merging methods and their effectiveness.</data>
  <data key="d3">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="W_{ij}">
  <data key="d0">W_{ij}</data>
  <data key="d1">category</data>
  <data key="d2">W_{ij} represents scaling parameters that are crucial in the performance of evolved models, as indicated by the study's findings.</data>
  <data key="d3">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="A and B">
  <data key="d0">A and B</data>
  <data key="d1">organization</data>
  <data key="d2">Models A and B are the two distinct models that were merged and analyzed in the study for performance.</data>
  <data key="d3">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="PS">
  <data key="d0">PS</data>
  <data key="d1">category</data>
  <data key="d2">PS refers to a specific method or approach used in the context of model merging and evaluation.</data>
  <data key="d3">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="DFS">
  <data key="d0">DFS</data>
  <data key="d1">category</data>
  <data key="d2">DFS is another method utilized in model merging, evaluated alongside other techniques in the study.</data>
  <data key="d3">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="10B model">
  <data key="d0">10B model</data>
  <data key="d1">organization</data>
  <data key="d2">The 10B model (PS+DFS) is a specific model configuration that was subjected to performance analysis.</data>
  <data key="d3">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Unoptimized Model Merging">
  <data key="d0">Unoptimized Model Merging</data>
  <data key="d1">category</data>
  <data key="d2">Unoptimized Model Merging refers to the category of methods compared against the proposed methods in the study.</data>
  <data key="d3">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Table 3">
  <data key="d0">Table 3</data>
  <data key="d1">event</data>
  <data key="d2">Table 3 summarizes the performance metrics of various merging methods and serves as a key reference point in the analysis.</data>
  <data key="d3">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Ablation Studies">
  <data key="d0">Ablation Studies</data>
  <data key="d1">event</data>
  <data key="d2">Ablation Studies are experiments conducted to understand the impact of different model configurations on performance.&lt;SEP&gt;Ablation Studies refer to experiments conducted to analyze the effect of removing certain parameters on model performance.</data>
  <data key="d3">chunk-d8ed04169bf3a7ed0de2249060ee26fb&lt;SEP&gt;chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="PS model">
  <data key="d0">PS model</data>
  <data key="d1">category</data>
  <data key="d2">The PS model is a merging method that significantly outperformed other baselines in the experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="shisa-gamma-7b-v1">
  <data key="d0">shisa-gamma-7b-v1</data>
  <data key="d1">organization</data>
  <data key="d2">Shisa-gamma-7b-v1 is a source model used in the merging experiments, fine-tuned for Japanese.&lt;SEP&gt;shisa-gamma-7b-v1 is a model categorized as a 7B source model, noted for its high performance metrics.</data>
  <data key="d3">chunk-08f49a80ddeee72c7972766e2571c12b&lt;SEP&gt;chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="WizardMath-7B-V1.1">
  <data key="d0">WizardMath-7B-V1.1</data>
  <data key="d1">organization</data>
  <data key="d2">WizardMath-7B-V1.1 is a 7B source model that demonstrates specific performance metrics in various categories.&lt;SEP&gt;WizardMath-7B-V1.1 is a math-specialized model that scored high on the GSM8k benchmark, indicating strong performance in math tasks.&lt;SEP&gt;WizardMath-7B-V1.1 is a mathematical language model released under a Non-Commercial, Research-only Microsoft License.&lt;SEP&gt;WizardMath-7B-V1.1 is a source model included in the experiments, but its training data is not publicly available.</data>
  <data key="d3">chunk-08f49a80ddeee72c7972766e2571c12b&lt;SEP&gt;chunk-ef78ab420a549ea9f241c278218c7a0a&lt;SEP&gt;chunk-96aa56aec4e4f18a590f28efaff42c99&lt;SEP&gt;chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Abel-7B-002">
  <data key="d0">Abel-7B-002</data>
  <data key="d1">organization</data>
  <data key="d2">Abel-7B-002 is a 7B source model with performance metrics that highlight its capabilities.&lt;SEP&gt;Abel-7B-002 is a source model utilized in the study, with proprietary training data.</data>
  <data key="d3">chunk-08f49a80ddeee72c7972766e2571c12b&lt;SEP&gt;chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="HuggingFace">
  <data key="d0">HuggingFace</data>
  <data key="d1">organization</data>
  <data key="d2">HuggingFace is a platform that provides models and tools for machine learning, including language models used in the evaluation.&lt;SEP&gt;HuggingFace is a platform where models like shisa-gamma-7b-v1, WizardMath-7B-V1.1, and Abel-7B-002 are published.&lt;SEP&gt;HuggingFace is an organization known for its contributions to natural language processing and machine learning, including the Open LLM Leaderboard.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a&lt;SEP&gt;chunk-96aa56aec4e4f18a590f28efaff42c99&lt;SEP&gt;chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Japanese-translated GSM8k">
  <data key="d0">Japanese-translated GSM8k</data>
  <data key="d1">event</data>
  <data key="d2">Japanese-translated GSM8k is the dataset used for fine-tuning the models in the experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="MGSM-JA Score">
  <data key="d0">MGSM-JA Score</data>
  <data key="d1">category</data>
  <data key="d2">The MGSM-JA Score is a performance metric used to evaluate the effectiveness of models in the context of machine learning experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="PS+DFS">
  <data key="d0">PS+DFS</data>
  <data key="d1">method</data>
  <data key="d2">PS+DFS is a hybrid merging method that combines PS and DFS approaches to achieve the best test performance on a target task.&lt;SEP&gt;PS+DFS is a method combining different model architectures to achieve better results in machine learning tasks.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a&lt;SEP&gt;chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="LoRA">
  <data key="d0">LoRA</data>
  <data key="d1">category</data>
  <data key="d2">LoRA (Low-Rank Adaptation) is a method used to fine-tune models efficiently, employed in the experiments for model optimization.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Epochs">
  <data key="d0">Epochs</data>
  <data key="d1">category</data>
  <data key="d2">Epochs refer to the number of complete passes through the training dataset during the model training process, set to 3 in the experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Learning Rates">
  <data key="d0">Learning Rates</data>
  <data key="d1">category</data>
  <data key="d2">Learning rates are hyperparameters that control the step size during optimization, tested at various values (1e-5, 5e-5, 1e-4) in the experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Distraction Effect">
  <data key="d0">Distraction Effect</data>
  <data key="d1">category</data>
  <data key="d2">The distraction effect refers to the impact of including irrelevant models in the selection process, affecting the performance outcomes in the experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Irrelevant Models">
  <data key="d0">Irrelevant Models</data>
  <data key="d1">category</data>
  <data key="d2">Irrelevant models are those that lack relevance to the specific tasks of Japanese or mathematics, included in the distraction experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="German Model">
  <data key="d0">German Model</data>
  <data key="d1">organization</data>
  <data key="d2">The German model, referred to as leo-mistral-hessianai-7b, is an irrelevant model included in the distraction experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Spanish Model">
  <data key="d0">Spanish Model</data>
  <data key="d1">organization</data>
  <data key="d2">The Spanish model, referred to as lince-mistral-7b-it-es, is another irrelevant model included in the distraction experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Chinese Model">
  <data key="d0">Chinese Model</data>
  <data key="d1">organization</data>
  <data key="d2">The Chinese model, referred to as Mistral-7B-v0.3-Chinese-Chat, is included as an irrelevant model in the distraction experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Biomedical Model">
  <data key="d0">Biomedical Model</data>
  <data key="d1">organization</data>
  <data key="d2">The biomedical model, referred to as BioMistral-7B, is classified as an irrelevant model in the distraction experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="French Model">
  <data key="d0">French Model</data>
  <data key="d1">organization</data>
  <data key="d2">The French model, referred to as Claire-Mistral-7B-0.1, is included as an irrelevant model in the distraction experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Korean Model">
  <data key="d0">Korean Model</data>
  <data key="d1">organization</data>
  <data key="d2">The Korean model, referred to as komt-mistral-7b-v1, is classified as an irrelevant model in the distraction experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Arabic Model">
  <data key="d0">Arabic Model</data>
  <data key="d1">organization</data>
  <data key="d2">The Arabic model, referred to as Mistral-7B-v0.1-arabic, is included as an irrelevant model in the distraction experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Italian Model">
  <data key="d0">Italian Model</data>
  <data key="d1">organization</data>
  <data key="d2">The Italian model, referred to as Loquace-7B-Mistral, is classified as an irrelevant model in the distraction experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Llama-2-13b">
  <data key="d0">Llama-2-13b</data>
  <data key="d1">organization</data>
  <data key="d2">Llama-2-13b is a large language model that serves as the backbone for various tasks but has training data only up to July 2023.&lt;SEP&gt;Llama-2-13b is an English general model used for various machine learning tasks, specifically in the context of the study.</data>
  <data key="d3">chunk-b650b88d4bebbc45d20b8908b045eff8&lt;SEP&gt;chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="ELYZA-japanese-Llama-2-13b-instruct">
  <data key="d0">ELYZA-japanese-Llama-2-13b-instruct</data>
  <data key="d1">model</data>
  <data key="d2">ELYZA-japanese-Llama-2-13b-instruct is a 13B source model that exhibits specific performance metrics.&lt;SEP&gt;ELYZA-japanese-Llama-2-13b-instruct is a Japanese general model fine-tuned for specific tasks in the Japanese language.</data>
  <data key="d3">chunk-08f49a80ddeee72c7972766e2571c12b&lt;SEP&gt;chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="MetaMath-13B-V1.0">
  <data key="d0">MetaMath-13B-V1.0</data>
  <data key="d1">organization</data>
  <data key="d2">MetaMath-13B-V1.0 is a 13B source model that showcases performance metrics in various categories.&lt;SEP&gt;MetaMath-13B-V1.0 is a specific model referenced in the context of the DFS-Merged Model, notable for its layer configurations and performance metrics.&lt;SEP&gt;MetaMath-13B-V1.0 is an English math model designed to perform mathematical tasks, utilized in the study for performance comparisons.</data>
  <data key="d3">chunk-08f49a80ddeee72c7972766e2571c12b&lt;SEP&gt;chunk-10ec50dbb94517b979f409f78b996890&lt;SEP&gt;chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Mistral-7B-v0.1">
  <data key="d0">Mistral-7B-v0.1</data>
  <data key="d1">model</data>
  <data key="d2">Mistral-7B-v0.1 is a model that falls under the category of 7B source models, showcasing various performance metrics.&lt;SEP&gt;Mistral-7B-v0.1 is a smaller model known for outperforming Llama-2-13b in basic mathematical abilities.</data>
  <data key="d3">chunk-08f49a80ddeee72c7972766e2571c12b&lt;SEP&gt;chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="GPU Memory Requirement">
  <data key="d0">GPU Memory Requirement</data>
  <data key="d1">category</data>
  <data key="d2">GPU Memory Requirement refers to the computational resources needed for model inference, which affects the feasibility of certain experiments.</data>
  <data key="d3">chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Performance Comparison">
  <data key="d0">Performance Comparison</data>
  <data key="d1">category</data>
  <data key="d2">Performance Comparison involves evaluating the effectiveness of different models based on specific metrics such as MGSM-JA and JP-LMEH.</data>
  <data key="d3">chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="DFS-Merged Model">
  <data key="d0">DFS-Merged Model</data>
  <data key="d1">organization</data>
  <data key="d2">The DFS-Merged Model is a computational model that integrates layers from different models to improve performance in specific tasks, particularly in reasoning and language processing.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model A">
  <data key="d0">Model A</data>
  <data key="d1">organization</data>
  <data key="d2">Model A is a source model used in the DFS-Merged Model, known for its initial performance in the DFS process.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model B">
  <data key="d0">Model B</data>
  <data key="d1">organization</data>
  <data key="d2">Model B is another source model incorporated into the DFS-Merged Model, which may have undergone extended fine-tuning affecting its layer outputs.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Layer #30">
  <data key="d0">Layer #30</data>
  <data key="d1">category</data>
  <data key="d2">Layer #30 is a specific layer from the MetaMath-13B-V1.0 model that was identified as redundant and skipped in the DFS-Merged Model to enhance performance.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Japanese General Model">
  <data key="d0">Japanese General Model</data>
  <data key="d1">organization</data>
  <data key="d2">The Japanese General Model is a source model that contributes layers to the DFS-Merged Model, helping to improve its performance in Japanese language tasks.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Task Scenarios">
  <data key="d0">Task Scenarios</data>
  <data key="d1">event</data>
  <data key="d2">Task Scenarios refer to specific instances where the DFS-Merged Model demonstrated improved reasoning capabilities compared to its source models.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model #5">
  <data key="d0">Model #5</data>
  <data key="d1">organization</data>
  <data key="d2">Model #5 refers to the merged model that utilizes a combination of layers from different models to enhance performance in tasks, particularly in the context of depth-first search (DFS) techniques.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model #6">
  <data key="d0">Model #6</data>
  <data key="d1">organization</data>
  <data key="d2">Model #6 is a variant of the merged model that was analyzed for its performance, particularly in relation to the scaling matrix's impact on results.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model #7">
  <data key="d0">Model #7</data>
  <data key="d1">organization</data>
  <data key="d2">Model #7 is another variant of the merged model used in the study to compare performance outcomes with other configurations.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Scaling Matrix W">
  <data key="d0">Scaling Matrix W</data>
  <data key="d1">category</data>
  <data key="d2">The Scaling Matrix W is a matrix utilized in the merged models to adjust layer outputs, crucial for performance optimization.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Layer Stacking">
  <data key="d0">Layer Stacking</data>
  <data key="d1">category</data>
  <data key="d2">Layer Stacking refers to the process of organizing and combining layers from different models to improve computational efficiency and output quality.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Performance Analysis">
  <data key="d0">Performance Analysis</data>
  <data key="d1">event</data>
  <data key="d2">Performance Analysis involves evaluating the effectiveness of the merged models in various scenarios to identify strengths and weaknesses.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Task Performance">
  <data key="d0">Task Performance</data>
  <data key="d1">event</data>
  <data key="d2">Task Performance measures how well the models perform specific tasks, particularly in reasoning and language processing.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Layer Removal">
  <data key="d0">Layer Removal</data>
  <data key="d1">event</data>
  <data key="d2">Layer Removal refers to the process of excluding certain layers from a model to enhance its overall performance based on analytical findings.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="ビリー">
  <data key="d0">ビリー</data>
  <data key="d1">person</data>
  <data key="d2">ビリーはDVDを販売する人物で、火曜日に8人のお客様がいる。彼はDVDを売る活動を行っている。&lt;SEP&gt;ビリーはDVDを販売する人物で、火曜日にお客様にDVDを売る活動を行っている。</data>
  <data key="d3">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="DVD">
  <data key="d0">DVD</data>
  <data key="d1">category</data>
  <data key="d2">DVDはビリーが販売している製品で、顧客によって購入される。</data>
  <data key="d3">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="火曜日">
  <data key="d0">火曜日</data>
  <data key="d1">event</data>
  <data key="d2">火曜日はビリーが8人のお客様にDVDを販売する日である。&lt;SEP&gt;火曜日はビリーが8人の顧客にDVDを販売する日である。</data>
  <data key="d3">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="お客様">
  <data key="d0">お客様</data>
  <data key="d1">person</data>
  <data key="d2">お客様はビリーからDVDを購入する人々で、合計8人がいる。</data>
  <data key="d3">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="アップルパイ">
  <data key="d0">アップルパイ</data>
  <data key="d1">category</data>
  <data key="d2">アップルパイはジョーンズおばあちゃんが消防士の昼食会のために焼いたお菓子である。</data>
  <data key="d3">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="消防士の昼食会">
  <data key="d0">消防士の昼食会</data>
  <data key="d1">event</data>
  <data key="d2">消防士の昼食会はジョーンズおばあちゃんがアップルパイを提供するイベントである。</data>
  <data key="d3">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="ジョーンズおばあちゃん">
  <data key="d0">ジョーンズおばあちゃん</data>
  <data key="d1">person</data>
  <data key="d2">ジョーンズおばあちゃんは消防士の昼食会のためにアップルパイを焼く人物である。</data>
  <data key="d3">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="パイ">
  <data key="d0">パイ</data>
  <data key="d1">category</data>
  <data key="d2">パイはジョーンズおばあちゃんが焼いた料理で、ゲストが自分で取ることができる。</data>
  <data key="d3">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="ゲスト">
  <data key="d0">ゲスト</data>
  <data key="d1">person</data>
  <data key="d2">ゲストは消防士の昼食会に参加し、ジョーンズおばあちゃんが用意したアップルパイを食べる人々である。</data>
  <data key="d3">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Grandma Jones">
  <data key="d0">Grandma Jones</data>
  <data key="d1">person</data>
  <data key="d2">Grandma Jones is a character who is involved in cutting pies into pieces for guests.</data>
  <data key="d3">chunk-3ca398087b4d5dd48a0c1fe169e9b7fd</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Pies">
  <data key="d0">Pies</data>
  <data key="d1">category</data>
  <data key="d2">Pies are baked goods that were cut into pieces for serving to guests.</data>
  <data key="d3">chunk-3ca398087b4d5dd48a0c1fe169e9b7fd</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Guests">
  <data key="d0">Guests</data>
  <data key="d1">person</data>
  <data key="d2">Guests are individuals who partake in eating the pies that were prepared.</data>
  <data key="d3">chunk-3ca398087b4d5dd48a0c1fe169e9b7fd</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Pieces of Pie">
  <data key="d0">Pieces of Pie</data>
  <data key="d1">category</data>
  <data key="d2">Pieces of pie refer to the individual servings cut from the original pies for guests.</data>
  <data key="d3">chunk-3ca398087b4d5dd48a0c1fe169e9b7fd</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Total Pieces">
  <data key="d0">Total Pieces</data>
  <data key="d1">category</data>
  <data key="d2">Total pieces refer to the calculated number of pie pieces available for guests, which is 40.</data>
  <data key="d3">chunk-3ca398087b4d5dd48a0c1fe169e9b7fd</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Remaining Pieces">
  <data key="d0">Remaining Pieces</data>
  <data key="d1">category</data>
  <data key="d2">Remaining pieces refer to the number of pie pieces left after guests have eaten, which is 14.</data>
  <data key="d3">chunk-3ca398087b4d5dd48a0c1fe169e9b7fd</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Eaten Pieces">
  <data key="d0">Eaten Pieces</data>
  <data key="d1">category</data>
  <data key="d2">Eaten pieces refer to the total number of pie pieces consumed by guests, which is calculated as 26.</data>
  <data key="d3">chunk-3ca398087b4d5dd48a0c1fe169e9b7fd</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Event of Pie Serving">
  <data key="d0">Event of Pie Serving</data>
  <data key="d1">event</data>
  <data key="d2">The event involves Grandma Jones serving pies to guests, where they eat and enjoy the food prepared.</data>
  <data key="d3">chunk-3ca398087b4d5dd48a0c1fe169e9b7fd</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="GPT-4-V">
  <data key="d0">GPT-4-V</data>
  <data key="d1">organization</data>
  <data key="d2">GPT-4-V is an advanced AI model used for generating captions and processing images in a culturally sensitive manner.</data>
  <data key="d3">chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="LLaVA-1.6-Mistral-7B">
  <data key="d0">LLaVA-1.6-Mistral-7B</data>
  <data key="d1">organization</data>
  <data key="d2">LLaVA-1.6-Mistral-7B is a language model that was tested for its ability to understand and respond to cultural and mathematical queries in Japanese.&lt;SEP&gt;LLaVA-1.6-Mistral-7B is a source model used in experiments for generating visual language models.</data>
  <data key="d3">chunk-57491ddb5284500415bc46fc834a6343&lt;SEP&gt;chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Japanese Stable VLM">
  <data key="d0">Japanese Stable VLM</data>
  <data key="d1">organization</data>
  <data key="d2">Japanese Stable VLM is an initiative focused on developing a stable vision-language model for Japanese.&lt;SEP&gt;Japanese Stable VLM is an open-sourced visual language model trained on Japanese datasets.</data>
  <data key="d3">chunk-77432ea4d82a2648008d6d26c50ccdc9&lt;SEP&gt;chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="JA-VG-VQA-500">
  <data key="d0">JA-VG-VQA-500</data>
  <data key="d1">event</data>
  <data key="d2">JA-VG-VQA-500 is a benchmark dataset used to evaluate visual question answering abilities in Japanese.</data>
  <data key="d3">chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="JA-VLM-Bench-In-the-Wild">
  <data key="d0">JA-VLM-Bench-In-the-Wild</data>
  <data key="d1">event</data>
  <data key="d2">JA-VLM-Bench-In-the-Wild is a benchmark that assesses the performance of models on complex visual question answering tasks within Japanese cultural contexts.</data>
  <data key="d3">chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Fasttext">
  <data key="d0">Fasttext</data>
  <data key="d1">category</data>
  <data key="d2">Fasttext is a library used for language detection tasks in the experiments.</data>
  <data key="d3">chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Evolutionary Techniques">
  <data key="d0">Evolutionary Techniques</data>
  <data key="d1">category</data>
  <data key="d2">Evolutionary techniques are methods used to discover optimal ways to combine different models with diverse capabilities.</data>
  <data key="d3">chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Culturally-Specific Content">
  <data key="d0">Culturally-Specific Content</data>
  <data key="d1">category</data>
  <data key="d2">Culturally-specific content refers to material that is tailored to the cultural nuances and context of a particular group, in this case, Japanese.</data>
  <data key="d3">chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model Performance Comparison">
  <data key="d0">Model Performance Comparison</data>
  <data key="d1">event</data>
  <data key="d2">Model Performance Comparison refers to the evaluation of various models based on their performance metrics across different benchmarks.</data>
  <data key="d3">chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Evolutionary Model Merging">
  <data key="d0">Evolutionary Model Merging</data>
  <data key="d1">category</data>
  <data key="d2">Evolutionary Model Merging is a method that integrates various models from different domains to enhance capabilities and performance in AI applications.&lt;SEP&gt;Evolutionary model merging is a method that combines various AI models to create efficient and capable models, aiming to democratize AI access.</data>
  <data key="d3">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b&lt;SEP&gt;chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="EvoSDXL">
  <data key="d0">EvoSDXL</data>
  <data key="d1">organization</data>
  <data key="d2">EvoSDXL is an application of evolutionary model merging to diffusion image generation models, demonstrating the versatility of the method.</data>
  <data key="d3">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="SDXL-Lightning">
  <data key="d0">SDXL-Lightning</data>
  <data key="d1">organization</data>
  <data key="d2">SDXL-Lightning is a specialized variant of SDXL that uses adversarial loss for rapid image generation, enhancing the efficiency of diffusion models.</data>
  <data key="d3">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="EvoVLM-JP-v2">
  <data key="d0">EvoVLM-JP-v2</data>
  <data key="d1">organization</data>
  <data key="d2">EvoVLM-JP-v2 is a model developed using evolutionary model merging techniques, showcasing adaptability in AI development.</data>
  <data key="d3">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="EvoUkiyoe">
  <data key="d0">EvoUkiyoe</data>
  <data key="d1">organization</data>
  <data key="d2">EvoUkiyoe is another model produced through evolutionary model merging, highlighting the method's potential in various domains.</data>
  <data key="d3">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="MergeKit">
  <data key="d0">MergeKit</data>
  <data key="d1">organization</data>
  <data key="d2">MergeKit is an open-source software package that implements evolutionary model merging, making the technique accessible to a wider audience.</data>
  <data key="d3">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Optuna Hub">
  <data key="d0">Optuna Hub</data>
  <data key="d1">organization</data>
  <data key="d2">Optuna Hub is an open-source software package that incorporates evolutionary model merging, facilitating practical applications in AI.</data>
  <data key="d3">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="SLERP">
  <data key="d0">SLERP</data>
  <data key="d1">category</data>
  <data key="d2">SLERP is a technique used in model merging to interpolate between two different models, enhancing their combined performance.</data>
  <data key="d3">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Foundation Models">
  <data key="d0">Foundation Models</data>
  <data key="d1">category</data>
  <data key="d2">Foundation Models are large-scale models that serve as the basis for various AI applications and can be enhanced through evolutionary techniques.</data>
  <data key="d3">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Image Diffusion Models">
  <data key="d0">Image Diffusion Models</data>
  <data key="d1">category</data>
  <data key="d2">Image Diffusion Models are a class of generative models used to create images through iterative refinement.</data>
  <data key="d3">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Benchmark Tasks">
  <data key="d0">Benchmark Tasks</data>
  <data key="d1">category</data>
  <data key="d2">Benchmark Tasks are standardized tests used to evaluate the performance of models in various AI domains.</data>
  <data key="d3">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="LLM Experiments">
  <data key="d0">LLM Experiments</data>
  <data key="d1">event</data>
  <data key="d2">LLM Experiments refer to the various experiments conducted to optimize language models using specific datasets.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="VLM Experiments">
  <data key="d0">VLM Experiments</data>
  <data key="d1">event</data>
  <data key="d2">VLM Experiments involve the use of visual-language models and benchmark datasets for optimization and evaluation.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Hugging Face">
  <data key="d0">Hugging Face</data>
  <data key="d1">organization</data>
  <data key="d2">Hugging Face is a platform that hosts the Open LLM leaderboard and is involved in various AI and machine learning projects.&lt;SEP&gt;Hugging Face is a platform that provides datasets and tools for machine learning and AI research.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52&lt;SEP&gt;chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="GitHub">
  <data key="d0">GitHub</data>
  <data key="d1">organization</data>
  <data key="d2">GitHub is a code hosting platform that provides access to various open-source projects and datasets.&lt;SEP&gt;GitHub is a platform where the EvoLLM-JP-A model will be released for public access.</data>
  <data key="d3">chunk-96aa56aec4e4f18a590f28efaff42c99&lt;SEP&gt;chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Japanese Stable LM Beta">
  <data key="d0">Japanese Stable LM Beta</data>
  <data key="d1">event</data>
  <data key="d2">Japanese Stable LM Beta is a language model developed by Stability AI, aimed at enhancing Japanese language processing.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Evolutionary Optimization of Model Merging Recipes">
  <data key="d0">Evolutionary Optimization of Model Merging Recipes</data>
  <data key="d1">category</data>
  <data key="d2">Evolutionary Optimization of Model Merging Recipes is a project aimed at improving the integration of various AI models to enhance their capabilities and efficiency.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Parameter Space Model Merging">
  <data key="d0">Parameter Space Model Merging</data>
  <data key="d1">category</data>
  <data key="d2">Parameter Space Model Merging refers to a methodology developed within the project to combine different models by optimizing their parameters.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Vision-Language Models">
  <data key="d0">Vision-Language Models</data>
  <data key="d1">category</data>
  <data key="d2">Vision-Language Models are AI models designed to process and understand both visual and textual information, enhancing multimodal AI capabilities.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Diffusion Models">
  <data key="d0">Diffusion Models</data>
  <data key="d1">category</data>
  <data key="d2">Diffusion Models are a type of generative model used in machine learning, particularly in image synthesis and transformation tasks.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Neural Architecture Search">
  <data key="d0">Neural Architecture Search</data>
  <data key="d1">category</data>
  <data key="d2">Neural Architecture Search is a technique in AI that automates the design of neural networks to optimize performance.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Morphology Evolution Literature">
  <data key="d0">Morphology Evolution Literature</data>
  <data key="d1">category</data>
  <data key="d2">Morphology Evolution Literature refers to research and studies focused on the evolution of structures and forms in biological and computational contexts.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Environmental Footprint">
  <data key="d0">Environmental Footprint</data>
  <data key="d1">category</data>
  <data key="d2">Environmental Footprint measures the impact of AI development and deployment on the environment, emphasizing sustainability.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Ethical and Societal Impact">
  <data key="d0">Ethical and Societal Impact</data>
  <data key="d1">category</data>
  <data key="d2">Ethical and Societal Impact examines the implications of AI technologies on society, highlighting the need for responsible AI deployment.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model Evaluation">
  <data key="d0">Model Evaluation</data>
  <data key="d1">category</data>
  <data key="d2">Model Evaluation is the process of assessing the performance and reliability of AI models to ensure their effectiveness in applications.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Datasets">
  <data key="d0">Datasets</data>
  <data key="d1">category</data>
  <data key="d2">Datasets are collections of data used for training and evaluating AI models, critical for their development and performance assessment.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="OpenAI">
  <data key="d0">OpenAI</data>
  <data key="d1">organization</data>
  <data key="d2">OpenAI is an artificial intelligence research organization known for developing advanced AI models and technologies.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Stability AI">
  <data key="d0">Stability AI</data>
  <data key="d1">organization</data>
  <data key="d2">Stability AI is a company focused on developing artificial intelligence technologies, including language models and evaluation tools.&lt;SEP&gt;Stability AI is a company focused on developing artificial intelligence technologies, including language models.&lt;SEP&gt;Stability AI is an organization that focuses on creating stable and reliable AI models for various applications.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52&lt;SEP&gt;chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Emanuele Aiello">
  <data key="d0">Emanuele Aiello</data>
  <data key="d1">person</data>
  <data key="d2">Emanuele Aiello is a researcher involved in the development of large autoregressive multimodal models and has published works in the field of AI.&lt;SEP&gt;Emanuele Aiello is a researcher involved in the development of large autoregressive multimodal models.</data>
  <data key="d3">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="shisa-gamma-7b">
  <data key="d0">shisa-gamma-7b</data>
  <data key="d1">category</data>
  <data key="d2">Shisa-gamma-7b is a generative AI model available on HuggingFace, designed for various applications in natural language processing.&lt;SEP&gt;Shisa-gamma-7b is a generative AI model available on HuggingFace, developed for various applications.</data>
  <data key="d3">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Stable Diffusion WebUI">
  <data key="d0">Stable Diffusion WebUI</data>
  <data key="d1">category</data>
  <data key="d2">Stable Diffusion WebUI is a user interface for interacting with the Stable Diffusion model, facilitating its use in generating images.&lt;SEP&gt;Stable Diffusion WebUI is an interface that allows users to interact with the Stable Diffusion model for image generation.</data>
  <data key="d3">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Qwen-VL">
  <data key="d0">Qwen-VL</data>
  <data key="d1">category</data>
  <data key="d2">Qwen-VL is a versatile vision-language model designed for understanding and processing visual and textual information.&lt;SEP&gt;Qwen-VL is a versatile vision-language model that integrates visual and textual understanding for various applications.</data>
  <data key="d3">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Generative AI for Math: Abel">
  <data key="d0">Generative AI for Math: Abel</data>
  <data key="d1">category</data>
  <data key="d2">Generative AI for Math: Abel is a project aimed at applying generative AI technologies to solve mathematical problems.&lt;SEP&gt;Generative AI for Math: Abel is a project that applies generative AI techniques to solve mathematical problems.</data>
  <data key="d3">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model Merging by Uncertainty-Based Gradient Matching">
  <data key="d0">Model Merging by Uncertainty-Based Gradient Matching</data>
  <data key="d1">category</data>
  <data key="d2">Model Merging by Uncertainty-Based Gradient Matching is a research paper focusing on techniques for combining machine learning models effectively.&lt;SEP&gt;Model Merging by Uncertainty-Based Gradient Matching is a research paper focusing on techniques for merging machine learning models.</data>
  <data key="d3">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="InstructBLIP">
  <data key="d0">InstructBLIP</data>
  <data key="d1">category</data>
  <data key="d2">InstructBLIP is a project aimed at creating general-purpose vision-language models with instruction tuning capabilities.&lt;SEP&gt;InstructBLIP is a project aimed at developing general-purpose vision-language models with capabilities for instruction tuning and various applications.</data>
  <data key="d3">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Lili Yu">
  <data key="d0">Lili Yu</data>
  <data key="d1">person</data>
  <data key="d2">Lili Yu is a researcher contributing to the field of artificial intelligence, particularly in multimodal models.</data>
  <data key="d3">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Yixin Nie">
  <data key="d0">Yixin Nie</data>
  <data key="d1">person</data>
  <data key="d2">Yixin Nie is a researcher who has worked on large-scale AI models and their applications.</data>
  <data key="d3">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Armen Aghajanyan">
  <data key="d0">Armen Aghajanyan</data>
  <data key="d1">person</data>
  <data key="d2">Armen Aghajanyan is a researcher known for contributions to AI model training and evaluation.</data>
  <data key="d3">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Barlas Oguz">
  <data key="d0">Barlas Oguz</data>
  <data key="d1">person</data>
  <data key="d2">Barlas Oguz is a researcher involved in developing AI technologies and models.</data>
  <data key="d3">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="JP Language Model Evaluation Harness">
  <data key="d0">JP Language Model Evaluation Harness</data>
  <data key="d1">category</data>
  <data key="d2">The JP Language Model Evaluation Harness is a tool developed by Stability AI for evaluating Japanese language models.</data>
  <data key="d3">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Daniel M Roy">
  <data key="d0">Daniel M Roy</data>
  <data key="d1">person</data>
  <data key="d2">Daniel M Roy is an author who contributed to the field of deep learning, focusing on generalization bounds for neural networks.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Adam Gaier">
  <data key="d0">Adam Gaier</data>
  <data key="d1">person</data>
  <data key="d2">Adam Gaier is an author known for his work on weight agnostic neural networks, contributing to advances in neural information processing systems.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Leo Gao">
  <data key="d0">Leo Gao</data>
  <data key="d1">person</data>
  <data key="d2">Leo Gao is an author involved in developing frameworks for few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Mistral 7B">
  <data key="d0">Mistral 7B</data>
  <data key="d1">event</data>
  <data key="d2">Mistral 7B refers to a specific model or event related to advancements in language models, as referenced in the research.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="arXiv preprint">
  <data key="d0">arXiv preprint</data>
  <data key="d1">category</data>
  <data key="d2">arXiv preprint is a category of preprints that includes early research outputs in various fields, particularly in computer science and artificial intelligence.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Baber Abbasi">
  <data key="d0">Baber Abbasi</data>
  <data key="d1">person</data>
  <data key="d2">Baber Abbasi is an author who contributed to the research on few-shot language model evaluation, focusing on advancements in AI.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Stella Biderman">
  <data key="d0">Stella Biderman</data>
  <data key="d1">person</data>
  <data key="d2">Stella Biderman is an author involved in the development of frameworks for few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Sid Black">
  <data key="d0">Sid Black</data>
  <data key="d1">person</data>
  <data key="d2">Sid Black is an author who contributed to the framework for evaluating language models in the context of AI research.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Anthony DiPofi">
  <data key="d0">Anthony DiPofi</data>
  <data key="d1">person</data>
  <data key="d2">Anthony DiPofi is an author associated with the research on few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Charles Foster">
  <data key="d0">Charles Foster</data>
  <data key="d1">person</data>
  <data key="d2">Charles Foster is an author contributing to the field of AI, particularly in few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Laurence Golding">
  <data key="d0">Laurence Golding</data>
  <data key="d1">person</data>
  <data key="d2">Laurence Golding is an author involved in the development of frameworks for few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Jeffrey Hsu">
  <data key="d0">Jeffrey Hsu</data>
  <data key="d1">person</data>
  <data key="d2">Jeffrey Hsu is an author who contributed to the research on few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Alain Le Noac'h">
  <data key="d0">Alain Le Noac'h</data>
  <data key="d1">person</data>
  <data key="d2">Alain Le Noac'h is an author involved in the few-shot language model evaluation research.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Haonan Li">
  <data key="d0">Haonan Li</data>
  <data key="d1">person</data>
  <data key="d2">Haonan Li is an author contributing to advancements in few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Kyle McDonell">
  <data key="d0">Kyle McDonell</data>
  <data key="d1">person</data>
  <data key="d2">Kyle McDonell is an author associated with the few-shot language model evaluation research.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Niklas Muennighoff">
  <data key="d0">Niklas Muennighoff</data>
  <data key="d1">person</data>
  <data key="d2">Niklas Muennighoff is an author who contributed to the framework for few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Chris Ociepa">
  <data key="d0">Chris Ociepa</data>
  <data key="d1">person</data>
  <data key="d2">Chris Ociepa is an author involved in the research on few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Jason Phang">
  <data key="d0">Jason Phang</data>
  <data key="d1">person</data>
  <data key="d2">Jason Phang is an author contributing to advancements in AI through few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Laria Reynolds">
  <data key="d0">Laria Reynolds</data>
  <data key="d1">person</data>
  <data key="d2">Laria Reynolds is an author involved in the few-shot language model evaluation research.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Hailey Schoelkopf">
  <data key="d0">Hailey Schoelkopf</data>
  <data key="d1">person</data>
  <data key="d2">Hailey Schoelkopf is an author contributing to the research on few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Aviya Skowron">
  <data key="d0">Aviya Skowron</data>
  <data key="d1">person</data>
  <data key="d2">Aviya Skowron is an author associated with the few-shot language model evaluation framework.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Lintang Sutawika">
  <data key="d0">Lintang Sutawika</data>
  <data key="d1">person</data>
  <data key="d2">Lintang Sutawika is an author contributing to advancements in few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Eric Tang">
  <data key="d0">Eric Tang</data>
  <data key="d1">person</data>
  <data key="d2">Eric Tang is a researcher involved in machine learning and its evaluation, particularly in mathematical problem-solving.&lt;SEP&gt;Eric Tang is an author involved in the few-shot language model evaluation research.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Anish Thite">
  <data key="d0">Anish Thite</data>
  <data key="d1">person</data>
  <data key="d2">Anish Thite is an author who contributed to the framework for few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Ben Wang">
  <data key="d0">Ben Wang</data>
  <data key="d1">person</data>
  <data key="d2">Ben Wang is an author associated with the few-shot language model evaluation research.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Kevin Wang">
  <data key="d0">Kevin Wang</data>
  <data key="d1">person</data>
  <data key="d2">Kevin Wang is an author contributing to the advancements in AI through few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Andy Zou">
  <data key="d0">Andy Zou</data>
  <data key="d1">person</data>
  <data key="d2">Andy Zou is an author involved in the research on few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Charles O. Goddard">
  <data key="d0">Charles O. Goddard</data>
  <data key="d1">person</data>
  <data key="d2">Charles O. Goddard is also associated with the mergekit project, which is relevant to AI model development.&lt;SEP&gt;Charles O. Goddard is an author who developed the mergekit tool, contributing to AI research.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Mor Geva">
  <data key="d0">Mor Geva</data>
  <data key="d1">person</data>
  <data key="d2">Mor Geva is an author contributing to the understanding of transformer feed-forward layers in AI.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Avi Caciularu">
  <data key="d0">Avi Caciularu</data>
  <data key="d1">person</data>
  <data key="d2">Avi Caciularu is an author involved in research on transformer models and their applications.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Kevin Ro Wang">
  <data key="d0">Kevin Ro Wang</data>
  <data key="d1">person</data>
  <data key="d2">Kevin Ro Wang is an author contributing to advancements in transformer models in AI.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Yoav Goldberg">
  <data key="d0">Yoav Goldberg</data>
  <data key="d1">person</data>
  <data key="d2">Yoav Goldberg is an author who has published research on transformer models and their implications in AI.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Nikolaus Hansen">
  <data key="d0">Nikolaus Hansen</data>
  <data key="d1">person</data>
  <data key="d2">Nikolaus Hansen is an author known for his work on evolutionary computation and related algorithms.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Sepp Hochreiter">
  <data key="d0">Sepp Hochreiter</data>
  <data key="d1">person</data>
  <data key="d2">Sepp Hochreiter is an author known for his contributions to neural networks and optimization techniques.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Jürgen Schmidhuber">
  <data key="d0">Jürgen Schmidhuber</data>
  <data key="d1">person</data>
  <data key="d2">Jürgen Schmidhuber is an author recognized for his work in deep learning and neural networks.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Gabriel Ilharco">
  <data key="d0">Gabriel Ilharco</data>
  <data key="d1">person</data>
  <data key="d2">Gabriel Ilharco is a co-author in the research discussing model soups in machine learning.&lt;SEP&gt;Gabriel Ilharco is a researcher working on editing models and task arithmetic in machine learning contexts.&lt;SEP&gt;Gabriel Ilharco is an author contributing to research on editing models in AI.&lt;SEP&gt;Gabriel Ilharco is an author contributing to research on model soups in AI.&lt;SEP&gt;Gabriel Ilharco is involved in research on AI and natural language processing.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2&lt;SEP&gt;chunk-6dacb14670f826534e89e51fc52ea26f&lt;SEP&gt;chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36&lt;SEP&gt;chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Marco Tulio Ribeiro">
  <data key="d0">Marco Tulio Ribeiro</data>
  <data key="d1">person</data>
  <data key="d2">Marco Tulio Ribeiro is an author involved in research on model interpretability and editing in AI.&lt;SEP&gt;Marco Tulio Ribeiro is known for his work in interpretability and adversarial machine learning, contributing to model editing techniques.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Mitchell Wortsman">
  <data key="d0">Mitchell Wortsman</data>
  <data key="d1">person</data>
  <data key="d2">Mitchell Wortsman is a researcher contributing to AI and machine learning methodologies.&lt;SEP&gt;Mitchell Wortsman is a researcher focusing on adversarial techniques and model editing in machine learning.&lt;SEP&gt;Mitchell Wortsman is an author contributing to advancements in model editing techniques.&lt;SEP&gt;Mitchell Wortsman is an author who studied model averaging techniques in AI.&lt;SEP&gt;Mitchell Wortsman is an author of research related to model soups and fine-tuning techniques in machine learning.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2&lt;SEP&gt;chunk-6dacb14670f826534e89e51fc52ea26f&lt;SEP&gt;chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36&lt;SEP&gt;chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Suchin Gururangan">
  <data key="d0">Suchin Gururangan</data>
  <data key="d1">person</data>
  <data key="d2">Suchin Gururangan is a researcher involved in natural language processing and its applications in adversarial settings.&lt;SEP&gt;Suchin Gururangan is an author involved in research on AI model editing.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Ludwig Schmidt">
  <data key="d0">Ludwig Schmidt</data>
  <data key="d1">person</data>
  <data key="d2">Ludwig Schmidt is a researcher known for his work on machine learning interpretability and adversarial robustness.&lt;SEP&gt;Ludwig Schmidt is an author contributing to the research on model editing in AI.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Hannaneh Hajishirzi">
  <data key="d0">Hannaneh Hajishirzi</data>
  <data key="d1">person</data>
  <data key="d2">Hannaneh Hajishirzi is a prominent researcher in natural language processing, focusing on adversarial techniques and model evaluation.&lt;SEP&gt;Hannaneh Hajishirzi is an author known for her contributions to AI research and model interpretability.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Ali Farhadi">
  <data key="d0">Ali Farhadi</data>
  <data key="d1">person</data>
  <data key="d2">Ali Farhadi is a prominent researcher known for his work in computer vision and AI.&lt;SEP&gt;Ali Farhadi is a researcher contributing to machine learning advancements, specifically in model training.&lt;SEP&gt;Ali Farhadi is a researcher known for contributions to computer vision and machine learning, particularly in the context of adversarial models.&lt;SEP&gt;Ali Farhadi is an author involved in research on AI models and their applications.&lt;SEP&gt;Ali Farhadi is an author known for contributions to AI and machine learning.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2&lt;SEP&gt;chunk-6dacb14670f826534e89e51fc52ea26f&lt;SEP&gt;chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36&lt;SEP&gt;chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Neural Information Processing Systems">
  <data key="d0">Neural Information Processing Systems</data>
  <data key="d1">event</data>
  <data key="d2">Neural Information Processing Systems is a prominent conference series focusing on advancements in machine learning and artificial intelligence.</data>
  <data key="d3">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Nitish Shirish Keskar">
  <data key="d0">Nitish Shirish Keskar</data>
  <data key="d1">person</data>
  <data key="d2">Nitish Shirish Keskar is a researcher known for his work in deep learning and has contributed to significant publications in the field.</data>
  <data key="d3">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Dheevatsa Mudigere">
  <data key="d0">Dheevatsa Mudigere</data>
  <data key="d1">person</data>
  <data key="d2">Dheevatsa Mudigere is a researcher who co-authored works on large-batch training in deep learning.</data>
  <data key="d3">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Jorge Nocedal">
  <data key="d0">Jorge Nocedal</data>
  <data key="d1">person</data>
  <data key="d2">Jorge Nocedal is a prominent figure in optimization and machine learning, contributing to various research papers.</data>
  <data key="d3">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Mikhail Smelyanskiy">
  <data key="d0">Mikhail Smelyanskiy</data>
  <data key="d1">person</data>
  <data key="d2">Mikhail Smelyanskiy is a researcher in AI, recognized for his contributions to advancements in deep learning.</data>
  <data key="d3">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Ping Tak Peter Tang">
  <data key="d0">Ping Tak Peter Tang</data>
  <data key="d1">person</data>
  <data key="d2">Ping Tak Peter Tang is a researcher who has contributed to the field of deep learning and optimization.</data>
  <data key="d3">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Maxime Labonne">
  <data key="d0">Maxime Labonne</data>
  <data key="d1">person</data>
  <data key="d2">Maxime Labonne is a researcher known for his work on merging models and has published several papers in the AI field.</data>
  <data key="d3">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="International Conference on Learning Representations">
  <data key="d0">International Conference on Learning Representations</data>
  <data key="d1">event</data>
  <data key="d2">The International Conference on Learning Representations is a major conference that focuses on the latest research in representation learning and deep learning.</data>
  <data key="d3">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Hugging Face Blog">
  <data key="d0">Hugging Face Blog</data>
  <data key="d1">organization</data>
  <data key="d2">Hugging Face Blog is a platform that shares insights and developments in natural language processing and machine learning.</data>
  <data key="d3">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="arXiv">
  <data key="d0">arXiv</data>
  <data key="d1">organization</data>
  <data key="d2">arXiv is a repository for preprints in various fields including computer science, where researchers share their findings before formal publication.&lt;SEP&gt;arXiv is a repository for research papers in various scientific fields, including AI and machine learning.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f&lt;SEP&gt;chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="CoRR">
  <data key="d0">CoRR</data>
  <data key="d1">organization</data>
  <data key="d2">CoRR (Computing Research Repository) is an online repository for computer science research papers.</data>
  <data key="d3">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="WizardMath">
  <data key="d0">WizardMath</data>
  <data key="d1">category</data>
  <data key="d2">WizardMath refers to a category of research focused on enhancing mathematical reasoning capabilities in large language models.</data>
  <data key="d3">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Large-Batch Training">
  <data key="d0">Large-Batch Training</data>
  <data key="d1">category</data>
  <data key="d2">Large-batch training is a technique in deep learning that focuses on optimizing the training process by using larger batches of data.</data>
  <data key="d3">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Henning Petzka">
  <data key="d0">Henning Petzka</data>
  <data key="d1">person</data>
  <data key="d2">Henning Petzka is one of the authors contributing to the research on relative flatness and generalization in neural networks.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Michael Kamp">
  <data key="d0">Michael Kamp</data>
  <data key="d1">person</data>
  <data key="d2">Michael Kamp is a co-author of the paper discussing relative flatness and generalization in neural networks.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Linara Adilova">
  <data key="d0">Linara Adilova</data>
  <data key="d1">person</data>
  <data key="d2">Linara Adilova is a researcher involved in the study of relative flatness and generalization in neural networks.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Cristian Sminchisescu">
  <data key="d0">Cristian Sminchisescu</data>
  <data key="d1">person</data>
  <data key="d2">Cristian Sminchisescu is a co-author of the paper on relative flatness and generalization in neural networks.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Mario Boley">
  <data key="d0">Mario Boley</data>
  <data key="d1">person</data>
  <data key="d2">Mario Boley is a researcher who contributed to the study on relative flatness and generalization in neural networks.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Advances in Neural Information Processing Systems">
  <data key="d0">Advances in Neural Information Processing Systems</data>
  <data key="d1">organization</data>
  <data key="d2">Advances in Neural Information Processing Systems is a conference where significant research in neural networks is presented.&lt;SEP&gt;Advances in Neural Information Processing Systems is a conference where significant research papers, including those on language models and reinforcement learning, are presented.&lt;SEP&gt;This is a leading conference where significant advancements in neural information processing are presented, including research on model merging techniques.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36&lt;SEP&gt;chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Colin Raffel">
  <data key="d0">Colin Raffel</data>
  <data key="d1">person</data>
  <data key="d2">Colin Raffel is an author advocating for building machine learning models similarly to open-source software.&lt;SEP&gt;Colin Raffel is an author contributing to research on AI model updates.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f&lt;SEP&gt;chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Eric Raymond">
  <data key="d0">Eric Raymond</data>
  <data key="d1">person</data>
  <data key="d2">Eric Raymond is known for his influential work 'The Cathedral and the Bazaar' discussing software development methodologies.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="LM Benchmark">
  <data key="d0">LM Benchmark</data>
  <data key="d1">organization</data>
  <data key="d2">LM Benchmark is a research initiative focused on evaluating language models.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Kigali">
  <data key="d0">Kigali</data>
  <data key="d1">geo</data>
  <data key="d2">Kigali is the capital city of Rwanda and the host location for the ICLR 2023 conference.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="The Eleventh International Conference on Learning Representations">
  <data key="d0">The Eleventh International Conference on Learning Representations</data>
  <data key="d1">event</data>
  <data key="d2">The Eleventh International Conference on Learning Representations is a significant event in the field of machine learning held in Kigali, Rwanda.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Freda Shi">
  <data key="d0">Freda Shi</data>
  <data key="d1">person</data>
  <data key="d2">Freda Shi is a researcher who contributed to the paper discussing multilingual chain-of-thought reasoning in language models.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Mirac Suzgun">
  <data key="d0">Mirac Suzgun</data>
  <data key="d1">person</data>
  <data key="d2">Mirac Suzgun is a co-author of the paper on multilingual chain-of-thought reasoning in language models.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Markus Freitag">
  <data key="d0">Markus Freitag</data>
  <data key="d1">person</data>
  <data key="d2">Markus Freitag is one of the authors who worked on language models as multilingual chain-of-thought reasoners.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Xuezhi Wang">
  <data key="d0">Xuezhi Wang</data>
  <data key="d1">person</data>
  <data key="d2">Xuezhi Wang is a researcher involved in the study of multilingual reasoning in language models.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Suraj Srivats">
  <data key="d0">Suraj Srivats</data>
  <data key="d1">person</data>
  <data key="d2">Suraj Srivats is a contributor to the research on multilingual chain-of-thought reasoning in language models.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Soroush Vosoughi">
  <data key="d0">Soroush Vosoughi</data>
  <data key="d1">person</data>
  <data key="d2">Soroush Vosoughi is a co-author of the paper discussing multilingual reasoning in language models.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Hyung Won Chung">
  <data key="d0">Hyung Won Chung</data>
  <data key="d1">person</data>
  <data key="d2">Hyung Won Chung is a researcher who contributed to the multilingual chain-of-thought reasoning study.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Yi Tay">
  <data key="d0">Yi Tay</data>
  <data key="d1">person</data>
  <data key="d2">Yi Tay is one of the authors involved in the research on multilingual reasoning in language models.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Sebastian Ruder">
  <data key="d0">Sebastian Ruder</data>
  <data key="d1">person</data>
  <data key="d2">Sebastian Ruder is a researcher who contributed to the study on multilingual chain-of-thought reasoning in language models.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Denny Zhou">
  <data key="d0">Denny Zhou</data>
  <data key="d1">person</data>
  <data key="d2">Denny Zhou is a co-author of the research paper on multilingual reasoning in language models.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Dipanjan Das">
  <data key="d0">Dipanjan Das</data>
  <data key="d1">person</data>
  <data key="d2">Dipanjan Das is a contributor to the research on multilingual chain-of-thought reasoning in language models.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Jason Wei">
  <data key="d0">Jason Wei</data>
  <data key="d1">person</data>
  <data key="d2">Jason Wei is a researcher involved in the study of multilingual reasoning in language models.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Proceedings of the AAAI Conference on Artificial Intelligence">
  <data key="d0">Proceedings of the AAAI Conference on Artificial Intelligence</data>
  <data key="d1">event</data>
  <data key="d2">Proceedings of the AAAI Conference on Artificial Intelligence is a collection of research presented at the AAAI conference.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition">
  <data key="d0">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</data>
  <data key="d1">event</data>
  <data key="d2">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition is a compilation of research papers presented at the conference.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="International Conference on Computational Linguistics">
  <data key="d0">International Conference on Computational Linguistics</data>
  <data key="d1">event</data>
  <data key="d2">International Conference on Computational Linguistics is a conference where significant research in computational linguistics is presented.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Kenneth O Stanley">
  <data key="d0">Kenneth O Stanley</data>
  <data key="d1">person</data>
  <data key="d2">Kenneth O Stanley is an author known for his work in evolutionary computation and neural networks.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Risto Miikkulainen">
  <data key="d0">Risto Miikkulainen</data>
  <data key="d1">person</data>
  <data key="d2">Risto Miikkulainen is an author recognized for contributions in the field of neural networks and evolutionary algorithms.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Marc Pickett">
  <data key="d0">Marc Pickett</data>
  <data key="d1">person</data>
  <data key="d2">Marc Pickett is an author involved in research related to transformer models.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Aakash Kumar Nain">
  <data key="d0">Aakash Kumar Nain</data>
  <data key="d1">person</data>
  <data key="d2">Aakash Kumar Nain is an author contributing to the study of transformer layers in AI.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Llion Jones">
  <data key="d0">Llion Jones</data>
  <data key="d1">person</data>
  <data key="d2">Llion Jones is an author who has worked on topics related to transformer models in AI.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Yi-Lin Sung">
  <data key="d0">Yi-Lin Sung</data>
  <data key="d1">person</data>
  <data key="d2">Yi-Lin Sung is an author focusing on multimodal model merging in AI research.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Linjie Li">
  <data key="d0">Linjie Li</data>
  <data key="d1">person</data>
  <data key="d2">Linjie Li is an author contributing to research in multimodal models.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Kevin Lin">
  <data key="d0">Kevin Lin</data>
  <data key="d1">person</data>
  <data key="d2">Kevin Lin is an author involved in the empirical study of model merging in AI.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Zhe Gan">
  <data key="d0">Zhe Gan</data>
  <data key="d1">person</data>
  <data key="d2">Zhe Gan is an author who has researched multimodal model merging.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Mohit Bansal">
  <data key="d0">Mohit Bansal</data>
  <data key="d1">person</data>
  <data key="d2">Mohit Bansal is an author known for contributions in AI and multimodal models.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Lijuan Wang">
  <data key="d0">Lijuan Wang</data>
  <data key="d1">person</data>
  <data key="d2">Lijuan Wang is an author who has worked on multimodal model research.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Yingtao Tian">
  <data key="d0">Yingtao Tian</data>
  <data key="d1">person</data>
  <data key="d2">Yingtao Tian is an author contributing to neuroevolution research.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Tom White">
  <data key="d0">Tom White</data>
  <data key="d1">person</data>
  <data key="d2">Tom White is an author who has researched generative networks and sampling methods.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Samir Ya Gadre">
  <data key="d0">Samir Ya Gadre</data>
  <data key="d1">person</data>
  <data key="d2">Samir Ya Gadre is a contributor to the research on model soups and their impact on accuracy in machine learning.&lt;SEP&gt;Samir Ya Gadre is a researcher working on advancements in machine learning and AI.&lt;SEP&gt;Samir Ya Gadre is an author involved in advancing AI model accuracy.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-144f32db3d945848c593404dfe014f4f&lt;SEP&gt;chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Rebecca Roelofs">
  <data key="d0">Rebecca Roelofs</data>
  <data key="d1">person</data>
  <data key="d2">Rebecca Roelofs is a researcher contributing to AI methodologies and applications.&lt;SEP&gt;Rebecca Roelofs is an author who has worked on model optimization techniques.&lt;SEP&gt;Rebecca Roelofs is involved in the research on averaging weights of multiple fine-tuned models.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-144f32db3d945848c593404dfe014f4f&lt;SEP&gt;chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Raphael Gontijo-Lopes">
  <data key="d0">Raphael Gontijo-Lopes</data>
  <data key="d1">person</data>
  <data key="d2">Raphael Gontijo-Lopes is a researcher who co-authored the paper on model soups in machine learning.&lt;SEP&gt;Raphael Gontijo-Lopes is an author focusing on AI model performance.&lt;SEP&gt;Raphael Gontijo-Lopes is involved in research related to AI and machine learning.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-144f32db3d945848c593404dfe014f4f&lt;SEP&gt;chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Ari S Morcos">
  <data key="d0">Ari S Morcos</data>
  <data key="d1">person</data>
  <data key="d2">Ari S Morcos is a contributor to the research on improving model accuracy through model soups.&lt;SEP&gt;Ari S Morcos is a researcher focusing on AI systems and methodologies.&lt;SEP&gt;Ari S Morcos is an author who has researched AI model efficiency.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-144f32db3d945848c593404dfe014f4f&lt;SEP&gt;chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Hongseok Namkoong">
  <data key="d0">Hongseok Namkoong</data>
  <data key="d1">person</data>
  <data key="d2">Hongseok Namkoong is a co-author of research discussing advancements in machine learning models.&lt;SEP&gt;Hongseok Namkoong is a researcher contributing to advancements in AI technologies.&lt;SEP&gt;Hongseok Namkoong is an author contributing to AI model studies.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-144f32db3d945848c593404dfe014f4f&lt;SEP&gt;chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Yair Carmon">
  <data key="d0">Yair Carmon</data>
  <data key="d1">person</data>
  <data key="d2">Yair Carmon is a co-author of the research paper on model soups and machine learning.&lt;SEP&gt;Yair Carmon is an author involved in AI research and model evaluation.&lt;SEP&gt;Yair Carmon is involved in AI research, particularly in model evaluation and training.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-144f32db3d945848c593404dfe014f4f&lt;SEP&gt;chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Simon Kornblith">
  <data key="d0">Simon Kornblith</data>
  <data key="d1">person</data>
  <data key="d2">Simon Kornblith is a researcher focused on AI and machine learning methodologies.&lt;SEP&gt;Simon Kornblith is an author contributing to AI model performance research.&lt;SEP&gt;Simon Kornblith is involved in research on improving machine learning model accuracy.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-144f32db3d945848c593404dfe014f4f&lt;SEP&gt;chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Prateek Yadav">
  <data key="d0">Prateek Yadav</data>
  <data key="d1">person</data>
  <data key="d2">Prateek Yadav is an author focused on compression techniques for AI models.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Leshem Choshen">
  <data key="d0">Leshem Choshen</data>
  <data key="d1">person</data>
  <data key="d2">Leshem Choshen is an author involved in parameter-efficient updates in AI.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Derek Tam">
  <data key="d0">Derek Tam</data>
  <data key="d1">person</data>
  <data key="d2">Derek Tam is an author who has studied model merging and interference.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Chao Li">
  <data key="d0">Chao Li</data>
  <data key="d1">person</data>
  <data key="d2">Chao Li is an author contributing to AI model research and development.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Chengen Huang">
  <data key="d0">Chengen Huang</data>
  <data key="d1">person</data>
  <data key="d2">Chengen Huang is an author involved in AI research.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Ge Zhang">
  <data key="d0">Ge Zhang</data>
  <data key="d1">person</data>
  <data key="d2">Ge Zhang is an author contributing to advancements in AI models.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Guanwei Zhang">
  <data key="d0">Guanwei Zhang</data>
  <data key="d1">person</data>
  <data key="d2">Guanwei Zhang is an author known for work in AI model efficiency.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Heng Li">
  <data key="d0">Heng Li</data>
  <data key="d1">person</data>
  <data key="d2">Heng Li is an author involved in AI model studies and advancements.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Jiangcheng Zhu">
  <data key="d0">Jiangcheng Zhu</data>
  <data key="d1">person</data>
  <data key="d2">Jiangcheng Zhu is an author contributing to AI research.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Jianqun Chen">
  <data key="d0">Jianqun Chen</data>
  <data key="d1">person</data>
  <data key="d2">Jianqun Chen is an author focused on AI model development.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Jing Chang">
  <data key="d0">Jing Chang</data>
  <data key="d1">person</data>
  <data key="d2">Jing Chang is an author contributing to AI model research.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="International Conference on Machine Learning">
  <data key="d0">International Conference on Machine Learning</data>
  <data key="d1">event</data>
  <data key="d2">The International Conference on Machine Learning is a prominent event where research on machine learning is presented and discussed.&lt;SEP&gt;The International Conference on Machine Learning is a prominent event where researchers present advancements in machine learning and AI.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Advances in Neural Information Processing Systems 36">
  <data key="d0">Advances in Neural Information Processing Systems 36</data>
  <data key="d1">event</data>
  <data key="d2">Advances in Neural Information Processing Systems 36 is a conference focusing on neural information processing and AI advancements.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Evolving Neural Networks Through Augmenting Topologies">
  <data key="d0">Evolving Neural Networks Through Augmenting Topologies</data>
  <data key="d1">event</data>
  <data key="d2">Evolving Neural Networks Through Augmenting Topologies is a significant paper discussing advancements in neural networks and evolutionary computation.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Transformer Layers as Painters">
  <data key="d0">Transformer Layers as Painters</data>
  <data key="d1">event</data>
  <data key="d2">Transformer Layers as Painters is a research paper analyzing the application of transformer layers in AI models.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="An Empirical Study of Multimodal Model Merging">
  <data key="d0">An Empirical Study of Multimodal Model Merging</data>
  <data key="d1">event</data>
  <data key="d2">An Empirical Study of Multimodal Model Merging is a research paper that explores techniques for merging different AI models effectively.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="EvoJAX: Hardware-Accelerated Neuroevolution">
  <data key="d0">EvoJAX: Hardware-Accelerated Neuroevolution</data>
  <data key="d1">event</data>
  <data key="d2">EvoJAX: Hardware-Accelerated Neuroevolution is a paper focusing on enhancing neuroevolution techniques using hardware acceleration.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Sampling Generative Networks">
  <data key="d0">Sampling Generative Networks</data>
  <data key="d1">event</data>
  <data key="d2">Sampling Generative Networks is a research paper that discusses methods for improving generative networks in AI.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model Soups">
  <data key="d0">Model Soups</data>
  <data key="d1">event</data>
  <data key="d2">Model Soups is a paper that examines the concept of averaging weights from multiple fine-tuned AI models to enhance accuracy.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Compeft">
  <data key="d0">Compeft</data>
  <data key="d1">event</data>
  <data key="d2">Compeft is a research paper that presents techniques for communicating parameter-efficient updates in AI models.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Yi: Open Foundation Models by 01.ai">
  <data key="d0">Yi: Open Foundation Models by 01.ai</data>
  <data key="d1">event</data>
  <data key="d2">Yi: Open Foundation Models by 01.ai is a research paper that discusses the development of open foundation models in AI.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Language Models are Super Mario">
  <data key="d0">Language Models are Super Mario</data>
  <data key="d1">event</data>
  <data key="d2">Language Models are Super Mario is a paper exploring how language models can absorb capabilities from similar models.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Stability AI Japan">
  <data key="d0">Stability AI Japan</data>
  <data key="d1">organization</data>
  <data key="d2">Stability AI Japan is an organization that has developed a fork of the lm-eval-harness for evaluating language models.</data>
  <data key="d3">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="EvoLLM-JP">
  <data key="d0">EvoLLM-JP</data>
  <data key="d1">category</data>
  <data key="d2">EvoLLM-JP is a language model developed by merging various models to improve Japanese language understanding and performance.</data>
  <data key="d3">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="EvoLLM-JP-A">
  <data key="d0">EvoLLM-JP-A</data>
  <data key="d1">category</data>
  <data key="d2">EvoLLM-JP-A is a model derived from merging open-source models, released under the Apache 2.0 license.</data>
  <data key="d3">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Setsubun">
  <data key="d0">Setsubun</data>
  <data key="d1">event</data>
  <data key="d2">Setsubun is a Japanese cultural event that marks the day before the beginning of spring (Risshun).&lt;SEP&gt;Setsubun is a traditional Japanese event marking the beginning of spring, where people throw beans to drive away evil spirits and invite good fortune.&lt;SEP&gt;Setsubun is a traditional Japanese event that marks the eve of the first day of spring, celebrated with rituals to drive away evil spirits.&lt;SEP&gt;Setsubun is a traditional Japanese festival that marks the day before the beginning of spring, celebrated with rituals to drive away evil spirits.</data>
  <data key="d3">chunk-19ca32b4278ab8247ef043d83b7b65ef&lt;SEP&gt;chunk-96aa56aec4e4f18a590f28efaff42c99&lt;SEP&gt;chunk-57491ddb5284500415bc46fc834a6343&lt;SEP&gt;chunk-62a104ca37857a0566f04bc4cd794e8c</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="lm-eval-harness">
  <data key="d0">lm-eval-harness</data>
  <data key="d1">category</data>
  <data key="d2">lm-eval-harness is a framework used for evaluating the performance of language models, particularly in the context of Japanese language models.</data>
  <data key="d3">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Rinna">
  <data key="d0">Rinna</data>
  <data key="d1">organization</data>
  <data key="d2">Rinna is an organization that maintains leaderboards for language models, allowing for performance comparisons.</data>
  <data key="d3">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Japanese Language Model Evaluation Harness">
  <data key="d0">Japanese Language Model Evaluation Harness</data>
  <data key="d1">category</data>
  <data key="d2">Japanese Language Model Evaluation Harness is a benchmark suite consisting of tasks to evaluate Japanese language proficiency.</data>
  <data key="d3">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Llama-2-13b-hf">
  <data key="d0">Llama-2-13b-hf</data>
  <data key="d1">organization</data>
  <data key="d2">Llama-2-13b-hf is a 13B source model recognized for its performance metrics.</data>
  <data key="d3">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Swallow-70b-instruct-hf">
  <data key="d0">Swallow-70b-instruct-hf</data>
  <data key="d1">organization</data>
  <data key="d2">Swallow-70b-instruct-hf is a 70B model that demonstrates high performance metrics across various categories.&lt;SEP&gt;Swallow-70b-instruct-hf is a model designed for instruction tasks, showcasing high performance across various metrics.&lt;SEP&gt;Swallow-70b-instruct-hf is a model that demonstrates high performance metrics in various tasks.</data>
  <data key="d3">chunk-9644ad737be96b755c1c9bdea916b724&lt;SEP&gt;chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="7B source models">
  <data key="d0">7B source models</data>
  <data key="d1">category</data>
  <data key="d2">7B source models refer to a category of AI models characterized by their performance metrics and capabilities.</data>
  <data key="d3">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="7B merged models">
  <data key="d0">7B merged models</data>
  <data key="d1">category</data>
  <data key="d2">7B merged models encompass models that combine various 7B source models to enhance performance.</data>
  <data key="d3">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="13B source models">
  <data key="d0">13B source models</data>
  <data key="d1">category</data>
  <data key="d2">13B source models are a category of AI models distinguished by their larger size and performance metrics.</data>
  <data key="d3">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="13B merged models">
  <data key="d0">13B merged models</data>
  <data key="d1">category</data>
  <data key="d2">13B merged models consist of models that integrate multiple 13B source models for improved performance.</data>
  <data key="d3">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Swallow-70b-hf">
  <data key="d0">Swallow-70b-hf</data>
  <data key="d1">organization</data>
  <data key="d2">Swallow-70b-hf is a model version that showcases competitive performance scores.&lt;SEP&gt;Swallow-70b-hf is a variant model that demonstrates competitive performance in several evaluations.</data>
  <data key="d3">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="japanese-stablelm-base-beta-70b">
  <data key="d0">japanese-stablelm-base-beta-70b</data>
  <data key="d1">organization</data>
  <data key="d2">japanese-stablelm-base-beta-70b is a model that combines features from multiple configurations for effective performance.&lt;SEP&gt;japanese-stablelm-base-beta-70b is a model that combines features from multiple configurations to achieve effective performance.</data>
  <data key="d3">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="nekomata-14b-instruction">
  <data key="d0">nekomata-14b-instruction</data>
  <data key="d1">organization</data>
  <data key="d2">nekomata-14b-instruction is a model focused on instruction-based tasks, yielding impressive performance metrics.&lt;SEP&gt;nekomata-14b-instruction is a model with specific instruction capabilities, yielding impressive performance metrics.</data>
  <data key="d3">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="japanese-stablelm-instruct-beta-70b">
  <data key="d0">japanese-stablelm-instruct-beta-70b</data>
  <data key="d1">organization</data>
  <data key="d2">japanese-stablelm-instruct-beta-70b is an advanced model designed for instruction-based tasks.&lt;SEP&gt;japanese-stablelm-instruct-beta-70b is an advanced model tailored for instruction tasks, demonstrating high effectiveness.</data>
  <data key="d3">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="nekomata-14b">
  <data key="d0">nekomata-14b</data>
  <data key="d1">organization</data>
  <data key="d2">nekomata-14b is a model that provides strong performance across various benchmarks and tasks.&lt;SEP&gt;nekomata-14b is a model that provides strong performance across various benchmarks.</data>
  <data key="d3">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="youri-7b-chat">
  <data key="d0">youri-7b-chat</data>
  <data key="d1">organization</data>
  <data key="d2">youri-7b-chat is a conversational model optimized for chat interactions, exhibiting high engagement.&lt;SEP&gt;youri-7b-chat is a conversational model optimized for chat interactions.</data>
  <data key="d3">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Llama-2-70b-hf">
  <data key="d0">Llama-2-70b-hf</data>
  <data key="d1">organization</data>
  <data key="d2">Llama-2-70b-hf is a model that integrates various capabilities for enhanced performance across tasks.&lt;SEP&gt;Llama-2-70b-hf is a model that integrates various capabilities for high performance.</data>
  <data key="d3">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="youri-7b-instruction">
  <data key="d0">youri-7b-instruction</data>
  <data key="d1">organization</data>
  <data key="d2">youri-7b-instruction is a model specifically designed for instruction-based tasks, achieving notable performance scores.&lt;SEP&gt;youri-7b-instruction is a model tailored for instruction-based tasks with notable performance scores.</data>
  <data key="d3">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Qwen-14B">
  <data key="d0">Qwen-14B</data>
  <data key="d1">organization</data>
  <data key="d2">Qwen-14B is a model that demonstrates competitive performance metrics in various tasks.&lt;SEP&gt;Qwen-14B is a model that showcases competitive performance metrics across different evaluations and tasks.</data>
  <data key="d3">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Swallow-MX-8x7b-NVE-v0.1">
  <data key="d0">Swallow-MX-8x7b-NVE-v0.1</data>
  <data key="d1">organization</data>
  <data key="d2">Swallow-MX-8x7b-NVE-v0.1 is a model that highlights high performance in specific evaluations and tasks.&lt;SEP&gt;Swallow-MX-8x7b-NVE-v0.1 is a model that showcases high performance in specific evaluations.</data>
  <data key="d3">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="youri-7b-chat-gptq">
  <data key="d0">youri-7b-chat-gptq</data>
  <data key="d1">organization</data>
  <data key="d2">youri-7b-chat-gptq is a variant of the youri model optimized for chat interactions.&lt;SEP&gt;youri-7b-chat-gptq is an enhanced variant of the youri model optimized for chat interactions with advanced capabilities.</data>
  <data key="d3">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="nekomata-14b-instruction-gguf">
  <data key="d0">nekomata-14b-instruction-gguf</data>
  <data key="d1">organization</data>
  <data key="d2">nekomata-14b-instruction-gguf is an enhanced model version focusing on instruction tasks.&lt;SEP&gt;nekomata-14b-instruction-gguf is an upgraded model focusing on instruction tasks with impressive performance metrics.</data>
  <data key="d3">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Llama-2-70b-chat-hf">
  <data key="d0">Llama-2-70b-chat-hf</data>
  <data key="d1">organization</data>
  <data key="d2">Llama-2-70b-chat-hf is a conversational model designed for effective chat interactions.&lt;SEP&gt;Llama-2-70b-chat-hf is a conversational model tailored for effective chat interactions, demonstrating high engagement.&lt;SEP&gt;Llama-2-70b-chat-hf is a large language model developed with 70 billion parameters, known for its performance in conversational AI and natural language processing tasks.&lt;SEP&gt;Llama-2-70b-chat-hf is a large language model with 70 billion parameters, known for its high performance in various natural language processing tasks.</data>
  <data key="d3">chunk-9644ad737be96b755c1c9bdea916b724&lt;SEP&gt;chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="youri-7b-instruction-gptq">
  <data key="d0">youri-7b-instruction-gptq</data>
  <data key="d1">organization</data>
  <data key="d2">youri-7b-instruction-gptq is a language model with 7 billion parameters, designed for instruction-based tasks with notable accuracy.&lt;SEP&gt;youri-7b-instruction-gptq is a language model with 7 billion parameters, optimized for instructional tasks, demonstrating high accuracy in following user prompts.</data>
  <data key="d3">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="japanese-stablelm-base-gamma-7b">
  <data key="d0">japanese-stablelm-base-gamma-7b</data>
  <data key="d1">organization</data>
  <data key="d2">japanese-stablelm-base-gamma-7b is a Japanese language model with 7 billion parameters, optimized for language understanding and generation tasks.&lt;SEP&gt;japanese-stablelm-base-gamma-7b is a language model with 7 billion parameters, specifically designed for understanding and generating Japanese text.</data>
  <data key="d3">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Swallow-13b-instruct-hf">
  <data key="d0">Swallow-13b-instruct-hf</data>
  <data key="d1">organization</data>
  <data key="d2">Swallow-13b-instruct-hf is a hybrid language model with 13 billion parameters, integrating instruction-following capabilities for enhanced usability in applications.&lt;SEP&gt;Swallow-13b-instruct-hf is a hybrid model combining 13 billion parameters and instruction-following capabilities, enhancing its usability in various applications.</data>
  <data key="d3">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="nekomata-14b-gguf">
  <data key="d0">nekomata-14b-gguf</data>
  <data key="d1">organization</data>
  <data key="d2">nekomata-14b-gguf is a language model with 14 billion parameters, recognized for its ability to generate coherent and contextually relevant text.&lt;SEP&gt;nekomata-14b-gguf is a language model with 14 billion parameters, recognized for its performance in generating coherent text.</data>
  <data key="d3">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Swallow-MS-7b-v0.1">
  <data key="d0">Swallow-MS-7b-v0.1</data>
  <data key="d1">organization</data>
  <data key="d2">Swallow-MS-7b-v0.1 is a model with 7 billion parameters, focused on multi-task learning and instruction following.&lt;SEP&gt;Swallow-MS-7b-v0.1 is a multi-task language model with 7 billion parameters, designed for instruction following and various natural language tasks.</data>
  <data key="d3">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Swallow-7b-instruct-hf">
  <data key="d0">Swallow-7b-instruct-hf</data>
  <data key="d1">organization</data>
  <data key="d2">Swallow-7b-instruct-hf is a language model with 7 billion parameters, designed for instruction-based interactions.&lt;SEP&gt;Swallow-7b-instruct-hf is a language model with 7 billion parameters, focused on instruction-based interactions to improve user experience.</data>
  <data key="d3">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="japanese-stablelm-instruct-beta-7b">
  <data key="d0">japanese-stablelm-instruct-beta-7b</data>
  <data key="d1">organization</data>
  <data key="d2">japanese-stablelm-instruct-beta-7b is a Japanese language model with 7 billion parameters, aimed at improving instruction-based tasks.&lt;SEP&gt;japanese-stablelm-instruct-beta-7b is a language model with 7 billion parameters, aimed at improving performance in instruction-based tasks in Japanese.</data>
  <data key="d3">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Qwen-7B">
  <data key="d0">Qwen-7B</data>
  <data key="d1">organization</data>
  <data key="d2">Qwen-7B is a language model with 7 billion parameters, known for its efficiency and performance in diverse language tasks.&lt;SEP&gt;Qwen-7B is a language model with 7 billion parameters, known for its efficiency in executing diverse language tasks and applications.</data>
  <data key="d3">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="youri-7b-gptq">
  <data key="d0">youri-7b-gptq</data>
  <data key="d1">organization</data>
  <data key="d2">youri-7b-gptq is a 7 billion parameter model designed for generating text based on prompts.&lt;SEP&gt;youri-7b-gptq is a 7 billion parameter model that generates text based on user prompts, showcasing capabilities in conversational AI.&lt;SEP&gt;youri-7b-gptq is a model characterized by its performance metrics, including various scores across different categories.</data>
  <data key="d3">chunk-de4fd08ec74cc6d0a33238a38b1bdb52&lt;SEP&gt;chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="youri-7b">
  <data key="d0">youri-7b</data>
  <data key="d1">organization</data>
  <data key="d2">youri-7b is a language model with 7 billion parameters, utilized for various natural language processing applications and tasks.&lt;SEP&gt;youri-7b is a language model with 7 billion parameters, utilized for various natural language processing applications.&lt;SEP&gt;youri-7b is a model with performance metrics that indicate its capabilities in various tasks.</data>
  <data key="d3">chunk-de4fd08ec74cc6d0a33238a38b1bdb52&lt;SEP&gt;chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0">
  <data key="d0">llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0</data>
  <data key="d1">organization</data>
  <data key="d2">llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0 is a 13 billion parameter model designed for instruction-following tasks in Japanese language processing.</data>
  <data key="d3">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="ELYZA-japanese-Llama-2-7b-instruct">
  <data key="d0">ELYZA-japanese-Llama-2-7b-instruct</data>
  <data key="d1">organization</data>
  <data key="d2">ELYZA-japanese-Llama-2-7b-instruct is a model known for its instruction-based performance across multiple metrics.</data>
  <data key="d3">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="ELYZA-japanese-Llama-2-7b">
  <data key="d0">ELYZA-japanese-Llama-2-7b</data>
  <data key="d1">organization</data>
  <data key="d2">ELYZA-japanese-Llama-2-7b is a model with specific performance scores that highlight its capabilities.</data>
  <data key="d3">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="nekomata-7b-gguf">
  <data key="d0">nekomata-7b-gguf</data>
  <data key="d1">organization</data>
  <data key="d2">nekomata-7b-gguf is a model with performance metrics that reflect its efficiency and effectiveness.</data>
  <data key="d3">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="japanese-stablelm-instruct-ja_vocab-beta-7b">
  <data key="d0">japanese-stablelm-instruct-ja_vocab-beta-7b</data>
  <data key="d1">organization</data>
  <data key="d2">japanese-stablelm-instruct-ja_vocab-beta-7b is a model designed for instruction tasks with notable performance scores.</data>
  <data key="d3">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="japanese-stablelm-base-ja_vocab-beta-7b">
  <data key="d0">japanese-stablelm-base-ja_vocab-beta-7b</data>
  <data key="d1">organization</data>
  <data key="d2">japanese-stablelm-base-ja_vocab-beta-7b is a model that exhibits performance across various parameters.</data>
  <data key="d3">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="stockmark-13b">
  <data key="d0">stockmark-13b</data>
  <data key="d1">organization</data>
  <data key="d2">stockmark-13b is a model characterized by its performance metrics in various categories.</data>
  <data key="d3">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Llama-2-7b-hf">
  <data key="d0">Llama-2-7b-hf</data>
  <data key="d1">organization</data>
  <data key="d2">Llama-2-7b-hf is a model noted for its performance scores in different tasks.</data>
  <data key="d3">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="calm2-7b">
  <data key="d0">calm2-7b</data>
  <data key="d1">organization</data>
  <data key="d2">calm2-7b is a model with performance metrics that indicate its capabilities in various areas.</data>
  <data key="d3">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="plamo-13b">
  <data key="d0">plamo-13b</data>
  <data key="d1">organization</data>
  <data key="d2">plamo-13b is a model with performance scores across several categories, indicating its capabilities.</data>
  <data key="d3">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="llm-jp-13b-v1.0">
  <data key="d0">llm-jp-13b-v1.0</data>
  <data key="d1">organization</data>
  <data key="d2">llm-jp-13b-v1.0 is a model characterized by its performance metrics.</data>
  <data key="d3">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="plamo-13b-instruct-nc">
  <data key="d0">plamo-13b-instruct-nc</data>
  <data key="d1">organization</data>
  <data key="d2">plamo-13b-instruct-nc is a model designed for instruction tasks with specific performance metrics.</data>
  <data key="d3">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="plamo-13b-instruct">
  <data key="d0">plamo-13b-instruct</data>
  <data key="d1">organization</data>
  <data key="d2">plamo-13b-instruct is a model with performance metrics that reflect its capabilities in instruction tasks.</data>
  <data key="d3">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="EvoLLM-JP-v1-7B">
  <data key="d0">EvoLLM-JP-v1-7B</data>
  <data key="d1">organization</data>
  <data key="d2">EvoLLM-JP-v1-7B is a language model that was tested for its ability to solve mathematical problems and understand cultural nuances in Japanese.</data>
  <data key="d3">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Koi-nobori">
  <data key="d0">Koi-nobori</data>
  <data key="d1">category</data>
  <data key="d2">Koi-nobori is a cultural tradition in Japan involving carp streamers that signify the celebration of Children's Day.</data>
  <data key="d3">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Japanese Culture">
  <data key="d0">Japanese Culture</data>
  <data key="d1">category</data>
  <data key="d2">Japanese Culture encompasses various traditions, customs, and practices unique to Japan, influencing language and understanding.</data>
  <data key="d3">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Traffic Lights">
  <data key="d0">Traffic Lights</data>
  <data key="d1">category</data>
  <data key="d2">Traffic Lights are signaling devices positioned at road intersections to control vehicle and pedestrian traffic.</data>
  <data key="d3">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Risshun">
  <data key="d0">Risshun</data>
  <data key="d1">event</data>
  <data key="d2">Risshun is the beginning of spring according to the lunisolar Japanese calendar, which coincides with Setsubun.&lt;SEP&gt;Risshun refers to the first day of spring in the Japanese lunar calendar, which varies each year.</data>
  <data key="d3">chunk-19ca32b4278ab8247ef043d83b7b65ef&lt;SEP&gt;chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Japanese-Stable-VLM">
  <data key="d0">Japanese-Stable-VLM</data>
  <data key="d1">organization</data>
  <data key="d2">Japanese-Stable-VLM is a language model that demonstrates knowledge of Japanese culture and language nuances.</data>
  <data key="d3">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="EvoVLM-JP">
  <data key="d0">EvoVLM-JP</data>
  <data key="d1">organization</data>
  <data key="d2">EvoVLM-JP is a language model that shows fluency in Japanese expression and cultural understanding.</data>
  <data key="d3">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="MGSM Test Set">
  <data key="d0">MGSM Test Set</data>
  <data key="d1">category</data>
  <data key="d2">MGSM Test Set is a collection of questions used to evaluate the performance of language models in solving mathematical problems.</data>
  <data key="d3">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="February">
  <data key="d0">February</data>
  <data key="d1">geo</data>
  <data key="d2">February is the second month of the year, during which Setsubun is celebrated.</data>
  <data key="d3">chunk-19ca32b4278ab8247ef043d83b7b65ef</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="2021">
  <data key="d0">2021</data>
  <data key="d1">category</data>
  <data key="d2">2021 is the year when last year's Setsubun was celebrated on February 3.</data>
  <data key="d3">chunk-19ca32b4278ab8247ef043d83b7b65ef</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="2022">
  <data key="d0">2022</data>
  <data key="d1">category</data>
  <data key="d2">2022 is the year when this year's Setsubun is celebrated on February 4.</data>
  <data key="d3">chunk-19ca32b4278ab8247ef043d83b7b65ef</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="2023">
  <data key="d0">2023</data>
  <data key="d1">category</data>
  <data key="d2">2023 is the year when the combined date of last year's and this year's Setsubun is calculated to be February 3.</data>
  <data key="d3">chunk-19ca32b4278ab8247ef043d83b7b65ef</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Last Year's Setsubun">
  <data key="d0">Last Year's Setsubun</data>
  <data key="d1">event</data>
  <data key="d2">Last year's Setsubun was celebrated on February 2, 2021, marking the end of winter and the beginning of spring.</data>
  <data key="d3">chunk-19ca32b4278ab8247ef043d83b7b65ef</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="This Year's Setsubun">
  <data key="d0">This Year's Setsubun</data>
  <data key="d1">event</data>
  <data key="d2">This year's Setsubun is celebrated on February 3, 2022, as part of the same annual tradition.</data>
  <data key="d3">chunk-19ca32b4278ab8247ef043d83b7b65ef</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="John">
  <data key="d0">John</data>
  <data key="d1">person</data>
  <data key="d2">John is a character who possesses three boxes and calculates their total inner volume.&lt;SEP&gt;John is a character who possesses three boxes with specific dimensions and is involved in a mathematical problem regarding their volume.</data>
  <data key="d3">chunk-30dbd5c2c163db07de6f5e159458d713&lt;SEP&gt;chunk-62a104ca37857a0566f04bc4cd794e8c</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Boxes">
  <data key="d0">Boxes</data>
  <data key="d1">category</data>
  <data key="d2">The boxes are categorized as containers with dimensions of 5 inches by 6 inches by 4 inches, with walls that are 1 inch thick.&lt;SEP&gt;The boxes are three containers with specific dimensions that John is measuring for volume.</data>
  <data key="d3">chunk-30dbd5c2c163db07de6f5e159458d713&lt;SEP&gt;chunk-62a104ca37857a0566f04bc4cd794e8c</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Volume Calculation">
  <data key="d0">Volume Calculation</data>
  <data key="d1">event</data>
  <data key="d2">Volume Calculation refers to the mathematical process of determining the total inner volume of the boxes, considering their dimensions and wall thickness.</data>
  <data key="d3">chunk-62a104ca37857a0566f04bc4cd794e8c</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Volume">
  <data key="d0">Volume</data>
  <data key="d1">category</data>
  <data key="d2">Volume is a mathematical concept that refers to the amount of space that a substance or object occupies, measured in cubic units.</data>
  <data key="d3">chunk-62a104ca37857a0566f04bc4cd794e8c</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Cubic Inches">
  <data key="d0">Cubic Inches</data>
  <data key="d1">category</data>
  <data key="d2">Cubic inches is a unit of measurement for volume, commonly used in the United States to measure the capacity of containers.</data>
  <data key="d3">chunk-62a104ca37857a0566f04bc4cd794e8c</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Dimensions">
  <data key="d0">Dimensions</data>
  <data key="d1">category</data>
  <data key="d2">Dimensions refer to the measurements of an object, including length, width, and height, which are essential for calculating volume.&lt;SEP&gt;Dimensions refer to the measurements of length, width, and height that define the size of the box.</data>
  <data key="d3">chunk-30dbd5c2c163db07de6f5e159458d713&lt;SEP&gt;chunk-62a104ca37857a0566f04bc4cd794e8c</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Wall Thickness">
  <data key="d0">Wall Thickness</data>
  <data key="d1">category</data>
  <data key="d2">Wall Thickness refers to the measurement of the thickness of the walls of the boxes, which affects the internal volume calculation.</data>
  <data key="d3">chunk-62a104ca37857a0566f04bc4cd794e8c</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="72 cubic inches">
  <data key="d0">72 cubic inches</data>
  <data key="d1">event</data>
  <data key="d2">The total inner volume calculated by John for the three boxes is 72 cubic inches.</data>
  <data key="d3">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Box">
  <data key="d0">Box</data>
  <data key="d1">category</data>
  <data key="d2">A box is a three-dimensional container used for storage or transportation, in this case, having specific dimensions and wall thickness.</data>
  <data key="d3">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Inner Volume">
  <data key="d0">Inner Volume</data>
  <data key="d1">category</data>
  <data key="d2">Inner volume is the space inside the box calculated by multiplying its internal dimensions.</data>
  <data key="d3">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="1 inch">
  <data key="d0">1 inch</data>
  <data key="d1">category</data>
  <data key="d2">1 inch is a unit of measurement used to denote the thickness of the walls of the box.</data>
  <data key="d3">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="5 inches">
  <data key="d0">5 inches</data>
  <data key="d1">category</data>
  <data key="d2">5 inches is the original length measurement of the box before accounting for wall thickness.</data>
  <data key="d3">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="6 inches">
  <data key="d0">6 inches</data>
  <data key="d1">category</data>
  <data key="d2">6 inches is the original width measurement of the box before accounting for wall thickness.</data>
  <data key="d3">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="4 inches">
  <data key="d0">4 inches</data>
  <data key="d1">category</data>
  <data key="d2">4 inches is the original height measurement of the box before accounting for wall thickness.</data>
  <data key="d3">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="3 boxes">
  <data key="d0">3 boxes</data>
  <data key="d1">category</data>
  <data key="d2">Three boxes are referenced in the text, indicating the total quantity John is measuring for volume.</data>
  <data key="d3">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Peace Tower">
  <data key="d0">Peace Tower</data>
  <data key="d1">organization</data>
  <data key="d2">The Peace Tower is a building located in Shibuya Ward, Tokyo, Japan, symbolizing peace and visited by many for prayer.</data>
  <data key="d3">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="August 15, 1945">
  <data key="d0">August 15, 1945</data>
  <data key="d1">event</data>
  <data key="d2">August 15, 1945, marks the day when the Peace Tower was destroyed during World War II by the Japanese military.</data>
  <data key="d3">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="1964">
  <data key="d0">1964</data>
  <data key="d1">event</data>
  <data key="d2">The year 1964 signifies when the Peace Tower was reconstructed after its destruction during World War II.</data>
  <data key="d3">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="World War II">
  <data key="d0">World War II</data>
  <data key="d1">category</data>
  <data key="d2">World War II was a global conflict that involved many countries and resulted in significant historical events, including the atomic bombings of Hiroshima and Nagasaki.&lt;SEP&gt;World War II was a global conflict that lasted from 1939 to 1945, significantly impacting many nations, including Japan.</data>
  <data key="d3">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Hiroshima">
  <data key="d0">Hiroshima</data>
  <data key="d1">geo</data>
  <data key="d2">Hiroshima is a city in Japan that was the target of an atomic bomb during World War II and is known for its historical significance.</data>
  <data key="d3">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Atomic Bomb Dome">
  <data key="d0">Atomic Bomb Dome</data>
  <data key="d1">organization</data>
  <data key="d2">The Atomic Bomb Dome is a historical building in Hiroshima, Japan, that remains as a symbol of the destruction caused by the atomic bomb during World War II.</data>
  <data key="d3">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Hiroshima Prefectural Industrial Promotion Hall">
  <data key="d0">Hiroshima Prefectural Industrial Promotion Hall</data>
  <data key="d1">organization</data>
  <data key="d2">The Hiroshima Prefectural Industrial Promotion Hall is the original name of the building now known as the Atomic Bomb Dome, which was damaged during the atomic bombing.</data>
  <data key="d3">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="August 6, 1945">
  <data key="d0">August 6, 1945</data>
  <data key="d1">event</data>
  <data key="d2">August 6, 1945, is the date when an atomic bomb was dropped on Hiroshima, leading to widespread destruction and loss of life.</data>
  <data key="d3">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="UNESCO World Heritage Site">
  <data key="d0">UNESCO World Heritage Site</data>
  <data key="d1">category</data>
  <data key="d2">A UNESCO World Heritage Site is a landmark or area with legal protection by an international convention, recognized for its cultural or historical significance, such as the Atomic Bomb Dome.</data>
  <data key="d3">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="fine-tuning">
  <data key="d0">fine-tuning</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d2">Fine-tuning often led to significant decreases in JP-LMEH scores, indicating its impact on model performance.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Alex Young">
  <data key="d0">Alex Young</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d2">Alex Young is an author of Yi: Open Foundation Models by 01.ai, indicating his contribution to the research.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Le Yu">
  <data key="d0">Le Yu</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d2">Le Yu is an author of Language Models are Super Mario, indicating his involvement in the research.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Yue Zhou">
  <data key="d0">Yue Zhou</data>
  <data key="d1">person</data>
  <data key="d2">Yue Zhou is a researcher affiliated with the School of Artificial Intelligence at Jilin University, contributing to advancements in model merging techniques.</data>
  <data key="d3">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Yi Chang">
  <data key="d0">Yi Chang</data>
  <data key="d1">person</data>
  <data key="d2">Yi Chang is a contributor to the research on data selection at scale in machine learning.&lt;SEP&gt;Yi Chang is a researcher associated with the Engineering Research Center of Knowledge-Driven Human-Machine Intelligence, contributing to the study of model merging.&lt;SEP&gt;Yi Chang is a researcher working on machine learning and natural language processing, contributing to the development of evaluation datasets.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-3df998a85b2dff618feb2e2ce206b62c&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Yuan Wu">
  <data key="d0">Yuan Wu</data>
  <data key="d1">person</data>
  <data key="d2">Yuan Wu is a co-author of the paper discussing bias-alleviating low-rank adaptation.&lt;SEP&gt;Yuan Wu is a researcher engaged in the evaluation of large language models, particularly in health-related applications.&lt;SEP&gt;Yuan Wu is a researcher involved in the study of data selection methods in machine learning.&lt;SEP&gt;Yuan Wu is a researcher involved in the study of model merging and is affiliated with Jilin University.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-3df998a85b2dff618feb2e2ce206b62c&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36&lt;SEP&gt;chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jilin University">
  <data key="d0">Jilin University</data>
  <data key="d1">organization</data>
  <data key="d2">Jilin University is an educational institution in China, known for its research in artificial intelligence and related fields.</data>
  <data key="d3">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Engineering Research Center of Knowledge-Driven Human-Machine Intelligence">
  <data key="d0">Engineering Research Center of Knowledge-Driven Human-Machine Intelligence</data>
  <data key="d1">organization</data>
  <data key="d2">This center focuses on research integrating human and machine intelligence, contributing to advancements in AI technologies.</data>
  <data key="d3">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="International Center of Future Science">
  <data key="d0">International Center of Future Science</data>
  <data key="d1">organization</data>
  <data key="d2">The International Center of Future Science at Jilin University is dedicated to innovative research in various scientific domains.</data>
  <data key="d3">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Mixup Model Merge (M3)">
  <data key="d0">Mixup Model Merge (M3)</data>
  <data key="d1">category</data>
  <data key="d2">Mixup Model Merge (M3) is a novel method for merging the parameters of large language models, enhancing their performance and robustness.</data>
  <data key="d3">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Natural Language Processing (NLP)">
  <data key="d0">Natural Language Processing (NLP)</data>
  <data key="d1">category</data>
  <data key="d2">NLP is a field of artificial intelligence that focuses on the interaction between computers and human language, utilizing large language models for various tasks.</data>
  <data key="d3">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Large Language Models (LLMs)">
  <data key="d0">Large Language Models (LLMs)</data>
  <data key="d1">category</data>
  <data key="d2">LLMs are advanced AI models that have shown significant capabilities in understanding and generating human language.</data>
  <data key="d3">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Supervised Fine-Tuning (SFT)">
  <data key="d0">Supervised Fine-Tuning (SFT)</data>
  <data key="d1">category</data>
  <data key="d2">Supervised Fine-Tuning (SFT) is a technique used to adapt large language models to specific tasks through training on domain-specific data.</data>
  <data key="d3">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="M&lt;sup&gt;3&lt;/sup&gt;">
  <data key="d0">M&lt;sup&gt;3&lt;/sup&gt;</data>
  <data key="d1">category</data>
  <data key="d2">M&lt;sup&gt;3&lt;/sup&gt; is a method for merging language models by interpolating their parameters, enhancing their performance on various tasks.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; is a method that combines parameters from different models through linear interpolation to enhance performance across various tasks.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; is a model enhancement strategy that improves performance across various tasks and merging strategies.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; is a model merging technique integrated into various merging strategies for fine-tuned LLMs.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; is a novel approach for merging fine-tuned language models by introducing randomness into the parameter linear interpolation process.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; is a novel approach for merging fine-tuned language models, enhancing performance through dynamic parameter adjustment.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; is a proposed approach for improving the performance of merged models in machine learning, specifically addressing issues related to merging ratios and fine-tuning.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; is a proposed method aimed at improving the out-of-distribution (OOD) robustness and adversarial robustness of merged models in machine learning.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; refers to a model merging technique that integrates multiple models to enhance performance.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; stands for Mixup Model Merge, a novel technique that introduces randomness in model merging, enabling better exploration of parameter space and improving model performance.</data>
  <data key="d3">chunk-070652f4f36e86620289fcafaaa29599&lt;SEP&gt;chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff&lt;SEP&gt;chunk-c8ff785e8102dfde59dbc9309abb6b6a&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7&lt;SEP&gt;chunk-6c3c559e2fcc2c84ba368ddf5b7db117&lt;SEP&gt;chunk-47449b4eaab59a26168fa9d821973678&lt;SEP&gt;chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="WizardLM-13B">
  <data key="d0">WizardLM-13B</data>
  <data key="d1">organization</data>
  <data key="d2">WizardLM-13B is a fine-tuned LLM that specializes in instruction following and is part of the experiments conducted to validate M&lt;sup&gt;3&lt;/sup&gt;.&lt;SEP&gt;WizardLM-13B is a large language model designed for various task-specific applications in machine learning.&lt;SEP&gt;WizardLM-13B is a task-specific fine-tuned LLM designed for instruction-following tasks.&lt;SEP&gt;WizardLM-13B is an instruction-following model based on Llama-2-13b, aimed at improving performance in open-domain tasks.</data>
  <data key="d3">chunk-17c651f0ad506eb37b0f1742513e2650&lt;SEP&gt;chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="WizardMath-13B">
  <data key="d0">WizardMath-13B</data>
  <data key="d1">organization</data>
  <data key="d2">WizardMath-13B is a fine-tuned LLM focused on mathematical reasoning, utilized in the evaluation of the M&lt;sup&gt;3&lt;/sup&gt; method.&lt;SEP&gt;WizardMath-13B is a model optimized for mathematical reasoning, designed to enhance Chain-of-Thought capabilities.&lt;SEP&gt;WizardMath-13B is a model that is being merged with another model to enhance performance in code generation tasks.&lt;SEP&gt;WizardMath-13B is a specialized large language model focused on mathematical tasks and computations.&lt;SEP&gt;WizardMath-13B is a task-specific fine-tuned LLM designed for mathematical reasoning tasks.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7&lt;SEP&gt;chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="llama-2-13b-code-alpaca">
  <data key="d0">llama-2-13b-code-alpaca</data>
  <data key="d1">organization</data>
  <data key="d2">llama-2-13b-code-alpaca is a fine-tuned LLM that specializes in code generation, also tested to assess the effectiveness of M&lt;sup&gt;3&lt;/sup&gt;.&lt;SEP&gt;llama-2-13b-code-alpaca is a model fine-tuned for code generation, enhancing understanding of programming tasks.&lt;SEP&gt;llama-2-13b-code-alpaca is a task-specific fine-tuned LLM designed for code generation tasks.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="LiveBench">
  <data key="d0">LiveBench</data>
  <data key="d1">event</data>
  <data key="d2">LiveBench is a dynamic benchmark for large language models, featuring frequently updated questions and diverse tasks to assess OOD robustness.&lt;SEP&gt;LiveBench is a framework used to evaluate the performance of language models across different tasks, including instruction following and coding.&lt;SEP&gt;LiveBench is an evaluation framework used to test the robustness of models, including those merged using the M&lt;sup&gt;3&lt;/sup&gt; technique.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="PromptBench">
  <data key="d0">PromptBench</data>
  <data key="d1">organization</data>
  <data key="d2">PromptBench is a framework used for validating the performance of machine learning models, particularly in the context of M&lt;sup&gt;3&lt;/sup&gt; and model merging.&lt;SEP&gt;PromptBench is a unified library designed for evaluating large language models, providing a standardized framework for prompt construction and adversarial testing.&lt;SEP&gt;PromptBench is another evaluation framework that assesses the performance of models, including the M&lt;sup&gt;3&lt;/sup&gt; method's impact on robustness.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-c8ff785e8102dfde59dbc9309abb6b6a&lt;SEP&gt;chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Ding et al.">
  <data key="d0">Ding et al.</data>
  <data key="d1">person</data>
  <data key="d2">Ding et al. refers to a group of authors contributing to research in the field, cited in the context of model merging and computational resources.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Xia et al.">
  <data key="d0">Xia et al.</data>
  <data key="d1">person</data>
  <data key="d2">Xia et al. are researchers mentioned in the text, contributing to the discussion on model merging and its implications.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Brown et al.">
  <data key="d0">Brown et al.</data>
  <data key="d1">person</data>
  <data key="d2">Brown et al. are cited authors discussing the computational requirements of SFT, contributing to the analysis of model merging.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Chang et al.">
  <data key="d0">Chang et al.</data>
  <data key="d1">person</data>
  <data key="d2">Chang et al. is another group of authors referenced in the context of model merging and fine-tuning.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Yang et al.">
  <data key="d0">Yang et al.</data>
  <data key="d1">person</data>
  <data key="d2">Yang et al. are researchers cited for their contributions to the development of the M&lt;sup&gt;3&lt;/sup&gt; technique.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Akiba et al.">
  <data key="d0">Akiba et al.</data>
  <data key="d1">person</data>
  <data key="d2">Akiba et al. are authors mentioned in relation to the advancements in model merging techniques.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Wortsman et al.">
  <data key="d0">Wortsman et al.</data>
  <data key="d1">person</data>
  <data key="d2">Wortsman et al. are researchers associated with the development of model merging techniques and methodologies.&lt;SEP&gt;Wortsman et al. are researchers referenced for their work on parameter fusion strategies in model merging.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Ilharco et al.">
  <data key="d0">Ilharco et al.</data>
  <data key="d1">person</data>
  <data key="d2">Ilharco et al. are authors mentioned in the context of limitations in existing model merging methods.&lt;SEP&gt;Ilharco et al. are researchers who have worked on model merging and related techniques.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Matena and Rafefel">
  <data key="d0">Matena and Rafefel</data>
  <data key="d1">person</data>
  <data key="d2">Matena and Rafefel are researchers cited for their contributions to the discussion on model merging limitations.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jin et al.">
  <data key="d0">Jin et al.</data>
  <data key="d1">person</data>
  <data key="d2">Jin et al. are authors referenced regarding the exploration of parameter space in model merging.&lt;SEP&gt;Jin et al. are researchers who have contributed to the field of model merging.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Yadav et al.">
  <data key="d0">Yadav et al.</data>
  <data key="d1">person</data>
  <data key="d2">Yadav et al. are researchers mentioned in the context of model merging and its challenges.&lt;SEP&gt;Yadav et al. are researchers who proposed TIES-Merging as a solution to task conflicts in model merging.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Zhang">
  <data key="d0">Zhang</data>
  <data key="d1">person</data>
  <data key="d2">Zhang is cited for introducing the Mixup technique, which inspired the development of the M&lt;sup&gt;3&lt;/sup&gt; method.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="White et al.">
  <data key="d0">White et al.</data>
  <data key="d1">person</data>
  <data key="d2">White et al. are researchers involved in the evaluation of model robustness using LiveBench.&lt;SEP&gt;White et al. is mentioned with respect to the LiveBench framework for evaluating out-of-distribution robustness.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Zhu et al.">
  <data key="d0">Zhu et al.</data>
  <data key="d1">person</data>
  <data key="d2">Zhu et al. are authors referenced for their work on evaluating the effectiveness of M&lt;sup&gt;3&lt;/sup&gt; using PromptBench.&lt;SEP&gt;Zhu et al. are researchers who contributed to the development of methods for validating the potential of M&lt;sup&gt;3&lt;/sup&gt; in machine learning.&lt;SEP&gt;Zhu et al. is noted for their work on the PromptBench module used for assessing LLMs against adversarial prompts.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-c8ff785e8102dfde59dbc9309abb6b6a&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Super Mario">
  <data key="d0">Super Mario</data>
  <data key="d1">event</data>
  <data key="d2">Super Mario is referenced as an analogy for the model merging process, illustrating how abilities are gained through integration.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Harry Potter">
  <data key="d0">Harry Potter</data>
  <data key="d1">event</data>
  <data key="d2">Harry Potter is used as an analogy to describe the mixing process in M&lt;sup&gt;3&lt;/sup&gt;, highlighting the dynamic control of parameters.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Fisher Merging">
  <data key="d0">Fisher Merging</data>
  <data key="d1">category</data>
  <data key="d2">Fisher Merging is a method that applies weights derived from the Fisher information matrix for precise parameter integration during model merging.</data>
  <data key="d3">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Mixup">
  <data key="d0">Mixup</data>
  <data key="d1">category</data>
  <data key="d2">Mixup is a data augmentation technique that generates new training samples by creating linear combinations of existing samples and their labels.&lt;SEP&gt;Mixup is a data augmentation technique that improves the generalization ability of deep learning models by creating virtual examples through linear interpolation.</data>
  <data key="d3">chunk-c8ff785e8102dfde59dbc9309abb6b6a&lt;SEP&gt;chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Empirical Risk Minimization (ERM)">
  <data key="d0">Empirical Risk Minimization (ERM)</data>
  <data key="d1">category</data>
  <data key="d2">Empirical Risk Minimization is a traditional framework in machine learning for training models based on minimizing empirical risk.</data>
  <data key="d3">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Vicinal Risk Minimization (VRM)">
  <data key="d0">Vicinal Risk Minimization (VRM)</data>
  <data key="d1">category</data>
  <data key="d2">VRM is a principle that introduces a vicinal distribution to enhance data diversity and model generalization.&lt;SEP&gt;Vicinal Risk Minimization is a principle that underlies the Mixup technique, focusing on improving model generalization.</data>
  <data key="d3">chunk-6c3c559e2fcc2c84ba368ddf5b7db117&lt;SEP&gt;chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Matena and Raffel">
  <data key="d0">Matena and Raffel</data>
  <data key="d1">person</data>
  <data key="d2">Matena and Raffel are researchers known for their contributions to Fisher Merging in machine learning.</data>
  <data key="d3">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Yu et al.">
  <data key="d0">Yu et al.</data>
  <data key="d1">person</data>
  <data key="d2">Yu et al. are researchers who have explored the capabilities of large language models (LLMs) in enhancing model merging.</data>
  <data key="d3">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Lin et al.">
  <data key="d0">Lin et al.</data>
  <data key="d1">person</data>
  <data key="d2">Lin et al. are researchers contributing to the understanding and advancements in model merging techniques.</data>
  <data key="d3">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Mixup Model Merge">
  <data key="d0">Mixup Model Merge</data>
  <data key="d1">category</data>
  <data key="d2">Mixup Model Merge (M&lt;sup&gt;3&lt;/sup&gt;) is a novel model merging method that combines parameters from two fine-tuned language models using random interpolation to enhance model performance.</data>
  <data key="d3">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Fine-Tuned LLMs">
  <data key="d0">Fine-Tuned LLMs</data>
  <data key="d1">category</data>
  <data key="d2">Fine-tuned LLMs are language models that have undergone additional training on specific tasks to improve their performance in those areas.</data>
  <data key="d3">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Average Merging">
  <data key="d0">Average Merging</data>
  <data key="d1">category</data>
  <data key="d2">Average Merging is a model merging method that is analyzed in the context of performance improvement with M&lt;sup&gt;3&lt;/sup&gt;.&lt;SEP&gt;Average Merging is a process that reformulates the merging of language model parameters, allowing for a weighted combination of fine-tuned models.&lt;SEP&gt;Average Merging is an established model merging method that combines the parameters of two models by averaging them.&lt;SEP&gt;Average Merging is one of the merging techniques evaluated for enhancing the performance of LLMs.</data>
  <data key="d3">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff&lt;SEP&gt;chunk-6c3c559e2fcc2c84ba368ddf5b7db117&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Beta Distribution">
  <data key="d0">Beta Distribution</data>
  <data key="d1">category</data>
  <data key="d2">Beta Distribution is a probability distribution used in statistics to model the behavior of random variables limited to a range of values.&lt;SEP&gt;The Beta distribution is a statistical distribution used in M&lt;sup&gt;3&lt;/sup&gt; to sample interpolation ratios between two fine-tuned models.</data>
  <data key="d3">chunk-6c3c559e2fcc2c84ba368ddf5b7db117&lt;SEP&gt;chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Llama 2">
  <data key="d0">Llama 2</data>
  <data key="d1">organization</data>
  <data key="d2">Llama 2 is a pre-trained language model that serves as a backbone for model merging processes.&lt;SEP&gt;Llama 2 refers to a series of models developed for chat and instruction-following tasks in AI.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2&lt;SEP&gt;chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Hyperparameter">
  <data key="d0">Hyperparameter</data>
  <data key="d1">category</data>
  <data key="d2">Hyperparameters are configuration settings used to control the learning process of machine learning algorithms, such as the interpolation ratio in Mixup.</data>
  <data key="d3">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="LLMs">
  <data key="d0">LLMs</data>
  <data key="d1">category</data>
  <data key="d2">LLMs, or Large Language Models, are advanced neural network architectures designed to understand and generate human-like text based on vast datasets.</data>
  <data key="d3">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Computational Cost">
  <data key="d0">Computational Cost</data>
  <data key="d1">category</data>
  <data key="d2">Computational Cost refers to the resources required, such as time and processing power, to execute algorithms or models.</data>
  <data key="d3">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Training Instability">
  <data key="d0">Training Instability</data>
  <data key="d1">category</data>
  <data key="d2">Training Instability describes the unpredictable behavior of a model during training, which can lead to poor performance or failure to converge.</data>
  <data key="d3">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Interpolation Ratio">
  <data key="d0">Interpolation Ratio</data>
  <data key="d1">category</data>
  <data key="d2">Interpolation Ratio is a hyperparameter that determines the weight of each sample in generating new samples during Mixup.</data>
  <data key="d3">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Synthetic Sample">
  <data key="d0">Synthetic Sample</data>
  <data key="d1">category</data>
  <data key="d2">Synthetic Sample refers to the new data points created through methods like Mixup, combining features and labels from existing samples.</data>
  <data key="d3">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Parameters">
  <data key="d0">Parameters</data>
  <data key="d1">category</data>
  <data key="d2">Parameters are the internal variables of a model that are adjusted during training to minimize error and improve performance.</data>
  <data key="d3">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Fine-Tuned Models">
  <data key="d0">Fine-Tuned Models</data>
  <data key="d1">category</data>
  <data key="d2">Fine-Tuned Models are pre-trained models that have been further trained on specific tasks to enhance their performance in those areas.</data>
  <data key="d3">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Pre-Trained Backbone">
  <data key="d0">Pre-Trained Backbone</data>
  <data key="d1">category</data>
  <data key="d2">Pre-Trained Backbone refers to the foundational model that has been trained on a broad dataset before being fine-tuned for specific tasks.</data>
  <data key="d3">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="SFT">
  <data key="d0">SFT</data>
  <data key="d1">category</data>
  <data key="d2">SFT stands for Supervised Fine-Tuning, a process that adjusts language model parameters for specific tasks.</data>
  <data key="d3">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="$\theta^t_{SFT}$">
  <data key="d0">$\theta^t_{SFT}$</data>
  <data key="d1">category</data>
  <data key="d2">$\theta^t_{SFT}$ represents the parameters of a language model after it has undergone Supervised Fine-Tuning for a specific task.</data>
  <data key="d3">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="$\theta_{PRE}$">
  <data key="d0">$\theta_{PRE}$</data>
  <data key="d1">category</data>
  <data key="d2">$\theta_{PRE}$ denotes the parameters of a language model prior to any fine-tuning process.</data>
  <data key="d3">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="$\delta^t$">
  <data key="d0">$\delta^t$</data>
  <data key="d1">category</data>
  <data key="d2">$\delta^t$ is the difference between the parameters of language models before and after Supervised Fine-Tuning.</data>
  <data key="d3">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="$\lambda_m$">
  <data key="d0">$\lambda_m$</data>
  <data key="d1">category</data>
  <data key="d2">$\lambda_m$ is a hyperparameter that determines the linear interpolation ratio between two fine-tuned language models.</data>
  <data key="d3">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="$\alpha$">
  <data key="d0">$\alpha$</data>
  <data key="d1">category</data>
  <data key="d2">$\alpha$ is a hyperparameter that controls the shape of the Beta distribution used in sampling the interpolation ratio $\lambda_m$.</data>
  <data key="d3">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Figure 3">
  <data key="d0">Figure 3</data>
  <data key="d1">event</data>
  <data key="d2">Figure 3 illustrates the Beta distribution visualization for different values of the hyperparameter $\alpha$.</data>
  <data key="d3">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Task-Specific Fine-Tuned LLMs">
  <data key="d0">Task-Specific Fine-Tuned LLMs</data>
  <data key="d1">category</data>
  <data key="d2">Task-Specific Fine-Tuned LLMs refers to large language models that are specifically fine-tuned for distinct tasks such as instruction following, mathematical reasoning, and code generation.</data>
  <data key="d3">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="AlpacaEval">
  <data key="d0">AlpacaEval</data>
  <data key="d1">event</data>
  <data key="d2">AlpacaEval is a benchmark for evaluating language models' performance on instruction-following tasks.&lt;SEP&gt;AlpacaEval is a benchmark used to evaluate the performance of language models, particularly in the context of model merging.&lt;SEP&gt;AlpacaEval is an LLM-based automated evaluation metric designed to assess model performance on instruction-following tasks.&lt;SEP&gt;AlpacaEval is an LLM-based automated evaluation metric for assessing model performance on instruction-following tasks.&lt;SEP&gt;AlpacaEval is an evaluation benchmark used to assess the performance of instruction-following tasks.</data>
  <data key="d3">chunk-f21dc353d188ca08cba26298157ff09d&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-9fbd0c0622e4d5fb84be83781e1c4521&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="GSM8K">
  <data key="d0">GSM8K</data>
  <data key="d1">event</data>
  <data key="d2">GSM8K is a dataset containing 8.5K high-quality, linguistically diverse grade school math word problems, used to evaluate multi-step mathematical reasoning abilities of large language models.&lt;SEP&gt;GSM8K is a dataset designed to evaluate multi-step mathematical reasoning abilities of large language models with 8.5K word problems.&lt;SEP&gt;GSM8K is a dataset used for evaluating zero-shot accuracy in mathematical problem-solving tasks.&lt;SEP&gt;GSM8K is a dataset used for training language models in mathematical reasoning tasks.&lt;SEP&gt;GSM8K is an evaluation dataset used for testing the performance of mathematical reasoning tasks.&lt;SEP&gt;GSM8K is another benchmark for assessing the capabilities of language models, often used in conjunction with others like AlpacaEval.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-9fbd0c0622e4d5fb84be83781e1c4521&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7&lt;SEP&gt;chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="MATH">
  <data key="d0">MATH</data>
  <data key="d1">dataset</data>
  <data key="d2">MATH is a dataset consisting of 12,500 competition-level math problems, aimed at evaluating and enhancing the problem-solving abilities of machine learning models.&lt;SEP&gt;MATH is a dataset containing competition-level math problems aimed at enhancing problem-solving abilities of machine learning models.&lt;SEP&gt;MATH is a dataset utilized for improving the mathematical reasoning capabilities of language models.&lt;SEP&gt;MATH is a dataset utilized to evaluate the performance of LLMs in solving mathematical problems.&lt;SEP&gt;MATH is an evaluation dataset designed to assess mathematical reasoning capabilities.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-9fbd0c0622e4d5fb84be83781e1c4521&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="HumanEval">
  <data key="d0">HumanEval</data>
  <data key="d1">dataset</data>
  <data key="d2">HumanEval is a benchmark used for evaluating code generation tasks.&lt;SEP&gt;HumanEval is a benchmark used to assess the code generation capabilities of language models.&lt;SEP&gt;HumanEval is a dataset of 164 hand-written programming problems used to evaluate the functional correctness of code generation models.&lt;SEP&gt;HumanEval is a dataset of programming problems used to evaluate the functional correctness of code generation models.</data>
  <data key="d3">chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-9fbd0c0622e4d5fb84be83781e1c4521&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="MBPP">
  <data key="d0">MBPP</data>
  <data key="d1">event</data>
  <data key="d2">MBPP is a benchmark for assessing the performance of code-generating tasks.&lt;SEP&gt;MBPP is a benchmark that evaluates programming tasks and problem-solving abilities of language models.&lt;SEP&gt;MBPP is a benchmark used for evaluating the performance of LLMs on coding tasks.&lt;SEP&gt;MBPP is a dataset containing 974 programming problems designed to assess a model's ability to synthesize Python programs from natural language descriptions.&lt;SEP&gt;MBPP is a dataset that evaluates a model's ability to synthesize Python programs from natural language descriptions.&lt;SEP&gt;MBPP is a dataset where the Math &amp; Code model achieved a specific performance score when combined with DARE.&lt;SEP&gt;MBPP is a dataset where the Math &amp; Code model achieved a specific score when combined with DARE.&lt;SEP&gt;MBPP refers to a benchmark for evaluating the performance of programming models, which shows improvement through the M&lt;sup&gt;3&lt;/sup&gt; approach.</data>
  <data key="d3">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-9fbd0c0622e4d5fb84be83781e1c4521&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7&lt;SEP&gt;chunk-f21dc353d188ca08cba26298157ff09d&lt;SEP&gt;chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="θ_{SFT}^{t_1}">
  <data key="d0">θ_{SFT}^{t_1}</data>
  <data key="d1">category</data>
  <data key="d2">θ_{SFT}^{t_1} is a parameter representation used in the context of model training, specifically in the linear interpolation of model parameters.</data>
  <data key="d3">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="θ_{SFT}^{t_2}">
  <data key="d0">θ_{SFT}^{t_2}</data>
  <data key="d1">category</data>
  <data key="d2">θ_{SFT}^{t_2} is another parameter representation that, along with θ_{SFT}^{t_1}, is utilized for linear interpolation in model training.</data>
  <data key="d3">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="λ_m">
  <data key="d0">λ_m</data>
  <data key="d1">category</data>
  <data key="d2">λ_m is the interpolation ratio used to combine parameters from different models in the M&lt;sup&gt;3&lt;/sup&gt; method.</data>
  <data key="d3">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="P_{ν}(θ_M)">
  <data key="d0">P_{ν}(θ_M)</data>
  <data key="d1">category</data>
  <data key="d2">P_{ν}(θ_M) represents the distribution resulting from the linear interpolation of model parameters in the M&lt;sup&gt;3&lt;/sup&gt; method.</data>
  <data key="d3">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Dirac Delta Function">
  <data key="d0">Dirac Delta Function</data>
  <data key="d1">category</data>
  <data key="d2">The Dirac delta function is a mathematical function used to enforce conditions in distributions, ensuring that θ_M satisfies the linear interpolation rule.</data>
  <data key="d3">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Neighborhood Distribution">
  <data key="d0">Neighborhood Distribution</data>
  <data key="d1">category</data>
  <data key="d2">Neighborhood distribution refers to the new distribution formed by merging knowledge from different models through linear interpolation.</data>
  <data key="d3">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Decision Boundaries">
  <data key="d0">Decision Boundaries</data>
  <data key="d1">category</data>
  <data key="d2">Decision boundaries are the thresholds that separate different tasks, which the M&lt;sup&gt;3&lt;/sup&gt; method aims to make smoother.</data>
  <data key="d3">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Occam's Razor">
  <data key="d0">Occam's Razor</data>
  <data key="d1">category</data>
  <data key="d2">Occam's Razor is a principle suggesting that simpler solutions tend to generalize better, which is applied in the M&lt;sup&gt;3&lt;/sup&gt; method for model merging.</data>
  <data key="d3">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Task Conflicts">
  <data key="d0">Task Conflicts</data>
  <data key="d1">category</data>
  <data key="d2">Task conflicts refer to the issues that arise when different tasks require conflicting parameter values, which the M&lt;sup&gt;3&lt;/sup&gt; method aims to mitigate.</data>
  <data key="d3">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Experiments">
  <data key="d0">Experiments</data>
  <data key="d1">event</data>
  <data key="d2">Experiments refer to the structured evaluations conducted to assess the performance of the M&lt;sup&gt;3&lt;/sup&gt; method across different tasks.</data>
  <data key="d3">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Instruction Following">
  <data key="d0">Instruction Following</data>
  <data key="d1">event</data>
  <data key="d2">Instruction Following is a task category in LiveBench that assesses a model's ability to follow given instructions correctly.&lt;SEP&gt;Instruction following is a specific task that the WizardLM-13B model is designed to perform.</data>
  <data key="d3">chunk-b650b88d4bebbc45d20b8908b045eff8&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Mathematical Reasoning">
  <data key="d0">Mathematical Reasoning</data>
  <data key="d1">event</data>
  <data key="d2">Mathematical reasoning is a task that the WizardMath-13B model is specifically fine-tuned for.</data>
  <data key="d3">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Code Generation">
  <data key="d0">Code Generation</data>
  <data key="d1">event</data>
  <data key="d2">Code generation is a task that the llama-2-13b-code-alpaca model is designed to perform.</data>
  <data key="d3">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Austin et al.">
  <data key="d0">Austin et al.</data>
  <data key="d1">person</data>
  <data key="d2">Austin et al. is referenced in the context of evaluating large language models (LLMs) and their datasets.</data>
  <data key="d3">chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="SST2">
  <data key="d0">SST2</data>
  <data key="d1">event</data>
  <data key="d2">SST2 (Stanford Sentiment Treebank 2) is a dataset used for sentiment analysis in natural language processing.&lt;SEP&gt;SST2 is a dataset used for evaluating sentiment analysis models, referenced in the context of model performance metrics.&lt;SEP&gt;SST2 is a dataset used for evaluating sentiment analysis models, referenced in the context of model performance.&lt;SEP&gt;SST2 is a dataset used for evaluating the performance of models in sentiment analysis tasks.&lt;SEP&gt;SST2 is a sentiment analysis dataset designed to assess whether a given sentence conveys a positive or negative sentiment.&lt;SEP&gt;SST2 is a sentiment analysis dataset referenced for evaluating accuracy in LLMs.</data>
  <data key="d3">chunk-070652f4f36e86620289fcafaaa29599&lt;SEP&gt;chunk-407b385e429f3495cdcf3f23f7e80f73&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-47449b4eaab59a26168fa9d821973678&lt;SEP&gt;chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="CoLA">
  <data key="d0">CoLA</data>
  <data key="d1">event</data>
  <data key="d2">CoLA (Corpus of Linguistic Acceptability) is a dataset used for evaluating the linguistic acceptability of sentences.&lt;SEP&gt;CoLA is a dataset for grammar correctness, where the model must determine whether a sentence is grammatically acceptable.&lt;SEP&gt;CoLA is a dataset that tests grammar correctness, mentioned in relation to model performance metrics.&lt;SEP&gt;CoLA is a dataset used for evaluating the performance of models in linguistic acceptability tasks.&lt;SEP&gt;CoLA is a dataset used to assess grammatical correctness in language models.</data>
  <data key="d3">chunk-070652f4f36e86620289fcafaaa29599&lt;SEP&gt;chunk-407b385e429f3495cdcf3f23f7e80f73&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-47449b4eaab59a26168fa9d821973678&lt;SEP&gt;chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="NVIDIA GeForce RTX 4090">
  <data key="d0">NVIDIA GeForce RTX 4090</data>
  <data key="d1">equipment</data>
  <data key="d2">NVIDIA GeForce RTX 4090 is a high-performance GPU utilized for conducting experiments in LLM model merging.&lt;SEP&gt;NVIDIA GeForce RTX 4090 is a high-performance graphics processing unit used for running experiments on LLMs.</data>
  <data key="d3">chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Appendix A.1">
  <data key="d0">Appendix A.1</data>
  <data key="d1">category</data>
  <data key="d2">Appendix A.1 contains details about large language models (LLMs) and datasets used in the evaluation process.</data>
  <data key="d3">chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Adversarial Prompt Attacks">
  <data key="d0">Adversarial Prompt Attacks</data>
  <data key="d1">category</data>
  <data key="d2">Adversarial Prompt Attacks are methods used to evaluate the robustness of language models against misleading prompts.&lt;SEP&gt;Adversarial Prompt Attacks is a module within PromptBench designed to assess the robustness of LLMs against adversarial prompts.</data>
  <data key="d3">chunk-b650b88d4bebbc45d20b8908b045eff8&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="DeepWordBug">
  <data key="d0">DeepWordBug</data>
  <data key="d1">category</data>
  <data key="d2">DeepWordBug is a character-level attack method used to evaluate LLMs' robustness against adversarial prompts.&lt;SEP&gt;DeepWordBug is a method that introduces subtle character-level perturbations to deceive language models, testing their robustness against small typographical errors.&lt;SEP&gt;DeepWordBug is a prompt attack method used to evaluate the adversarial robustness of models.</data>
  <data key="d3">chunk-070652f4f36e86620289fcafaaa29599&lt;SEP&gt;chunk-407b385e429f3495cdcf3f23f7e80f73&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="BERTAttack">
  <data key="d0">BERTAttack</data>
  <data key="d1">category</data>
  <data key="d2">BERTAttack is a method used to evaluate the adversarial robustness of models by applying prompt attacks.&lt;SEP&gt;BERTAttack is a word-level attack method employed to test the resilience of LLMs against adversarial inputs.&lt;SEP&gt;BERTAttack is another prompt attack method that assesses the robustness of models against adversarial inputs.&lt;SEP&gt;BERTAttack manipulates text at the word level by replacing words with contextually similar synonyms to mislead language models.</data>
  <data key="d3">chunk-070652f4f36e86620289fcafaaa29599&lt;SEP&gt;chunk-568ea9e09f85a7b95cfe61d48b0ba893&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="StressTest">
  <data key="d0">StressTest</data>
  <data key="d1">category</data>
  <data key="d2">StressTest appends irrelevant or redundant sentences to prompts to distract language models and assess their ability to maintain accuracy.&lt;SEP&gt;StressTest is a prompt attack method designed to evaluate the performance drop rate of models under adversarial conditions.&lt;SEP&gt;StressTest is a sentence-level attack method used to evaluate the robustness of LLMs in handling adversarial prompts.</data>
  <data key="d3">chunk-070652f4f36e86620289fcafaaa29599&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Matthews Correlation Coefficient (MCC)">
  <data key="d0">Matthews Correlation Coefficient (MCC)</data>
  <data key="d1">category</data>
  <data key="d2">Matthews Correlation Coefficient (MCC) is a metric used to evaluate the performance of models on the CoLA dataset for grammatical correctness.</data>
  <data key="d3">chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="llama-2-13b-codealpaca">
  <data key="d0">llama-2-13b-codealpaca</data>
  <data key="d1">organization</data>
  <data key="d2">llama-2-13b-codealpaca is a large language model aimed at coding and programming tasks in machine learning.&lt;SEP&gt;llama-2-13b-codealpaca is a model that has been identified as not being well fine-tuned for code generation, impacting its merging effectiveness.</data>
  <data key="d3">chunk-17c651f0ad506eb37b0f1742513e2650&lt;SEP&gt;chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="LiveBench-Instruction">
  <data key="d0">LiveBench-Instruction</data>
  <data key="d1">event</data>
  <data key="d2">LiveBench-Instruction is a benchmark event that assesses the performance of models in instruction-following tasks.&lt;SEP&gt;LiveBench-Instruction is a benchmarking event assessing language models' performance in following instructions.&lt;SEP&gt;LiveBench-Instruction is a dataset used for evaluating model performance on instruction-following tasks.</data>
  <data key="d3">chunk-17c651f0ad506eb37b0f1742513e2650&lt;SEP&gt;chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff&lt;SEP&gt;chunk-070652f4f36e86620289fcafaaa29599</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="LiveBench-Coding">
  <data key="d0">LiveBench-Coding</data>
  <data key="d1">event</data>
  <data key="d2">LiveBench-Coding is a benchmark event utilized to evaluate coding-related model performance.&lt;SEP&gt;LiveBench-Coding is a benchmarking event that evaluates the performance of language models on coding tasks.&lt;SEP&gt;LiveBench-Coding is a dataset used for evaluating model performance specifically on coding tasks.</data>
  <data key="d3">chunk-17c651f0ad506eb37b0f1742513e2650&lt;SEP&gt;chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff&lt;SEP&gt;chunk-070652f4f36e86620289fcafaaa29599</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="LiveBench-TypoFixing">
  <data key="d0">LiveBench-TypoFixing</data>
  <data key="d1">event</data>
  <data key="d2">LiveBench-TypoFixing is a benchmark event used to evaluate model performance in typo fixing tasks.&lt;SEP&gt;LiveBench-TypoFixing is a benchmarking event that tests the ability of language models to correct typographical errors.&lt;SEP&gt;LiveBench-TypoFixing is a dataset that assesses model performance on correcting typographical errors in code.</data>
  <data key="d3">chunk-17c651f0ad506eb37b0f1742513e2650&lt;SEP&gt;chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff&lt;SEP&gt;chunk-070652f4f36e86620289fcafaaa29599</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="OOD Datasets">
  <data key="d0">OOD Datasets</data>
  <data key="d1">category</data>
  <data key="d2">OOD Datasets refer to out-of-distribution datasets that are used to evaluate the robustness of machine learning models.</data>
  <data key="d3">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Model Robustness">
  <data key="d0">Model Robustness</data>
  <data key="d1">category</data>
  <data key="d2">Model Robustness refers to the ability of a machine learning model to perform well on unseen or out-of-distribution data.</data>
  <data key="d3">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Merging Ratio">
  <data key="d0">Merging Ratio</data>
  <data key="d1">category</data>
  <data key="d2">Merging Ratio is a critical factor in determining the performance of a merged model, influencing how different models are combined.</data>
  <data key="d3">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Performance Metrics">
  <data key="d0">Performance Metrics</data>
  <data key="d1">category</data>
  <data key="d2">Performance Metrics refer to the various measurements used to evaluate the effectiveness and accuracy of language models during testing events.&lt;SEP&gt;Performance metrics refer to the criteria used to evaluate the effectiveness of models, including accuracy and MCC.</data>
  <data key="d3">chunk-17c651f0ad506eb37b0f1742513e2650&lt;SEP&gt;chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Accuracy Improvement">
  <data key="d0">Accuracy Improvement</data>
  <data key="d1">category</data>
  <data key="d2">Accuracy Improvement refers to the enhancement in the precision of language models as measured by their performance in specific tasks.</data>
  <data key="d3">chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Win Rate">
  <data key="d0">Win Rate</data>
  <data key="d1">category</data>
  <data key="d2">Win Rate is a performance indicator that reflects the success rate of a language model in competitive scenarios against benchmarks.</data>
  <data key="d3">chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Pass@1">
  <data key="d0">Pass@1</data>
  <data key="d1">category</data>
  <data key="d2">Pass@1 is a specific evaluation metric that indicates whether a model's first response to a task is correct.</data>
  <data key="d3">chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Math &amp; Code">
  <data key="d0">Math &amp; Code</data>
  <data key="d1">organization</data>
  <data key="d2">Math &amp; Code is a merged model that utilizes mathematical and coding strategies to enhance performance on various tasks.&lt;SEP&gt;Math &amp; Code is a model configuration tested on the SST2 and CoLA datasets for performance evaluation.&lt;SEP&gt;Math &amp; Code is a model that demonstrated significant performance improvements when combined with M&lt;sup&gt;3&lt;/sup&gt; in model merging.&lt;SEP&gt;Math &amp; Code is a model that demonstrated significant performance improvements when merged with M&lt;sup&gt;3&lt;/sup&gt; in model merging experiments.</data>
  <data key="d3">chunk-070652f4f36e86620289fcafaaa29599&lt;SEP&gt;chunk-407b385e429f3495cdcf3f23f7e80f73&lt;SEP&gt;chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="LM &amp; Math">
  <data key="d0">LM &amp; Math</data>
  <data key="d1">category</data>
  <data key="d2">LM &amp; Math is a merged model that combines language modeling with mathematical strategies to improve task performance.&lt;SEP&gt;LM &amp; Math is another model configuration evaluated on the SST2 and CoLA datasets, focusing on language model performance.</data>
  <data key="d3">chunk-070652f4f36e86620289fcafaaa29599&lt;SEP&gt;chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="LM &amp; Code">
  <data key="d0">LM &amp; Code</data>
  <data key="d1">organization</data>
  <data key="d2">LM &amp; Code is a merged model that integrates language modeling with coding strategies to achieve better task outcomes.&lt;SEP&gt;LM &amp; Code is a model setup tested on the SST2 and CoLA datasets, assessing its robustness against adversarial attacks.&lt;SEP&gt;LM &amp; Code is a model that achieved notable accuracy increases when enhanced with M&lt;sup&gt;3&lt;/sup&gt; during merging.&lt;SEP&gt;LM &amp; Code is another model that achieved notable accuracy increases when enhanced with M&lt;sup&gt;3&lt;/sup&gt; during merging.</data>
  <data key="d3">chunk-070652f4f36e86620289fcafaaa29599&lt;SEP&gt;chunk-407b385e429f3495cdcf3f23f7e80f73&lt;SEP&gt;chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Performance Drop Rate (PDR)">
  <data key="d0">Performance Drop Rate (PDR)</data>
  <data key="d1">category</data>
  <data key="d2">Performance Drop Rate (PDR) is a metric used to evaluate the adversarial robustness of models under attack.&lt;SEP&gt;Performance Drop Rate (PDR) is a metric used to evaluate the adversarial robustness of models, with a lower PDR indicating stronger robustness.</data>
  <data key="d3">chunk-070652f4f36e86620289fcafaaa29599&lt;SEP&gt;chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Model Merging Techniques">
  <data key="d0">Model Merging Techniques</data>
  <data key="d1">category</data>
  <data key="d2">Model merging techniques include Average Merging, Task Arithmetic, and TIES-Merging, which are evaluated for their effectiveness in improving model performance.&lt;SEP&gt;Model merging techniques include Average Merging, Task Arithmetic, and TIES-Merging, which are evaluated for their effectiveness.</data>
  <data key="d3">chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Parameter Linear Interpolation Process">
  <data key="d0">Parameter Linear Interpolation Process</data>
  <data key="d1">category</data>
  <data key="d2">The parameter linear interpolation process is a method used in M&lt;sup&gt;3&lt;/sup&gt; to dynamically adjust merging ratios during model merging.</data>
  <data key="d3">chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Adversarial Robustness">
  <data key="d0">Adversarial Robustness</data>
  <data key="d1">category</data>
  <data key="d2">Adversarial Robustness is a property of machine learning models that measures their ability to maintain performance when faced with adversarial inputs or attacks.&lt;SEP&gt;Adversarial robustness refers to the ability of merged models to maintain performance under adversarial conditions, which M&lt;sup&gt;3&lt;/sup&gt; aims to enhance.</data>
  <data key="d3">chunk-407b385e429f3495cdcf3f23f7e80f73&lt;SEP&gt;chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Ouyang et al.">
  <data key="d0">Ouyang et al.</data>
  <data key="d1">person</data>
  <data key="d2">Ouyang et al. is referenced in the context of developing models that incorporate Reinforcement Learning with Human Feedback for improved alignment.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jacob Austin">
  <data key="d0">Jacob Austin</data>
  <data key="d1">person</data>
  <data key="d2">Jacob Austin is one of the authors of the referenced paper on program synthesis with large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Augustus Odena">
  <data key="d0">Augustus Odena</data>
  <data key="d1">person</data>
  <data key="d2">Augustus Odena is a co-author of the paper discussing program synthesis with large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Maxwell Nye">
  <data key="d0">Maxwell Nye</data>
  <data key="d1">person</data>
  <data key="d2">Maxwell Nye is a contributor to the research on program synthesis with large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Maarten Bosma">
  <data key="d0">Maarten Bosma</data>
  <data key="d1">person</data>
  <data key="d2">Maarten Bosma is involved in the study of program synthesis with large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Henryk Michalewski">
  <data key="d0">Henryk Michalewski</data>
  <data key="d1">person</data>
  <data key="d2">Henryk Michalewski is a co-author of the referenced paper on program synthesis with large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="David Dohan">
  <data key="d0">David Dohan</data>
  <data key="d1">person</data>
  <data key="d2">David Dohan is one of the authors of the paper on program synthesis with large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Ellen Jiang">
  <data key="d0">Ellen Jiang</data>
  <data key="d1">person</data>
  <data key="d2">Ellen Jiang is a contributor to the research on program synthesis with large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Carrie Cai">
  <data key="d0">Carrie Cai</data>
  <data key="d1">person</data>
  <data key="d2">Carrie Cai is involved in the research on program synthesis with large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Michael Terry">
  <data key="d0">Michael Terry</data>
  <data key="d1">person</data>
  <data key="d2">Michael Terry is a co-author of the paper discussing program synthesis with large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Quoc Le">
  <data key="d0">Quoc Le</data>
  <data key="d1">person</data>
  <data key="d2">Quoc Le is one of the authors of the paper on program synthesis with large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Yupeng Chang">
  <data key="d0">Yupeng Chang</data>
  <data key="d1">person</data>
  <data key="d2">Yupeng Chang is involved in multiple papers on large language models and their evaluation.&lt;SEP&gt;Yupeng Chang is involved in the research on bias-alleviating low-rank adaptation for large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Edward Beeching">
  <data key="d0">Edward Beeching</data>
  <data key="d1">person</data>
  <data key="d2">Edward Beeching is a co-author of the paper on the Open LLM leaderboard.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Clémentine Fourrier">
  <data key="d0">Clémentine Fourrier</data>
  <data key="d1">person</data>
  <data key="d2">Clémentine Fourrier is involved in the research on the Open LLM leaderboard.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Nathan Habib">
  <data key="d0">Nathan Habib</data>
  <data key="d1">person</data>
  <data key="d2">Nathan Habib is a contributor to the study on the Open LLM leaderboard.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Sheon Han">
  <data key="d0">Sheon Han</data>
  <data key="d1">person</data>
  <data key="d2">Sheon Han is a co-author of the paper on the Open LLM leaderboard.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Nathan Lambert">
  <data key="d0">Nathan Lambert</data>
  <data key="d1">person</data>
  <data key="d2">Nathan Lambert is involved in the research on the Open LLM leaderboard.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Nazneen Rajani">
  <data key="d0">Nazneen Rajani</data>
  <data key="d1">person</data>
  <data key="d2">Nazneen Rajani is a contributor to the study on the Open LLM leaderboard.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Omar Sanseviero">
  <data key="d0">Omar Sanseviero</data>
  <data key="d1">person</data>
  <data key="d2">Omar Sanseviero is a co-author of the paper on the Open LLM leaderboard.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Lewis Tunstall">
  <data key="d0">Lewis Tunstall</data>
  <data key="d1">person</data>
  <data key="d2">Lewis Tunstall is involved in the research on the Open LLM leaderboard.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Thomas Wolf">
  <data key="d0">Thomas Wolf</data>
  <data key="d1">person</data>
  <data key="d2">Thomas Wolf is a co-author of the paper discussing the Open LLM leaderboard.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Tom Brown">
  <data key="d0">Tom Brown</data>
  <data key="d1">person</data>
  <data key="d2">Tom Brown is one of the authors of the paper on language models as few-shot learners.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Benjamin Mann">
  <data key="d0">Benjamin Mann</data>
  <data key="d1">person</data>
  <data key="d2">Benjamin Mann is a co-author of the paper discussing few-shot learning in language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Nick Ryder">
  <data key="d0">Nick Ryder</data>
  <data key="d1">person</data>
  <data key="d2">Nick Ryder is involved in the research on language models as few-shot learners.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Melanie Subbiah">
  <data key="d0">Melanie Subbiah</data>
  <data key="d1">person</data>
  <data key="d2">Melanie Subbiah is a co-author of the paper on language models as few-shot learners.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jared D Kaplan">
  <data key="d0">Jared D Kaplan</data>
  <data key="d1">person</data>
  <data key="d2">Jared D Kaplan is involved in the research on few-shot learning in language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Prafulla Dhariwal">
  <data key="d0">Prafulla Dhariwal</data>
  <data key="d1">person</data>
  <data key="d2">Prafulla Dhariwal is a co-author of the paper discussing language models as few-shot learners.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Arvind Neelakantan">
  <data key="d0">Arvind Neelakantan</data>
  <data key="d1">person</data>
  <data key="d2">Arvind Neelakantan is involved in the research on language models as few-shot learners.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Pranav Shyam">
  <data key="d0">Pranav Shyam</data>
  <data key="d1">person</data>
  <data key="d2">Pranav Shyam is a co-author of the paper on language models as few-shot learners.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Girish Sastry">
  <data key="d0">Girish Sastry</data>
  <data key="d1">person</data>
  <data key="d2">Girish Sastry is involved in the research on few-shot learning in language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Amanda Askell">
  <data key="d0">Amanda Askell</data>
  <data key="d1">person</data>
  <data key="d2">Amanda Askell is a co-author of the paper discussing language models as few-shot learners.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Xu Wang">
  <data key="d0">Xu Wang</data>
  <data key="d1">person</data>
  <data key="d2">Xu Wang is a co-author of the paper discussing evaluation of large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jindong Wang">
  <data key="d0">Jindong Wang</data>
  <data key="d1">person</data>
  <data key="d2">Jindong Wang is involved in the research on evaluation of large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Linyi Yang">
  <data key="d0">Linyi Yang</data>
  <data key="d1">person</data>
  <data key="d2">Linyi Yang is a co-author of the paper on evaluation of large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Kaijie Zhu">
  <data key="d0">Kaijie Zhu</data>
  <data key="d1">person</data>
  <data key="d2">Kaijie Zhu is involved in the research on evaluation of large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Hao Chen">
  <data key="d0">Hao Chen</data>
  <data key="d1">person</data>
  <data key="d2">Hao Chen is a co-author of the paper discussing evaluation of large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Xiaoyuan Yi">
  <data key="d0">Xiaoyuan Yi</data>
  <data key="d1">person</data>
  <data key="d2">Xiaoyuan Yi is involved in the research on evaluation of large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Cunxiang Wang">
  <data key="d0">Cunxiang Wang</data>
  <data key="d1">person</data>
  <data key="d2">Cunxiang Wang is a co-author of the paper on evaluation of large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Yidong Wang">
  <data key="d0">Yidong Wang</data>
  <data key="d1">person</data>
  <data key="d2">Yidong Wang is involved in the research on evaluation of large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Karl Cobbe">
  <data key="d0">Karl Cobbe</data>
  <data key="d1">person</data>
  <data key="d2">Karl Cobbe is a co-author of the paper on training verifiers for math word problems.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Vineet Kosaraju">
  <data key="d0">Vineet Kosaraju</data>
  <data key="d1">person</data>
  <data key="d2">Vineet Kosaraju is involved in the research on training verifiers for math word problems.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Mohammad Bavarian">
  <data key="d0">Mohammad Bavarian</data>
  <data key="d1">person</data>
  <data key="d2">Mohammad Bavarian is a co-author of the paper on training verifiers for math word problems.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Mark Chen">
  <data key="d0">Mark Chen</data>
  <data key="d1">person</data>
  <data key="d2">Mark Chen is involved in the research on training verifiers for math word problems.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Heewoo Jun">
  <data key="d0">Heewoo Jun</data>
  <data key="d1">person</data>
  <data key="d2">Heewoo Jun is a co-author of the paper on training verifiers for math word problems.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Lukasz Kaiser">
  <data key="d0">Lukasz Kaiser</data>
  <data key="d1">person</data>
  <data key="d2">Lukasz Kaiser is involved in the research on training verifiers for math word problems.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Matthias Plappert">
  <data key="d0">Matthias Plappert</data>
  <data key="d1">person</data>
  <data key="d2">Matthias Plappert is a co-author of the paper on training verifiers for math word problems.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jerry Tworek">
  <data key="d0">Jerry Tworek</data>
  <data key="d1">person</data>
  <data key="d2">Jerry Tworek is involved in the research on training verifiers for math word problems.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jacob Hilton">
  <data key="d0">Jacob Hilton</data>
  <data key="d1">person</data>
  <data key="d2">Jacob Hilton is a co-author of the paper on training verifiers for math word problems.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Reiichiro Nakano">
  <data key="d0">Reiichiro Nakano</data>
  <data key="d1">person</data>
  <data key="d2">Reiichiro Nakano is involved in the research on training verifiers for math word problems.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Ronald A Fisher">
  <data key="d0">Ronald A Fisher</data>
  <data key="d1">person</data>
  <data key="d2">Ronald A Fisher is referenced for his foundational work in theoretical statistics.&lt;SEP&gt;Ronald A Fisher was a prominent statistician known for his contributions to the mathematical foundations of theoretical statistics, particularly in the early 20th century.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36&lt;SEP&gt;chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Philosophical Transactions of the Royal">
  <data key="d0">Philosophical Transactions of the Royal</data>
  <data key="d1">organization</data>
  <data key="d2">Philosophical Transactions of the Royal is a journal that published Ronald A Fisher's foundational work in theoretical statistics.&lt;SEP&gt;Philosophical Transactions of the Royal is a journal where Ronald A Fisher's work on theoretical statistics was published.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Reinforcement Learning with Human Feedback">
  <data key="d0">Reinforcement Learning with Human Feedback</data>
  <data key="d1">category</data>
  <data key="d2">Reinforcement Learning with Human Feedback (RLHF) is a method that combines reinforcement learning techniques with human feedback to improve model alignment and performance.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="arXiv preprint arXiv:2403.13187">
  <data key="d0">arXiv preprint arXiv:2403.13187</data>
  <data key="d1">organization</data>
  <data key="d2">arXiv preprint arXiv:2403.13187 is a research paper authored by Takuya Akiba and others discussing evolutionary optimization of model merging recipes.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="UCI Machine Learning Repository">
  <data key="d0">UCI Machine Learning Repository</data>
  <data key="d1">organization</data>
  <data key="d2">The UCI Machine Learning Repository is a widely-used collection of datasets for machine learning research, referenced in the context of model evaluation.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="arXiv preprint arXiv:2108.07732">
  <data key="d0">arXiv preprint arXiv:2108.07732</data>
  <data key="d1">organization</data>
  <data key="d2">arXiv preprint arXiv:2108.07732 is a research paper on program synthesis with large language models, authored by Jacob Austin and others.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="arXiv preprint arXiv:2408.04556">
  <data key="d0">arXiv preprint arXiv:2408.04556</data>
  <data key="d1">organization</data>
  <data key="d2">arXiv preprint arXiv:2408.04556 is a research paper by Yupeng Chang and others discussing bias-alleviating low-rank adaptation.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="ACM Transactions on Intelligent Systems and Technology">
  <data key="d0">ACM Transactions on Intelligent Systems and Technology</data>
  <data key="d1">organization</data>
  <data key="d2">ACM Transactions on Intelligent Systems and Technology is a journal that published a survey on the evaluation of large language models, authored by Yupeng Chang and others.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Code Alpaca">
  <data key="d0">Code Alpaca</data>
  <data key="d1">category</data>
  <data key="d2">Code Alpaca is an instruction-following LLaMA model for code generation, discussed in the context of recent advancements in language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Nature Machine Intelligence">
  <data key="d0">Nature Machine Intelligence</data>
  <data key="d1">organization</data>
  <data key="d2">Nature Machine Intelligence is a journal where research on parameter-efficient fine-tuning of large-scale pretrained language models is published, authored by Ning Ding and others.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="arXiv preprint arXiv:2404.04475">
  <data key="d0">arXiv preprint arXiv:2404.04475</data>
  <data key="d1">event</data>
  <data key="d2">This arXiv preprint discusses advancements in LLM fine-tuning and is part of the larger body of research available on arXiv.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Philosophical Transactions of the Royal Society of London">
  <data key="d0">Philosophical Transactions of the Royal Society of London</data>
  <data key="d1">organization</data>
  <data key="d2">This publication is a prestigious journal that contains papers of a mathematical or physical character, including works by Ronald A Fisher.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="IEEE Security and Privacy Workshops (SPW)">
  <data key="d0">IEEE Security and Privacy Workshops (SPW)</data>
  <data key="d1">event</data>
  <data key="d2">The IEEE Security and Privacy Workshops is an annual event that focuses on security and privacy topics, featuring research presentations and discussions.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Chbench">
  <data key="d0">Chbench</data>
  <data key="d1">category</data>
  <data key="d2">Chbench is a Chinese dataset designed for evaluating health in large language models, highlighting the intersection of language processing and health metrics.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Math Dataset">
  <data key="d0">Math Dataset</data>
  <data key="d1">category</data>
  <data key="d2">The Math Dataset is used for measuring mathematical problem-solving abilities and is a significant resource for evaluating LLM performance.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Lora">
  <data key="d0">Lora</data>
  <data key="d1">category</data>
  <data key="d2">Lora refers to a method for low-rank adaptation of large language models, enhancing their efficiency and performance.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Wizardmath">
  <data key="d0">Wizardmath</data>
  <data key="d1">category</data>
  <data key="d2">Wizardmath is a project aimed at empowering mathematical reasoning in large language models through reinforced instruction.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing">
  <data key="d0">Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</data>
  <data key="d1">event</data>
  <data key="d2">This conference focuses on empirical methods in natural language processing, featuring a range of studies and findings from the field.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Ji Gao">
  <data key="d0">Ji Gao</data>
  <data key="d1">person</data>
  <data key="d2">Ji Gao is a researcher known for contributions in the field of adversarial machine learning and text generation, particularly in the context of black-box generation of adversarial text sequences.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jack Lanchantin">
  <data key="d0">Jack Lanchantin</data>
  <data key="d1">person</data>
  <data key="d2">Jack Lanchantin is a researcher who has worked on adversarial machine learning, focusing on generating adversarial text sequences to evade classifiers.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Mary Lou Soffa">
  <data key="d0">Mary Lou Soffa</data>
  <data key="d1">person</data>
  <data key="d2">Mary Lou Soffa is a computer scientist recognized for her work in software engineering and computer security, contributing to the development of adversarial techniques in text generation.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Yanjun Qi">
  <data key="d0">Yanjun Qi</data>
  <data key="d1">person</data>
  <data key="d2">Yanjun Qi is a researcher involved in machine learning, particularly in the areas of adversarial learning and its applications in natural language processing.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Chenlu Guo">
  <data key="d0">Chenlu Guo</data>
  <data key="d1">person</data>
  <data key="d2">Chenlu Guo is a researcher focusing on health evaluation in large language models, contributing to the creation of datasets for this purpose.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Nuo Xu">
  <data key="d0">Nuo Xu</data>
  <data key="d1">person</data>
  <data key="d2">Nuo Xu is a researcher involved in developing datasets aimed at evaluating language models, particularly in the health domain.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Dan Hendrycks">
  <data key="d0">Dan Hendrycks</data>
  <data key="d1">person</data>
  <data key="d2">Dan Hendrycks is a researcher known for his work on evaluating machine learning models, including contributions to the Math Dataset.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Collin Burns">
  <data key="d0">Collin Burns</data>
  <data key="d1">person</data>
  <data key="d2">Collin Burns is a researcher who has collaborated on projects related to evaluating mathematical problem-solving in machine learning models.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Saurav Kadavath">
  <data key="d0">Saurav Kadavath</data>
  <data key="d1">person</data>
  <data key="d2">Saurav Kadavath is a researcher involved in machine learning evaluation, particularly in the context of mathematical datasets.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Akul Arora">
  <data key="d0">Akul Arora</data>
  <data key="d1">person</data>
  <data key="d2">Akul Arora is a researcher focused on machine learning evaluation techniques, particularly in mathematical problem-solving.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Steven Basart">
  <data key="d0">Steven Basart</data>
  <data key="d1">person</data>
  <data key="d2">Steven Basart is a researcher contributing to the evaluation of machine learning models, particularly in mathematical contexts.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Dawn Song">
  <data key="d0">Dawn Song</data>
  <data key="d1">person</data>
  <data key="d2">Dawn Song is a prominent researcher in the field of machine learning and artificial intelligence, contributing to various evaluation methodologies.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jacob Steinhardt">
  <data key="d0">Jacob Steinhardt</data>
  <data key="d1">person</data>
  <data key="d2">Jacob Steinhardt is a researcher known for his work on machine learning evaluation and safety, particularly in the context of mathematical datasets.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Edward J Hu">
  <data key="d0">Edward J Hu</data>
  <data key="d1">person</data>
  <data key="d2">Edward J Hu is a researcher focused on low-rank adaptation techniques for large language models, contributing to advancements in model efficiency.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Yelong Shen">
  <data key="d0">Yelong Shen</data>
  <data key="d1">person</data>
  <data key="d2">Yelong Shen is involved in research on adapting language models, particularly through low-rank techniques to improve performance.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Phillip Wallis">
  <data key="d0">Phillip Wallis</data>
  <data key="d1">person</data>
  <data key="d2">Phillip Wallis is a researcher contributing to the development of techniques for adapting large language models.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Zeyuan Allen-Zhu">
  <data key="d0">Zeyuan Allen-Zhu</data>
  <data key="d1">person</data>
  <data key="d2">Zeyuan Allen-Zhu is a researcher known for his work on optimization techniques in machine learning, particularly in model adaptation.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Yuanzhi Li">
  <data key="d0">Yuanzhi Li</data>
  <data key="d1">person</data>
  <data key="d2">Yuanzhi Li is a researcher contributing to the field of machine learning, particularly in the context of adapting large models.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Shean Wang">
  <data key="d0">Shean Wang</data>
  <data key="d1">person</data>
  <data key="d2">Shean Wang is a researcher focused on enhancing the performance of language models through adaptation techniques.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Lu Wang">
  <data key="d0">Lu Wang</data>
  <data key="d1">person</data>
  <data key="d2">Lu Wang is involved in research on machine learning, focusing on adaptation strategies for large language models.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Weizhu Chen">
  <data key="d0">Weizhu Chen</data>
  <data key="d1">person</data>
  <data key="d2">Weizhu Chen is a researcher known for contributions to the efficiency and performance of language models through innovative adaptation methods.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Wenxiang Jiao">
  <data key="d0">Wenxiang Jiao</data>
  <data key="d1">person</data>
  <data key="d2">Wenxiang Jiao is a researcher focused on evaluating language models, particularly in translation tasks.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Wenxuan Wang">
  <data key="d0">Wenxuan Wang</data>
  <data key="d1">person</data>
  <data key="d2">Wenxuan Wang is involved in research on language model evaluation and translation quality assessment.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jen-tse Huang">
  <data key="d0">Jen-tse Huang</data>
  <data key="d1">person</data>
  <data key="d2">Jen-tse Huang is a researcher contributing to the evaluation of language models, particularly in translation contexts.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Xing Wang">
  <data key="d0">Xing Wang</data>
  <data key="d1">person</data>
  <data key="d2">Xing Wang is a researcher focused on machine learning evaluation and its applications in translation tasks.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Zhaopeng Tu">
  <data key="d0">Zhaopeng Tu</data>
  <data key="d1">person</data>
  <data key="d2">Zhaopeng Tu is involved in research on evaluating machine learning models, particularly in language translation.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Xisen Jin">
  <data key="d0">Xisen Jin</data>
  <data key="d1">person</data>
  <data key="d2">Xisen Jin is a researcher focused on knowledge fusion and its applications in language models.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Xiang Ren">
  <data key="d0">Xiang Ren</data>
  <data key="d1">person</data>
  <data key="d2">Xiang Ren is a researcher known for contributions to natural language processing, particularly in knowledge fusion techniques.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Daniel Preotiuc-Pietro">
  <data key="d0">Daniel Preotiuc-Pietro</data>
  <data key="d1">person</data>
  <data key="d2">Daniel Preotiuc-Pietro is a researcher involved in the development of techniques for knowledge integration in language models.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Pengxiang Cheng">
  <data key="d0">Pengxiang Cheng</data>
  <data key="d1">person</data>
  <data key="d2">Pengxiang Cheng is a researcher focused on machine learning and knowledge integration techniques.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Alex Krizhevsky">
  <data key="d0">Alex Krizhevsky</data>
  <data key="d1">person</data>
  <data key="d2">Alex Krizhevsky is a researcher known for his work on deep learning and convolutional neural networks.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Geoffrey Hinton">
  <data key="d0">Geoffrey Hinton</data>
  <data key="d1">person</data>
  <data key="d2">Geoffrey Hinton is a prominent figure in artificial intelligence and deep learning, known for his contributions to neural networks.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Xuechen Li">
  <data key="d0">Xuechen Li</data>
  <data key="d1">person</data>
  <data key="d2">Xuechen Li is a researcher involved in evaluating instruction-following models and their performance.&lt;SEP&gt;Xuechen Li is a researcher who works on enhancing language models and AI systems.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Tianyi Zhang">
  <data key="d0">Tianyi Zhang</data>
  <data key="d1">person</data>
  <data key="d2">Tianyi Zhang is a researcher focused on evaluating machine learning models, particularly in instruction-following tasks.&lt;SEP&gt;Tianyi Zhang is involved in research related to language models and AI applications.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Yann Dubois">
  <data key="d0">Yann Dubois</data>
  <data key="d1">person</data>
  <data key="d2">Yann Dubois is a researcher contributing to the evaluation of language models and their capabilities.&lt;SEP&gt;Yann Dubois is a researcher focusing on AI methodologies and language understanding.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Rohan Taori">
  <data key="d0">Rohan Taori</data>
  <data key="d1">person</data>
  <data key="d2">Rohan Taori is a researcher known for his work on evaluating machine learning models, particularly in instruction-following contexts.&lt;SEP&gt;Rohan Taori is a researcher working on instruction-following models and AI methodologies.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Ishaan Gulrajani">
  <data key="d0">Ishaan Gulrajani</data>
  <data key="d1">person</data>
  <data key="d2">Ishaan Gulrajani is a researcher contributing to advancements in AI and language models.&lt;SEP&gt;Ishaan Gulrajani is a researcher focused on machine learning evaluation and performance assessment.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Carlos Guestrin">
  <data key="d0">Carlos Guestrin</data>
  <data key="d1">person</data>
  <data key="d2">Carlos Guestrin is a researcher known for contributions to machine learning and its evaluation methodologies.&lt;SEP&gt;Carlos Guestrin is a researcher known for his contributions to machine learning and AI.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Percy Liang">
  <data key="d0">Percy Liang</data>
  <data key="d1">person</data>
  <data key="d2">Percy Liang is a researcher involved in natural language processing and machine learning evaluation techniques.&lt;SEP&gt;Percy Liang is a researcher specializing in machine learning and natural language processing.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Tatsunori B Hashimoto">
  <data key="d0">Tatsunori B Hashimoto</data>
  <data key="d1">person</data>
  <data key="d2">Tatsunori B Hashimoto is a researcher focused on evaluating instruction-following models and their applications.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Norman Sadeh">
  <data key="d0">Norman Sadeh</data>
  <data key="d1">person</data>
  <data key="d2">Norman Sadeh is a researcher involved in natural language inference, contributing to academic publications.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Carolyn Rose">
  <data key="d0">Carolyn Rose</data>
  <data key="d1">person</data>
  <data key="d2">Carolyn Rose is a researcher in the field of natural language processing, co-authoring relevant studies.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Graham Neubig">
  <data key="d0">Graham Neubig</data>
  <data key="d1">person</data>
  <data key="d2">Graham Neubig is an academic who works on natural language processing and inference, contributing to various research papers.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Daye Nam">
  <data key="d0">Daye Nam</data>
  <data key="d1">person</data>
  <data key="d2">Daye Nam is a researcher who has worked on code understanding using language models.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Andrew Macvean">
  <data key="d0">Andrew Macvean</data>
  <data key="d1">person</data>
  <data key="d2">Andrew Macvean is a contributor to research on code understanding and programming languages.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Vincent Hellendoorn">
  <data key="d0">Vincent Hellendoorn</data>
  <data key="d1">person</data>
  <data key="d2">Vincent Hellendoorn is involved in research related to code understanding and artificial intelligence.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Bogdan Vasilescu">
  <data key="d0">Bogdan Vasilescu</data>
  <data key="d1">person</data>
  <data key="d2">Bogdan Vasilescu is a researcher focused on software engineering and programming.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Brad Myers">
  <data key="d0">Brad Myers</data>
  <data key="d1">person</data>
  <data key="d2">Brad Myers is a prominent figure in the field of human-computer interaction and software engineering.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="R OpenAI">
  <data key="d0">R OpenAI</data>
  <data key="d1">organization</data>
  <data key="d2">R OpenAI is an organization that conducts research in artificial intelligence, including the development of language models.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="GPT-4 Technical Report">
  <data key="d0">GPT-4 Technical Report</data>
  <data key="d1">event</data>
  <data key="d2">The GPT-4 Technical Report details the advancements and capabilities of the GPT-4 language model.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Stanford Alpaca">
  <data key="d0">Stanford Alpaca</data>
  <data key="d1">organization</data>
  <data key="d2">Stanford Alpaca is a model that serves as a basis for the Llama-2-13b-code-alpaca, focusing on instruction-following tasks.&lt;SEP&gt;Stanford Alpaca is a model that serves as a foundation for Llama-2-13b-code-alpaca, focusing on instruction-following tasks.&lt;SEP&gt;Stanford Alpaca is a project focused on instruction-following models in the field of AI.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2&lt;SEP&gt;chunk-9fbd0c0622e4d5fb84be83781e1c4521</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Imagenet Challenge">
  <data key="d0">Imagenet Challenge</data>
  <data key="d1">event</data>
  <data key="d2">The Imagenet Challenge is a significant competition in computer vision, assessing large-scale visual recognition.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Speech Commands Dataset">
  <data key="d0">Speech Commands Dataset</data>
  <data key="d1">event</data>
  <data key="d2">The Speech Commands Dataset is a collection of audio samples used for training speech recognition systems.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Chain-of-Thought Prompting">
  <data key="d0">Chain-of-Thought Prompting</data>
  <data key="d1">event</data>
  <data key="d2">Chain-of-Thought Prompting is a technique that enhances reasoning abilities in language models.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Richard Socher">
  <data key="d0">Richard Socher</data>
  <data key="d1">person</data>
  <data key="d2">Richard Socher is a researcher known for his contributions to natural language processing and deep learning.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Alex Perelygin">
  <data key="d0">Alex Perelygin</data>
  <data key="d1">person</data>
  <data key="d2">Alex Perelygin is a researcher who has worked on various aspects of natural language processing.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jean Wu">
  <data key="d0">Jean Wu</data>
  <data key="d1">person</data>
  <data key="d2">Jean Wu is a researcher involved in natural language processing, particularly in sentiment analysis.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jason Chuang">
  <data key="d0">Jason Chuang</data>
  <data key="d1">person</data>
  <data key="d2">Jason Chuang is a researcher who has contributed to the field of natural language processing and deep learning.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Christopher D Manning">
  <data key="d0">Christopher D Manning</data>
  <data key="d1">person</data>
  <data key="d2">Christopher D Manning is a prominent figure in the field of natural language processing, known for his work on linguistic structure and machine learning.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Andrew Y Ng">
  <data key="d0">Andrew Y Ng</data>
  <data key="d1">person</data>
  <data key="d2">Andrew Y Ng is a renowned AI researcher and educator, co-founder of Google Brain and Coursera.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Christopher Potts">
  <data key="d0">Christopher Potts</data>
  <data key="d1">person</data>
  <data key="d2">Christopher Potts is a researcher specializing in semantics and natural language processing.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Tatsunori B. Hashimoto">
  <data key="d0">Tatsunori B. Hashimoto</data>
  <data key="d1">person</data>
  <data key="d2">Tatsunori B. Hashimoto is a researcher focused on AI and its applications in language understanding.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Hugo Touvron">
  <data key="d0">Hugo Touvron</data>
  <data key="d1">person</data>
  <data key="d2">Hugo Touvron is a researcher working on advanced AI models and methodologies.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Louis Martin">
  <data key="d0">Louis Martin</data>
  <data key="d1">person</data>
  <data key="d2">Louis Martin is involved in research related to AI and machine learning.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Kevin Stone">
  <data key="d0">Kevin Stone</data>
  <data key="d1">person</data>
  <data key="d2">Kevin Stone is a researcher contributing to advancements in AI technologies.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Peter Albert">
  <data key="d0">Peter Albert</data>
  <data key="d1">person</data>
  <data key="d2">Peter Albert is a researcher focused on AI applications and model development.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Amjad Almahairi">
  <data key="d0">Amjad Almahairi</data>
  <data key="d1">person</data>
  <data key="d2">Amjad Almahairi is a researcher working on AI and machine learning methodologies.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Yasmine Babaei">
  <data key="d0">Yasmine Babaei</data>
  <data key="d1">person</data>
  <data key="d2">Yasmine Babaei is involved in AI research, particularly in model development.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Nikolay Bashlykov">
  <data key="d0">Nikolay Bashlykov</data>
  <data key="d1">person</data>
  <data key="d2">Nikolay Bashlykov is a researcher contributing to advancements in AI and machine learning.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Soumya Batra">
  <data key="d0">Soumya Batra</data>
  <data key="d1">person</data>
  <data key="d2">Soumya Batra is a researcher working on AI methodologies and applications.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Prajjwal Bhargava">
  <data key="d0">Prajjwal Bhargava</data>
  <data key="d1">person</data>
  <data key="d2">Prajjwal Bhargava is involved in AI research, particularly in model training and evaluation.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Shruti Bhosale">
  <data key="d0">Shruti Bhosale</data>
  <data key="d1">person</data>
  <data key="d2">Shruti Bhosale is a researcher focusing on AI technologies and their applications.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="arXiv preprint arXiv:2410.09335">
  <data key="d0">arXiv preprint arXiv:2410.09335</data>
  <data key="d1">event</data>
  <data key="d2">This arXiv preprint discusses data selection at scale in machine learning, highlighting significant findings.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="ACM Transactions on Management Information Systems">
  <data key="d0">ACM Transactions on Management Information Systems</data>
  <data key="d1">organization</data>
  <data key="d2">This organization publishes research related to management information systems and technology.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="The Twelfth International Conference on Learning Representations">
  <data key="d0">The Twelfth International Conference on Learning Representations</data>
  <data key="d1">event</data>
  <data key="d2">This conference focuses on learning representations in machine learning and artificial intelligence.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="arXiv preprint arXiv:2306.01708">
  <data key="d0">arXiv preprint arXiv:2306.01708</data>
  <data key="d1">event</data>
  <data key="d2">This arXiv preprint presents research on resolving interference when merging models in machine learning.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="arXiv preprint arXiv:2408.07666">
  <data key="d0">arXiv preprint arXiv:2408.07666</data>
  <data key="d1">event</data>
  <data key="d2">This arXiv preprint discusses model merging in large language models and related applications.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Forty-first International Conference on Machine Learning">
  <data key="d0">Forty-first International Conference on Machine Learning</data>
  <data key="d1">event</data>
  <data key="d2">This conference is a major venue for presenting new research in the field of machine learning.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="mixup">
  <data key="d0">mixup</data>
  <data key="d1">category</data>
  <data key="d2">Mixup is a technique used in machine learning to improve model generalization by blending training examples.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Promptrobust">
  <data key="d0">Promptrobust</data>
  <data key="d1">category</data>
  <data key="d2">Promptrobust refers to a framework for evaluating the robustness of large language models against adversarial prompts.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Promptbench">
  <data key="d0">Promptbench</data>
  <data key="d1">category</data>
  <data key="d2">Promptbench is a unified library designed for evaluating large language models in various tasks.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Tingyu Xia">
  <data key="d0">Tingyu Xia</data>
  <data key="d1">person</data>
  <data key="d2">Tingyu Xia is a researcher who co-authored a paper on data selection at scale, emphasizing the effectiveness of random selection in machine learning.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Bowen Yu">
  <data key="d0">Bowen Yu</data>
  <data key="d1">person</data>
  <data key="d2">Bowen Yu is a co-author of the paper discussing data selection methods in machine learning.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Kai Dang">
  <data key="d0">Kai Dang</data>
  <data key="d1">person</data>
  <data key="d2">Kai Dang is a contributor to the research on data selection at scale in machine learning.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="An Yang">
  <data key="d0">An Yang</data>
  <data key="d1">person</data>
  <data key="d2">An Yang is a co-author of the paper focusing on data selection techniques in machine learning.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Yuan Tian">
  <data key="d0">Yuan Tian</data>
  <data key="d1">person</data>
  <data key="d2">Yuan Tian is a co-author of the research discussing random selection in machine learning.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Junyang Lin">
  <data key="d0">Junyang Lin</data>
  <data key="d1">person</data>
  <data key="d2">Junyang Lin is a co-author of the paper focusing on effective data selection methods for machine learning.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Frank Xing">
  <data key="d0">Frank Xing</data>
  <data key="d1">person</data>
  <data key="d2">Frank Xing is a researcher who authored a paper on designing heterogeneous language model agents for financial sentiment analysis.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Can Xu">
  <data key="d0">Can Xu</data>
  <data key="d1">person</data>
  <data key="d2">Can Xu is a co-author of the paper discussing empowering large pre-trained language models to follow complex instructions.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Qingfeng Sun">
  <data key="d0">Qingfeng Sun</data>
  <data key="d1">person</data>
  <data key="d2">Qingfeng Sun is a researcher involved in the study of large pre-trained language models for instruction following.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Kai Zheng">
  <data key="d0">Kai Zheng</data>
  <data key="d1">person</data>
  <data key="d2">Kai Zheng is a contributor to the research on enhancing language models to follow complex instructions.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Xiubo Geng">
  <data key="d0">Xiubo Geng</data>
  <data key="d1">person</data>
  <data key="d2">Xiubo Geng is a co-author of the paper on empowering language models for complex tasks.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Pu Zhao">
  <data key="d0">Pu Zhao</data>
  <data key="d1">person</data>
  <data key="d2">Pu Zhao is a researcher involved in the study of large language models and their capabilities.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jiazhan Feng">
  <data key="d0">Jiazhan Feng</data>
  <data key="d1">person</data>
  <data key="d2">Jiazhan Feng is a co-author of the research on instruction-following capabilities of language models.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Chongyang Tao">
  <data key="d0">Chongyang Tao</data>
  <data key="d1">person</data>
  <data key="d2">Chongyang Tao is a contributor to the research on enhancing language models for complex instructions.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Qingwei Lin">
  <data key="d0">Qingwei Lin</data>
  <data key="d1">person</data>
  <data key="d2">Qingwei Lin is a co-author of the paper discussing advancements in large language models.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Daxin Jiang">
  <data key="d0">Daxin Jiang</data>
  <data key="d1">person</data>
  <data key="d2">Daxin Jiang is a researcher involved in the study of empowering language models for better performance.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Evol-Instruct">
  <data key="d0">Evol-Instruct</data>
  <data key="d1">category</data>
  <data key="d2">Evol-Instruct is a method used to generate high-complexity instruction data to improve the performance of language models.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="PMLR">
  <data key="d0">PMLR</data>
  <data key="d1">organization</data>
  <data key="d2">PMLR is the publisher associated with the International Conference on Machine Learning, disseminating research in machine learning.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Proceedings of the 1st ACM Workshop on Large AI Systems and Models with Privacy and Safety Analysis">
  <data key="d0">Proceedings of the 1st ACM Workshop on Large AI Systems and Models with Privacy and Safety Analysis</data>
  <data key="d1">event</data>
  <data key="d2">This event focuses on the implications and challenges of large AI systems, including privacy and safety concerns.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Journal of Machine Learning Research">
  <data key="d0">Journal of Machine Learning Research</data>
  <data key="d1">organization</data>
  <data key="d2">The Journal of Machine Learning Research publishes significant research findings in the field of machine learning.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="WizardMath-Mistral 7B">
  <data key="d0">WizardMath-Mistral 7B</data>
  <data key="d1">organization</data>
  <data key="d2">WizardMath-Mistral 7B is a mathematical model that excels in basic and advanced math problems, outperforming other open-source models.&lt;SEP&gt;WizardMath-Mistral 7B is a mathematical model that excels in both basic and advanced math problems, outperforming all open-source models with fewer training data.</data>
  <data key="d3">chunk-9fbd0c0622e4d5fb84be83781e1c4521</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="WizardMath 70B">
  <data key="d0">WizardMath 70B</data>
  <data key="d1">organization</data>
  <data key="d2">WizardMath 70B is a mathematical model that surpasses GPT-3.5-Turbo and Claude 2 in mathematical reasoning tasks.&lt;SEP&gt;WizardMath 70B is a mathematical model that surpasses GPT-3.5-Turbo, Claude 2, and early GPT-4 versions in mathematical reasoning tasks.</data>
  <data key="d3">chunk-9fbd0c0622e4d5fb84be83781e1c4521</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="GPT-3.5-Turbo">
  <data key="d0">GPT-3.5-Turbo</data>
  <data key="d1">organization</data>
  <data key="d2">GPT-3.5-Turbo is a language model compared against WizardMath 70B for its performance in mathematical reasoning tasks.&lt;SEP&gt;GPT-3.5-Turbo is a language model that is compared against WizardMath 70B in terms of mathematical reasoning capabilities.</data>
  <data key="d3">chunk-9fbd0c0622e4d5fb84be83781e1c4521</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Claude 2">
  <data key="d0">Claude 2</data>
  <data key="d1">organization</data>
  <data key="d2">Claude 2 is a language model evaluated alongside other models, including WizardMath 70B, for mathematical reasoning tasks.&lt;SEP&gt;Claude 2 is a language model that is evaluated alongside GPT-3.5-Turbo and WizardMath 70B for mathematical reasoning tasks.</data>
  <data key="d3">chunk-9fbd0c0622e4d5fb84be83781e1c4521</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Llama-2-13b-code-alpaca">
  <data key="d0">Llama-2-13b-code-alpaca</data>
  <data key="d1">organization</data>
  <data key="d2">Llama-2-13b-code-alpaca is a code generation model fine-tuned from Llama-2-13b to enhance code understanding and generation.&lt;SEP&gt;Llama-2-13b-code-alpaca is a code generation model fine-tuned to enhance code understanding and generation.</data>
  <data key="d3">chunk-9fbd0c0622e4d5fb84be83781e1c4521</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Hyperparameter Settings">
  <data key="d0">Hyperparameter Settings</data>
  <data key="d1">category</data>
  <data key="d2">Hyperparameter Settings refer to the configuration values used in machine learning models to control the learning process, such as scaling terms and retain ratios.</data>
  <data key="d3">chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Out-of-Distribution Dataset Selection">
  <data key="d0">Out-of-Distribution Dataset Selection</data>
  <data key="d1">event</data>
  <data key="d2">Out-of-Distribution Dataset Selection involves choosing datasets that help evaluate the robustness of models against data that differs from their training set.</data>
  <data key="d3">chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Adversarial Robustness Evaluation">
  <data key="d0">Adversarial Robustness Evaluation</data>
  <data key="d1">event</data>
  <data key="d2">Adversarial Robustness Evaluation refers to the process of testing how well models perform when faced with adversarial prompts designed to mislead them.</data>
  <data key="d3">chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Coding">
  <data key="d0">Coding</data>
  <data key="d1">category</data>
  <data key="d2">Coding is a task category in LiveBench that evaluates a model's capability to perform coding-related tasks.</data>
  <data key="d3">chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Language Comprehension">
  <data key="d0">Language Comprehension</data>
  <data key="d1">category</data>
  <data key="d2">Language Comprehension is a task category in LiveBench that tests a model's understanding of language and its nuances.</data>
  <data key="d3">chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Task Vectors">
  <data key="d0">Task Vectors</data>
  <data key="d1">category</data>
  <data key="d2">Task Vectors are mathematical representations used in machine learning to encapsulate information about tasks, which can be merged or manipulated for various purposes.</data>
  <data key="d3">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Mean Figure">
  <data key="d0">Mean Figure</data>
  <data key="d1">category</data>
  <data key="d2">Mean Figure refers to a statistical measure that represents the average of a set of values, often used in the context of data analysis.</data>
  <data key="d3">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Merged Task Vector">
  <data key="d0">Merged Task Vector</data>
  <data key="d1">category</data>
  <data key="d2">Merged Task Vector is a composite representation created by combining two or more task vectors, typically used to enhance model performance.</data>
  <data key="d3">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Parameter Value">
  <data key="d0">Parameter Value</data>
  <data key="d1">category</data>
  <data key="d2">Parameter Value denotes specific settings or configurations used in machine learning models that affect their behavior and outcomes.</data>
  <data key="d3">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Evaluation">
  <data key="d0">Evaluation</data>
  <data key="d1">event</data>
  <data key="d2">Evaluation refers to the systematic assessment of model performance using specific datasets and metrics to determine effectiveness.</data>
  <data key="d3">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Extraneous Information">
  <data key="d0">Extraneous Information</data>
  <data key="d1">category</data>
  <data key="d2">Extraneous Information refers to data or inputs that are not directly relevant to the task at hand, which can confuse or mislead models.</data>
  <data key="d3">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Sentiment Analysis Dataset">
  <data key="d0">Sentiment Analysis Dataset</data>
  <data key="d1">category</data>
  <data key="d2">Sentiment Analysis Dataset encompasses collections of textual data used for training models to classify sentiments as positive, negative, or neutral.</data>
  <data key="d3">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Grammar Correctness Dataset">
  <data key="d0">Grammar Correctness Dataset</data>
  <data key="d1">category</data>
  <data key="d2">Grammar Correctness Dataset is a collection of sentences used to evaluate the grammatical accuracy of language models.</data>
  <data key="d3">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Model Performance Metrics">
  <data key="d0">Model Performance Metrics</data>
  <data key="d1">category</data>
  <data key="d2">Model Performance Metrics are quantitative measures used to assess the effectiveness of machine learning models in various tasks.</data>
  <data key="d3">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Metric_no attack">
  <data key="d0">Metric_no attack</data>
  <data key="d1">category</data>
  <data key="d2">Metric_no attack denotes the performance metric of a model without any prompt attack applied.</data>
  <data key="d3">chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Metric_attack">
  <data key="d0">Metric_attack</data>
  <data key="d1">category</data>
  <data key="d2">Metric_attack represents the performance metric of a model when subjected to a prompt attack.</data>
  <data key="d3">chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="delta parameters">
  <data key="d0">delta parameters</data>
  <data key="d1">category</data>
  <data key="d2">Delta parameters refer to the differences between fine-tuned and pre-trained parameters in machine learning models.</data>
  <data key="d3">chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="model parameters">
  <data key="d0">model parameters</data>
  <data key="d1">category</data>
  <data key="d2">Model parameters are the variables in a model that are learned from training data and adjusted during the training process.</data>
  <data key="d3">chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="adversarial robustness">
  <data key="d0">adversarial robustness</data>
  <data key="d1">category</data>
  <data key="d2">Adversarial robustness refers to a model's ability to maintain performance when subjected to adversarial attacks.</data>
  <data key="d3">chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Super-Mario v2">
  <data key="d0">Super-Mario v2</data>
  <data key="d1">organization</data>
  <data key="d2">Super-Mario v2 is a language model that ranked first among models of the same scale on the Open LLM Leaderboard after being merged using DARE.</data>
  <data key="d3">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Marthe Ballon">
  <data key="d0">Marthe Ballon</data>
  <data key="d1">person</data>
  <data key="d2">Marthe Ballon is a researcher affiliated with the Data Analytics Lab at Vrije Universiteit Brussel, contributing to the study of reasoning and performance in language models.</data>
  <data key="d3">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Andres Algaba">
  <data key="d0">Andres Algaba</data>
  <data key="d1">person</data>
  <data key="d2">Andres Algaba is a researcher associated with both the Data Analytics Lab at Vrije Universiteit Brussel and the School of Engineering and Applied Sciences at Harvard University.</data>
  <data key="d3">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Vincent Ginis">
  <data key="d0">Vincent Ginis</data>
  <data key="d1">person</data>
  <data key="d2">Vincent Ginis is a researcher involved in the study of large language models, affiliated with the Data Analytics Lab and Harvard University.</data>
  <data key="d3">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Data Analytics Lab">
  <data key="d0">Data Analytics Lab</data>
  <data key="d1">organization</data>
  <data key="d2">The Data Analytics Lab at Vrije Universiteit Brussel conducts research on data analysis and machine learning, including studies on language models.</data>
  <data key="d3">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Vrije Universiteit Brussel">
  <data key="d0">Vrije Universiteit Brussel</data>
  <data key="d1">organization</data>
  <data key="d2">Vrije Universiteit Brussel is a university in Belgium known for its research and education in various fields, including data science.</data>
  <data key="d3">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Harvard University">
  <data key="d0">Harvard University</data>
  <data key="d1">organization</data>
  <data key="d2">Harvard University is a prestigious institution in the United States, recognized for its research and academic programs, including engineering and applied sciences.</data>
  <data key="d3">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Benjamin Mann&quot;&lt;||&gt;&quot;Tom Brown and Benjamin Mann are co-authors of a paper discussing language models as few-shot learners.">
  <data key="d0">Benjamin Mann"&lt;||&gt;"Tom Brown and Benjamin Mann are co-authors of a paper discussing language models as few-shot learners.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d2">co-authorship, research</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jared D Kaplan&quot;&lt;||&quot;Melanie Subbiah and Jared D Kaplan are co-authors of a paper discussing language models as few-shot learners.">
  <data key="d0">Jared D Kaplan"&lt;||"Melanie Subbiah and Jared D Kaplan are co-authors of a paper discussing language models as few-shot learners.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d2">co-authorship, research</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Eric Tang&quot;&lt;||&quot;Steven Basart and Eric Tang have worked together on projects evaluating machine learning models, particularly in mathematics.">
  <data key="d0">Eric Tang"&lt;||"Steven Basart and Eric Tang have worked together on projects evaluating machine learning models, particularly in mathematics.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d2">research collaboration, evaluation focus</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jacob Steinhardt&quot;&lt;||&quot;Dawn Song and Jacob Steinhardt are both prominent researchers in machine learning, collaborating on evaluation methodologies.">
  <data key="d0">Jacob Steinhardt"&lt;||"Dawn Song and Jacob Steinhardt are both prominent researchers in machine learning, collaborating on evaluation methodologies.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d2">collaboration, research focus</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Stress test evaluation for natural language inference">
  <data key="d0">Stress test evaluation for natural language inference</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d2">Norman Sadeh contributed to the research on stress test evaluation in natural language inference, indicating his involvement in the study.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Using an llm to help with code understanding">
  <data key="d0">Using an llm to help with code understanding</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d2">Daye Nam is a contributor to research that explores the use of large language models for code understanding, indicating his role in software engineering research.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Instruction-following llama model">
  <data key="d0">Instruction-following llama model</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d2">Stanford Alpaca is a project that focuses on instruction-following models, showcasing advancements in AI technology.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Large scale visual recognition challenge">
  <data key="d0">Large scale visual recognition challenge</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d2">The Imagenet Challenge assesses large-scale visual recognition capabilities, indicating its significance in computer vision research.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Launching the speech commands dataset">
  <data key="d0">Launching the speech commands dataset</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d2">The Speech Commands Dataset was launched by R OpenAI, indicating their role in advancing speech recognition technology.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Chain-of-thought prompting elicits reasoning in large language models">
  <data key="d0">Chain-of-thought prompting elicits reasoning in large language models</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d2">Chain-of-Thought Prompting is a technique used to enhance reasoning in AI, indicating its significance in model training.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Rethinking data selection at scale">
  <data key="d0">Rethinking data selection at scale</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d2">The preprint discusses data selection, a critical aspect of machine learning, reflecting ongoing research in the field.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Designing heterogeneous llm agents for financial sentiment analysis">
  <data key="d0">Designing heterogeneous llm agents for financial sentiment analysis</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d2">The research published in this journal includes advancements in designing agents for financial analysis.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Wizardlm: Empowering large pre-trained language models">
  <data key="d0">Wizardlm: Empowering large pre-trained language models</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d2">This conference features research on empowering language models, including the Wizardlm project.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Resolving interference when merging models">
  <data key="d0">Resolving interference when merging models</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d2">The preprint addresses challenges in model merging, relevant to the ongoing research in the field.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Model merging in llms, mllms, and beyond">
  <data key="d0">Model merging in llms, mllms, and beyond</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d2">This preprint discusses model merging techniques, contributing to the understanding of large language models.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Language models are super mario">
  <data key="d0">Language models are super mario</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d2">The conference presents research on innovative techniques for language models, including the Super Mario analogy.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="large AI systems">
  <data key="d0">large AI systems</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d2">This event focuses on the implications of large AI systems, discussing privacy and safety analysis.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<edge source="Takuya Akiba" target="Sakana AI">
  <data key="d5">8.0</data>
  <data key="d6">Takuya Akiba is affiliated with Sakana AI, contributing to the organization's research in evolutionary model merging.</data>
  <data key="d7">research affiliation, collaboration</data>
  <data key="d8">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Takuya Akiba" target="Evolutionary Model Merging">
  <data key="d5">27.0</data>
  <data key="d6">Takuya Akiba initiated and led the project on Evolutionary Model Merging, contributing to its foundational methodology.&lt;SEP&gt;Takuya Akiba initiated the project on evolutionary model merging, significantly contributing to its research and development.</data>
  <data key="d7">project leadership, model development&lt;SEP&gt;research contribution, project initiation</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b&lt;SEP&gt;chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Takuya Akiba" target="Evolutionary Optimization of Model Merging Recipes">
  <data key="d5">9.0</data>
  <data key="d6">Takuya Akiba initiated the project on Evolutionary Optimization of Model Merging Recipes, playing a key role in its development.</data>
  <data key="d7">project initiation, leadership</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Takuya Akiba" target="Optuna">
  <data key="d5">20.0</data>
  <data key="d6">Takuya Akiba contributed to the creation of Optuna, which is integral to optimizing machine learning models.&lt;SEP&gt;Takuya Akiba is one of the key contributors to the Optuna framework, which is essential for hyperparameter optimization in AI models.</data>
  <data key="d7">framework development, machine learning&lt;SEP&gt;framework development, optimization</data>
  <data key="d8">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Takuya Akiba" target="Ouyang et al.">
  <data key="d5">5.0</data>
  <data key="d6">Ouyang et al. is referenced alongside Takuya Akiba in the context of model merging recipes.</data>
  <data key="d7">collaboration, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Takuya Akiba" target="Makoto Shing">
  <data key="d5">6.0</data>
  <data key="d6">Takuya Akiba and Makoto Shing are co-authors of a paper on model merging recipes.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Makoto Shing" target="Sakana AI">
  <data key="d5">8.0</data>
  <data key="d6">Makoto Shing works at Sakana AI, focusing on evolutionary optimization methods in AI development.</data>
  <data key="d7">research affiliation, collaboration</data>
  <data key="d8">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Makoto Shing" target="Evolutionary Model Merging">
  <data key="d5">8.0</data>
  <data key="d6">Makoto Shing's work on expanding model merging includes vision-language models, contributing to the project's scope.</data>
  <data key="d7">project contribution, model expansion</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Makoto Shing" target="Parameter Space Model Merging">
  <data key="d5">8.0</data>
  <data key="d6">Makoto Shing's work on expanding the parameter space model merging methodology is integral to the project's success.</data>
  <data key="d7">methodology expansion, project contribution</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Makoto Shing" target="Yujin Tang">
  <data key="d5">6.0</data>
  <data key="d6">Makoto Shing and Yujin Tang are co-authors of a paper on model merging recipes.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Yujin Tang" target="Sakana AI">
  <data key="d5">8.0</data>
  <data key="d6">Yujin Tang is part of Sakana AI, engaged in research on model merging techniques using evolutionary algorithms.</data>
  <data key="d7">research affiliation, collaboration</data>
  <data key="d8">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Yujin Tang" target="Evolutionary Model Merging">
  <data key="d5">8.0</data>
  <data key="d6">Yujin Tang's direction in data flow space model merging is a key aspect of the Evolutionary Model Merging project.</data>
  <data key="d7">project direction, model merging</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Yujin Tang" target="Neural Architecture Search">
  <data key="d5">8.0</data>
  <data key="d6">Yujin Tang incorporated ideas from neural architecture search into the data flow space model merging efforts.</data>
  <data key="d7">methodological integration, project contribution</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Yujin Tang" target="David Ha">
  <data key="d5">8.0</data>
  <data key="d6">Yujin Tang and David Ha are authors who have contributed to neuroevolution research, indicating a shared focus area.</data>
  <data key="d7">collaboration, research</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Yujin Tang" target="EvoJAX: Hardware-Accelerated Neuroevolution">
  <data key="d5">9.0</data>
  <data key="d6">Yujin Tang is the author of EvoJAX: Hardware-Accelerated Neuroevolution, indicating his contribution to the research.</data>
  <data key="d7">authorship, research collaboration</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Yujin Tang" target="Qi Sun">
  <data key="d5">6.0</data>
  <data key="d6">Yujin Tang and Qi Sun are co-authors of a paper on model merging recipes.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Qi Sun" target="Sakana AI">
  <data key="d5">8.0</data>
  <data key="d6">Qi Sun is affiliated with Sakana AI, contributing to the exploration of effective model merging strategies.</data>
  <data key="d7">research affiliation, collaboration</data>
  <data key="d8">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Qi Sun" target="Evolutionary Model Merging">
  <data key="d5">8.0</data>
  <data key="d6">Qi Sun's implementation efforts in the parameter space model merging framework are crucial to the project's success.</data>
  <data key="d7">implementation, project support</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Qi Sun" target="Model Evaluation">
  <data key="d5">8.0</data>
  <data key="d6">Qi Sun's contributions to the implementation of the framework include aspects of model evaluation and performance assessment.</data>
  <data key="d7">implementation, evaluation</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Qi Sun" target="Marc Pickett">
  <data key="d5">8.0</data>
  <data key="d6">Qi Sun and Marc Pickett co-authored a paper on transformer layers, indicating their work in AI model development.</data>
  <data key="d7">collaboration, research</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Qi Sun" target="Transformer Layers as Painters">
  <data key="d5">9.0</data>
  <data key="d6">Qi Sun is an author of the paper Transformer Layers as Painters, indicating his involvement in the research.</data>
  <data key="d7">authorship, research collaboration</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Qi Sun" target="David Ha">
  <data key="d5">6.0</data>
  <data key="d6">Qi Sun and David Ha are co-authors of a paper on model merging recipes.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="David Ha" target="Sakana AI">
  <data key="d5">8.0</data>
  <data key="d6">David Ha is a researcher at Sakana AI, involved in the development of advanced AI models through evolutionary methods.</data>
  <data key="d7">research affiliation, collaboration</data>
  <data key="d8">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="David Ha" target="Evolutionary Model Merging">
  <data key="d5">9.0</data>
  <data key="d6">David Ha's guidance and technical insight provide essential support for the Evolutionary Model Merging project.</data>
  <data key="d7">guidance, project support</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="David Ha" target="Ethical and Societal Impact">
  <data key="d5">9.0</data>
  <data key="d6">David Ha provided guidance on the ethical and societal impacts of the research, ensuring responsible AI deployment.</data>
  <data key="d7">guidance, ethical considerations</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="David Ha" target="arXiv preprint">
  <data key="d5">16.0</data>
  <data key="d6">David Ha's work on hypernetworks is documented in an arXiv preprint, contributing to the understanding of neural network architectures.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Sakana AI" target="Tokyo">
  <data key="d5">9.0</data>
  <data key="d6">Sakana AI is located in Tokyo, Japan, where it conducts its research and development activities.</data>
  <data key="d7">organization location, operational base</data>
  <data key="d8">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Sakana AI" target="Large Language Models">
  <data key="d5">8.0</data>
  <data key="d6">Sakana AI is involved in the development of Large Language Models, utilizing innovative techniques such as model merging to enhance capabilities.</data>
  <data key="d7">AI development, innovative techniques</data>
  <data key="d8">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Sakana AI" target="VLM Experiments">
  <data key="d5">16.0</data>
  <data key="d6">Sakana AI provides benchmark datasets for VLM Experiments, contributing to advancements in visual-language models.&lt;SEP&gt;Sakana AI provides benchmark datasets that are crucial for conducting VLM experiments in the research.</data>
  <data key="d7">dataset provision, model evaluation&lt;SEP&gt;dataset provision, research support</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Sakana AI" target="Japanese Stable LM Beta">
  <data key="d5">9.0</data>
  <data key="d6">Japanese Stable LM Beta is developed by Sakana AI, showcasing their efforts in language model advancements.</data>
  <data key="d7">model development, organizational effort</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Tokyo" target="Peace Tower">
  <data key="d5">8.0</data>
  <data key="d6">The Peace Tower is located in Tokyo, specifically in Shibuya Ward, making it a part of the city's landscape.</data>
  <data key="d7">location, significance</data>
  <data key="d8">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Model Merge" target="Model Merging">
  <data key="d5">9.0</data>
  <data key="d6">Evolutionary Model Merge is a specific approach to model merging that automates the discovery of effective combinations of models.</data>
  <data key="d7">methodology, optimization</data>
  <data key="d8">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Model Merge" target="Japanese LLM">
  <data key="d5">26.0</data>
  <data key="d6">The Evolutionary Model Merge approach utilizes the Japanese LLM to solve math problems, demonstrating its application in merging models across different domains.&lt;SEP&gt;The Japanese LLM was developed using the Evolutionary Model Merge approach, demonstrating its effectiveness in creating advanced models.</data>
  <data key="d7">model application, domain merging&lt;SEP&gt;model development, application</data>
  <data key="d8">chunk-72a25142a6a0a013507aa055b73531c4&lt;SEP&gt;chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Model Merge" target="Japanese VLM">
  <data key="d5">10.0</data>
  <data key="d6">The Japanese VLM was also generated through the Evolutionary Model Merge methodology, showcasing its capabilities in cultural contexts.</data>
  <data key="d7">model development, application</data>
  <data key="d8">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Model Merge" target="Japanese Math LLM">
  <data key="d5">10.0</data>
  <data key="d6">The Japanese Math LLM was developed using the Evolutionary Model Merge approach, showcasing the capabilities of this methodology in enhancing model performance.</data>
  <data key="d7">model development, optimization technique</data>
  <data key="d8">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Model Merge" target="Culturally-Aware Japanese VLM">
  <data key="d5">10.0</data>
  <data key="d6">The Culturally-Aware Japanese VLM was generated through the Evolutionary Model Merge methodology, demonstrating its effectiveness in producing culturally relevant models.</data>
  <data key="d7">model development, cultural relevance</data>
  <data key="d8">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese LLM" target="Math LLMs">
  <data key="d5">20.0</data>
  <data key="d6">The Japanese LLM is merged with Math LLMs to create a model capable of solving math problems in Japanese, indicating collaboration between different types of models.</data>
  <data key="d7">collaboration, model merging</data>
  <data key="d8">chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese LLM" target="Section 3.1">
  <data key="d5">8.0</data>
  <data key="d6">Section 3.1 details the experimental setup for evolving the Japanese LLM, highlighting its role in the merging process.</data>
  <data key="d7">experimental setup, model development</data>
  <data key="d8">chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese LLM" target="Section 3.3">
  <data key="d5">9.0</data>
  <data key="d6">Section 3.3 describes the merging of the Japanese LLM with an English VLM, illustrating cross-linguistic model integration.</data>
  <data key="d7">cross-linguistic integration, model merging</data>
  <data key="d8">chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese LLM" target="MGSM-JA task">
  <data key="d5">9.0</data>
  <data key="d6">The integration of a Japanese LLM in Model 6 contributes to its proficiency in tackling problems in the MGSM-JA task.</data>
  <data key="d7">language integration, task performance</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese LLM" target="Evolutionary Model Merging">
  <data key="d5">16.0</data>
  <data key="d6">The Japanese LLM utilizes evolutionary model merging to enhance its capabilities in math reasoning and cultural content.</data>
  <data key="d7">model enhancement, AI development</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese VLM" target="Evolutionary Model Merging">
  <data key="d5">16.0</data>
  <data key="d6">The Japanese VLM benefits from evolutionary model merging, which improves its performance on vision-language tasks.</data>
  <data key="d7">model enhancement, AI development</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Open LLM Leaderboard">
  <data key="d5">9.0</data>
  <data key="d6">Model Merging has gained prominence as a technique showcased on the Open LLM Leaderboard, demonstrating its effectiveness in creating competitive models.</data>
  <data key="d7">model performance, competitive ranking</data>
  <data key="d8">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Transfer Learning">
  <data key="d5">16.0</data>
  <data key="d6">Model Merging offers a different approach compared to Transfer Learning, focusing on combining multiple models rather than fine-tuning a single model.</data>
  <data key="d7">model techniques, machine learning</data>
  <data key="d8">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Machine Learning Community">
  <data key="d5">18.0</data>
  <data key="d6">The Machine Learning Community actively engages in developing and applying techniques like Model Merging to enhance model performance.</data>
  <data key="d7">collaboration, community engagement</data>
  <data key="d8">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Stable Diffusion">
  <data key="d5">20.0</data>
  <data key="d6">Stable Diffusion has inspired enthusiasts to create merged models by combining strengths from various fine-tuned versions.</data>
  <data key="d7">model merging, generative models</data>
  <data key="d8">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Task Arithmetic">
  <data key="d5">23.0</data>
  <data key="d6">Task Arithmetic is a specific method that enhances the process of merging models, particularly for language models.&lt;SEP&gt;Task Arithmetic is utilized in model merging to create new models through linear combinations of existing models.</data>
  <data key="d7">method, integration&lt;SEP&gt;model enhancement, merging techniques</data>
  <data key="d8">chunk-5fcb50e3e44399e7bec0bc28c47a74f7&lt;SEP&gt;chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Latent Diffusion Models">
  <data key="d5">8.0</data>
  <data key="d6">Latent Diffusion Models can benefit from model merging techniques to enhance their generative capabilities.</data>
  <data key="d7">generative models, enhancement</data>
  <data key="d8">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Parameter Interference">
  <data key="d5">9.0</data>
  <data key="d6">Model merging techniques must address parameter interference to ensure optimal performance of the merged model.</data>
  <data key="d7">performance issues, optimization</data>
  <data key="d8">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="TIES-Merging">
  <data key="d5">31.0</data>
  <data key="d6">Model Merging encompasses the TIES-Merging technique, which is a specific method for combining models to enhance performance.&lt;SEP&gt;TIES-Merging is a specific technique within the broader category of model merging that aims to improve merging performance.&lt;SEP&gt;TIES-Merging is another approach under the umbrella of model merging, addressing task conflicts during the process.</data>
  <data key="d7">methodology, performance enhancement&lt;SEP&gt;methodology, performance improvement&lt;SEP&gt;task conflict resolution, model merging</data>
  <data key="d8">chunk-c8ff785e8102dfde59dbc9309abb6b6a&lt;SEP&gt;chunk-67c2717229a96dfe2b74a6bceef933f8&lt;SEP&gt;chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="DARE">
  <data key="d5">8.0</data>
  <data key="d6">DARE is another method that enhances the process of model merging by addressing differences between models.</data>
  <data key="d7">technique, enhancement</data>
  <data key="d8">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="ComPEFT">
  <data key="d5">7.0</data>
  <data key="d6">ComPEFT focuses on the compression aspect of model merging, particularly regarding fine-tuned updates.</data>
  <data key="d7">compression, efficiency</data>
  <data key="d8">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Multimodal Model Development">
  <data key="d5">8.0</data>
  <data key="d6">Model merging is applied in multimodal model development to enhance the integration of diverse data types.</data>
  <data key="d7">data integration, application</data>
  <data key="d8">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Image Generation">
  <data key="d5">8.0</data>
  <data key="d6">Model merging techniques are increasingly used in image generation to create advanced models for producing new images.</data>
  <data key="d7">application, technology</data>
  <data key="d8">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="LLM Communities">
  <data key="d5">9.0</data>
  <data key="d6">LLM communities are actively engaged in the practice of model merging to create innovative language models.</data>
  <data key="d7">community engagement, innovation</data>
  <data key="d8">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Transformer Blocks">
  <data key="d5">9.0</data>
  <data key="d6">Model merging involves utilizing transformer blocks to create new architectures that leverage pre-trained capabilities for improved performance.</data>
  <data key="d7">model integration, performance enhancement</data>
  <data key="d8">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Frankenmerging">
  <data key="d5">8.0</data>
  <data key="d6">Frankenmerging is a specific technique used within the broader context of model merging, illustrating a method of combining models.</data>
  <data key="d7">merging techniques, model integration</data>
  <data key="d8">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Hugging Face Blog">
  <data key="d5">9.0</data>
  <data key="d6">The Hugging Face Blog features articles and discussions on model merging techniques, reflecting advancements in AI.</data>
  <data key="d7">technology discussion, community engagement</data>
  <data key="d8">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Mixup Model Merge (M3)">
  <data key="d5">9.0</data>
  <data key="d6">M3 is a specific approach to model merging that enhances the performance of merged models using randomized linear interpolation.</data>
  <data key="d7">methodology, model integration</data>
  <data key="d8">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="M&lt;sup&gt;3&lt;/sup&gt;">
  <data key="d5">27.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; is a method specifically designed to enhance the process of model merging by improving robustness.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; is a specific technique within the broader category of Model Merging that enhances model performance through a unique approach.</data>
  <data key="d7">model integration, robustness enhancement&lt;SEP&gt;model technique, performance enhancement</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Fisher Merging">
  <data key="d5">16.0</data>
  <data key="d6">Fisher Merging is a specific approach within the broader category of model merging that focuses on precise parameter integration.</data>
  <data key="d7">parameter integration, method classification</data>
  <data key="d8">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Wortsman et al.">
  <data key="d5">7.0</data>
  <data key="d6">Wortsman et al. have researched model merging techniques that relate to the concepts of M&lt;sup&gt;3&lt;/sup&gt;.</data>
  <data key="d7">research connection, model merging</data>
  <data key="d8">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Ilharco et al.">
  <data key="d5">7.0</data>
  <data key="d6">Ilharco et al. have explored various methods in model merging, contributing to the understanding of M&lt;sup&gt;3&lt;/sup&gt;.</data>
  <data key="d7">research influence, model integration</data>
  <data key="d8">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Lin et al.">
  <data key="d5">7.0</data>
  <data key="d6">Lin et al. have contributed to advancements in model merging techniques that inform the development of M&lt;sup&gt;3&lt;/sup&gt;.</data>
  <data key="d7">research contribution, model development</data>
  <data key="d8">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Mixup">
  <data key="d5">8.0</data>
  <data key="d6">Mixup is a technique that can be integrated into Model Merging to enhance the diversity of training samples.</data>
  <data key="d7">data augmentation, model improvement</data>
  <data key="d8">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Fine-Tuned Models">
  <data key="d5">8.0</data>
  <data key="d6">Model Merging often involves combining Fine-Tuned Models to create a more versatile model for various tasks.</data>
  <data key="d7">model integration, task versatility</data>
  <data key="d8">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Open LLM Leaderboard" target="mergekit">
  <data key="d5">7.0</data>
  <data key="d6">The Open LLM Leaderboard features top models that are frequently developed using techniques and tools provided by Mergekit.".</data>
  <data key="d7">model ranking, community development</data>
  <data key="d8">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Open LLM Leaderboard" target="Evolutionary Model Merging">
  <data key="d5">7.0</data>
  <data key="d6">The Open LLM Leaderboard serves as a reference point for evaluating models developed through evolutionary model merging.</data>
  <data key="d7">evaluation, performance metrics</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Open LLM Leaderboard" target="HuggingFace">
  <data key="d5">9.0</data>
  <data key="d6">HuggingFace hosts the Open LLM Leaderboard, which evaluates large language models and promotes advancements in the field.</data>
  <data key="d7">evaluation platform, AI research</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Open LLM Leaderboard" target="Hugging Face">
  <data key="d5">9.0</data>
  <data key="d6">Hugging Face hosts the Open LLM leaderboard, which ranks various large language models based on their performance.</data>
  <data key="d7">platform, model evaluation</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Open LLM Leaderboard" target="Super-Mario v2">
  <data key="d5">16.0</data>
  <data key="d6">Super-Mario v2's performance is evaluated and recognized on the Open LLM Leaderboard, highlighting its competitive edge.</data>
  <data key="d7">model ranking, performance evaluation</data>
  <data key="d8">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Evolutionary Algorithms" target="Foundation Model Development">
  <data key="d5">9.0</data>
  <data key="d6">Evolutionary Algorithms play a crucial role in Foundation Model Development, particularly in discovering effective model combinations for enhanced capabilities.</data>
  <data key="d7">optimization, model enhancement</data>
  <data key="d8">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Algorithms" target="Deep Learning">
  <data key="d5">9.0</data>
  <data key="d6">Evolutionary algorithms are applied within deep learning frameworks to optimize model architectures and enhance performance.</data>
  <data key="d7">optimization, deep learning</data>
  <data key="d8">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Machine Learning Community" target="Open Source Software Development">
  <data key="d5">14.0</data>
  <data key="d6">The principles of Open Source Software Development are reflected in the collaborative nature of the Machine Learning Community.</data>
  <data key="d7">collaborative principles, software development</data>
  <data key="d8">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Task Arithmetic" target="Mixup Model Merge">
  <data key="d5">8.0</data>
  <data key="d6">Mixup Model Merge builds upon Task Arithmetic by introducing randomness in the merging process.</data>
  <data key="d7">model merging methods, randomness</data>
  <data key="d8">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Task Arithmetic" target="Average Merging">
  <data key="d5">8.0</data>
  <data key="d6">Average Merging and Task Arithmetic are both processes that reformulate how language model parameters are combined for improved performance.</data>
  <data key="d7">parameter merging, model enhancement</data>
  <data key="d8">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Task Arithmetic" target="M&lt;sup&gt;3&lt;/sup&gt;">
  <data key="d5">26.0</data>
  <data key="d6">Combining Task Arithmetic with M&lt;sup&gt;3&lt;/sup&gt; leads to significant improvements in model performance on various datasets.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; shows improvements in Task Arithmetic when merging fine-tuned LLMs, indicating its effectiveness in this context."|</data>
  <data key="d7">method integration, performance boost&lt;SEP&gt;model merging, performance enhancement</data>
  <data key="d8">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Task Arithmetic" target="TIES-Merging">
  <data key="d5">7.0</data>
  <data key="d6">Task Arithmetic and TIES-Merging are both methods used for merging model parameters in machine learning.".</data>
  <data key="d7">model merging methods, parameter adjustment</data>
  <data key="d8">chunk-9fbd0c0622e4d5fb84be83781e1c4521</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Model Soup" target="Weighted Model Averaging">
  <data key="d5">9.0</data>
  <data key="d6">Model Soup is a practical application of Weighted Model Averaging to improve machine learning model performance.</data>
  <data key="d7">model techniques, performance improvement</data>
  <data key="d8">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="TIES-Merging" target="DARE">
  <data key="d5">46.0</data>
  <data key="d6">Both TIES-Merging and DARE are methods focused on improving the performance and effectiveness of model merging in machine learning.".&lt;SEP&gt;TIES-Merging and DARE are both methods employed for merging models to enhance their capabilities.&lt;SEP&gt;TIES-Merging enhances its functionality through the integration of DARE, allowing for more precise model merging techniques.&lt;SEP&gt;DARE complements TIES-Merging by reducing parameter interference during the merging of models, enhancing overall task performance.</data>
  <data key="d7">model enhancement, integration&lt;SEP&gt;model merging techniques, performance enhancement&lt;SEP&gt;model merging, optimization techniques&lt;SEP&gt;sparsification, model performance</data>
  <data key="d8">chunk-568ea9e09f85a7b95cfe61d48b0ba893&lt;SEP&gt;chunk-67c2717229a96dfe2b74a6bceef933f8&lt;SEP&gt;chunk-77432ea4d82a2648008d6d26c50ccdc9&lt;SEP&gt;chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</edge>
<edge source="TIES-Merging" target="Parameter Space (PS)">
  <data key="d5">8.0</data>
  <data key="d6">TIES-Merging operates within the parameter space to refine how model weights are integrated during merging processes.</data>
  <data key="d7">model optimization, parameter integration</data>
  <data key="d8">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="TIES-Merging" target="Data Flow Space (DFS)">
  <data key="d5">7.0</data>
  <data key="d6">Merging in the Data Flow Space (DFS) complements TIES-Merging by preserving original weights while optimizing inference paths.</data>
  <data key="d7">model merging, optimization</data>
  <data key="d8">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="TIES-Merging" target="Prateek Yadav">
  <data key="d5">9.0</data>
  <data key="d6">Prateek Yadav is an author of TIES-Merging, indicating his involvement in the research.</data>
  <data key="d7">authorship, research collaboration</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="TIES-Merging" target="Yadav et al.">
  <data key="d5">9.0</data>
  <data key="d6">Yadav et al. proposed TIES-Merging as a method to resolve task conflicts in model merging.</data>
  <data key="d7">methodology development, task resolution</data>
  <data key="d8">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="TIES-Merging" target="M&lt;sup&gt;3&lt;/sup&gt;">
  <data key="d5">27.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; is an approach that can be integrated into the TIES-Merging framework for improved model performance during merging.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; is utilized in TIES-Merging, leading to significant performance gains in LLMs."|</data>
  <data key="d7">model integration, performance enhancement&lt;SEP&gt;model merging, performance improvement</data>
  <data key="d8">chunk-568ea9e09f85a7b95cfe61d48b0ba893&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="TIES-Merging" target="WizardLM-13B">
  <data key="d5">8.0</data>
  <data key="d6">TIES-Merging was applied to WizardLM-13B to evaluate its impact on performance metrics during testing.</data>
  <data key="d7">performance enhancement, model improvement</data>
  <data key="d8">chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="TIES-Merging" target="Performance Metrics">
  <data key="d5">8.0</data>
  <data key="d6">TIES-Merging is assessed through various performance metrics to evaluate its effectiveness in enhancing language models.</data>
  <data key="d7">performance evaluation, model enhancement</data>
  <data key="d8">chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="TIES-Merging" target="Hyperparameter Settings">
  <data key="d5">8.0</data>
  <data key="d6">TIES-Merging involves specific hyperparameter settings that dictate how model outputs are merged in machine learning processes.</data>
  <data key="d7">model optimization, configuration</data>
  <data key="d8">chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="DARE" target="Mixup Model Merge (M3)">
  <data key="d5">8.0</data>
  <data key="d6">DARE can be combined with M3 to achieve superior results in model merging, enhancing overall performance.</data>
  <data key="d7">performance enhancement, technique combination</data>
  <data key="d8">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="DARE" target="M&lt;sup&gt;3&lt;/sup&gt;">
  <data key="d5">30.0</data>
  <data key="d6">DARE is used alongside M&lt;sup&gt;3&lt;/sup&gt; to improve the performance of model merging techniques.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; and DARE are combined to explore their effects on model merging performance, showing that they can complement each other.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; and DARE are combined to explore their effects on model merging performance.</data>
  <data key="d7">model enhancement, performance improvement&lt;SEP&gt;performance improvement, sparsification</data>
  <data key="d8">chunk-c8ff785e8102dfde59dbc9309abb6b6a&lt;SEP&gt;chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="DARE" target="delta parameters">
  <data key="d5">7.0</data>
  <data key="d6">Delta parameters are reduced in redundancy by DARE to enhance model performance during tasks.</data>
  <data key="d7">parameter reduction, model efficiency</data>
  <data key="d8">chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="DARE" target="model parameters">
  <data key="d5">8.0</data>
  <data key="d6">DARE targets model parameters to optimize their use in fine-tuned models, improving task performance.</data>
  <data key="d7">parameter optimization, task performance</data>
  <data key="d8">chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="DARE" target="Super-Mario v2">
  <data key="d5">18.0</data>
  <data key="d6">DARE was used to merge models, resulting in the creation of Super-Mario v2, which achieved top ranking on the Open LLM Leaderboard.</data>
  <data key="d7">model merging, performance enhancement</data>
  <data key="d8">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="DARE" target="AlpacaEval">
  <data key="d5">14.0</data>
  <data key="d6">DARE's effectiveness in model merging is demonstrated through evaluations on benchmarks like AlpacaEval.</data>
  <data key="d7">benchmark evaluation, model performance</data>
  <data key="d8">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="DARE" target="GSM8K">
  <data key="d5">14.0</data>
  <data key="d6">DARE contributes to improved task performance as shown in evaluations on GSM8K, a benchmark for language models.</data>
  <data key="d7">benchmark evaluation, model performance</data>
  <data key="d8">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="DARE" target="MBPP">
  <data key="d5">14.0</data>
  <data key="d6">The performance of models merged with DARE is assessed on MBPP, showcasing its capabilities in programming tasks.</data>
  <data key="d7">benchmark evaluation, model performance</data>
  <data key="d8">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="mergekit" target="Mistral">
  <data key="d5">9.0</data>
  <data key="d6">Mergekit provides tools and recipes for merging models, including those based on the Mistral architecture, facilitating the creation of new models.".</data>
  <data key="d7">toolkit, model architecture</data>
  <data key="d8">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="mergekit" target="Frankenmerging">
  <data key="d5">8.0</data>
  <data key="d6">Frankenmerging is one of the experimental methods supported by Mergekit, enabling users to create new model architectures.".</data>
  <data key="d7">experimental techniques, model creation</data>
  <data key="d8">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="mergekit" target="Charles O. Goddard">
  <data key="d5">9.0</data>
  <data key="d6">Charles O. Goddard is associated with the mergekit project, which focuses on AI model development and optimization.</data>
  <data key="d7">project involvement, AI development</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Frankenmerging" target="MGSM-JA">
  <data key="d5">9.0</data>
  <data key="d6">Frankenmerging resulted in a score of 0 in the MGSM-JA task, indicating poor model performance.</data>
  <data key="d7">model evaluation, performance</data>
  <data key="d8">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Frankenmerging" target="Abel 7B 002">
  <data key="d5">7.0</data>
  <data key="d6">Abel 7B 002 was utilized in the Frankenmerging method, contributing to the performance evaluations.</data>
  <data key="d7">model source, comparative analysis</data>
  <data key="d8">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Frankenmerging" target="TIES-Merge">
  <data key="d5">8.0</data>
  <data key="d6">Both TIES-Merge and Frankenmerging were compared in terms of performance in model merging experiments.</data>
  <data key="d7">model comparison, performance evaluation</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Neural Architecture Search" target="Neural Architecture Search (NAS)">
  <data key="d5">8.0</data>
  <data key="d6">Both evolutionary neural architecture search and NAS aim to discover optimal neural network architectures, with evolutionary methods providing a systematic approach.</data>
  <data key="d7">architecture discovery, optimization</data>
  <data key="d8">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Neural Architecture Search (NAS)" target="Weight Agnostic Neural Networks">
  <data key="d5">7.0</data>
  <data key="d6">NAS methods often focus on discovering architectures that can be optimized without reliance on weight training, aligning with the principles of weight agnostic networks.</data>
  <data key="d7">architecture optimization, training efficiency</data>
  <data key="d8">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Neural Architecture Search (NAS)" target="SMASH">
  <data key="d5">9.0</data>
  <data key="d6">SMASH is a specific NAS method that enhances the efficiency of architecture search by avoiding extensive training, showcasing a relationship with broader NAS techniques.</data>
  <data key="d7">efficiency, architecture search</data>
  <data key="d8">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Neural Architecture Search (NAS)" target="Hypernetwork">
  <data key="d5">9.0</data>
  <data key="d6">Hypernetworks are utilized in NAS methods to estimate weights for candidate architectures, improving the efficiency of architecture search.</data>
  <data key="d7">architecture search, weight estimation</data>
  <data key="d8">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="NEAT" target="Weight Agnostic Neural Networks">
  <data key="d5">8.0</data>
  <data key="d6">NEAT and weight agnostic neural networks both focus on evolving neural network structures without conventional training methods, highlighting innovative approaches to neural network design.</data>
  <data key="d7">evolutionary algorithms, neural design</data>
  <data key="d8">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Data Flow Space (DFS)" target="Merging Configuration Parameters">
  <data key="d5">7.0</data>
  <data key="d6">Data flow space interacts with merging configuration parameters to determine how data is processed in merged models.</data>
  <data key="d7">data processing, model configuration</data>
  <data key="d8">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Data Flow Space (DFS)" target="Inference Path">
  <data key="d5">8.0</data>
  <data key="d6">The Inference Path is optimized within the Data Flow Space (DFS) during model merging to improve processing efficiency.</data>
  <data key="d7">processing optimization, model efficiency</data>
  <data key="d8">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Merging Configuration Parameters" target="CMA-ES">
  <data key="d5">9.0</data>
  <data key="d6">CMA-ES optimizes the merging configuration parameters to improve model performance based on task-specific metrics.</data>
  <data key="d7">optimization, performance improvement</data>
  <data key="d8">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Merging Configuration Parameters" target="Evolutionary Search">
  <data key="d5">8.0</data>
  <data key="d6">Evolutionary Search is employed to navigate the search space of merging configuration parameters, enhancing the merging process.</data>
  <data key="d7">search optimization, parameter tuning</data>
  <data key="d8">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="CMA-ES" target="EvoJAX">
  <data key="d5">18.0</data>
  <data key="d6">EvoJAX employs the CMA-ES algorithm for optimizing model parameters during the merging process, enhancing model performance.&lt;SEP&gt;EvoJAX utilizes the CMA-ES algorithm for optimizing model parameters in the merging process.</data>
  <data key="d7">optimization, algorithm utilization</data>
  <data key="d8">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="CMA-ES" target="Model #5">
  <data key="d5">9.0</data>
  <data key="d6">Model #5 employs CMA-ES as part of its optimization strategy to enhance search efficiency and flexibility in layer management.</data>
  <data key="d7">optimization strategy, model performance</data>
  <data key="d8">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Layer-wise Merging" target="Granular Merging">
  <data key="d5">8.0</data>
  <data key="d6">Layer-wise Merging is a type of Granular Merging that focuses on optimizing individual layers for better performance.</data>
  <data key="d7">optimization, model refinement</data>
  <data key="d8">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Input/Output Embedding Layers" target="Transformer Block">
  <data key="d5">9.0</data>
  <data key="d6">Input/Output Embedding Layers are integral parts of the Transformer Block, facilitating data transformation in neural networks.</data>
  <data key="d7">model architecture, data processing</data>
  <data key="d8">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Sparsification" target="Weight Mixing">
  <data key="d5">7.0</data>
  <data key="d6">Sparsification can be used in conjunction with Weight Mixing to enhance the efficiency of model merging.</data>
  <data key="d7">efficiency, model optimization</data>
  <data key="d8">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Search Space" target="Layer Indices">
  <data key="d5">8.0</data>
  <data key="d6">The Search Space is defined by the Layer Indices, which represent the possible configurations to explore during model optimization.</data>
  <data key="d7">model exploration, optimization</data>
  <data key="d8">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Layer Indices" target="Distribution Shift">
  <data key="d5">6.0</data>
  <data key="d6">Distribution Shift may occur when inputs are processed through different Layer Indices, affecting model performance.</data>
  <data key="d7">performance impact, model dynamics</data>
  <data key="d8">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Sequential Order" target="Indicator Array">
  <data key="d5">7.0</data>
  <data key="d6">The Sequential Order of layers is managed by the Indicator Array during the merging process to determine layer inclusion.</data>
  <data key="d7">layer management, merging process</data>
  <data key="d8">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Indicator Array" target="Layer Stacking">
  <data key="d5">7.0</data>
  <data key="d6">The Indicator Array is used to manage Layer Stacking in the models, helping to track layer configurations during processing.</data>
  <data key="d7">data structure, model management</data>
  <data key="d8">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="PS Merging" target="DFS Merging">
  <data key="d5">18.0</data>
  <data key="d6">PS Merging is applied first to optimize models before DFS Merging is used to refine them, indicating a sequential relationship in the merging process.</data>
  <data key="d7">model optimization, sequential methods</data>
  <data key="d8">chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="PS Merging" target="Hybrid Model">
  <data key="d5">8.0</data>
  <data key="d6">The Hybrid Model incorporates PS Merging as one of its strategies to enhance performance on tasks.</data>
  <data key="d7">model strategy, performance enhancement</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="DFS Merging" target="Hybrid Model">
  <data key="d5">8.0</data>
  <data key="d6">DFS Merging is a critical component of the Hybrid Model that contributes to its overall performance improvements.</data>
  <data key="d7">model strategy, performance enhancement</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="DFS Merging" target="PS+DFS">
  <data key="d5">16.0</data>
  <data key="d6">DFS Merging is a method that is part of the PS+DFS approach, combining models for enhanced performance.</data>
  <data key="d7">methodology, model integration</data>
  <data key="d8">chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="DFS Merging" target="Ablation Studies">
  <data key="d5">7.0</data>
  <data key="d6">Ablation Studies help understand the effectiveness of DFS Merging by analyzing different configurations and their impact on performance.</data>
  <data key="d7">experimental analysis, model evaluation</data>
  <data key="d8">chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MGSM Dataset" target="GSM8k Dataset">
  <data key="d5">16.0</data>
  <data key="d6">The MGSM Dataset is derived from the GSM8k Dataset, indicating a direct relationship in their usage for evaluating language models on math problems.</data>
  <data key="d7">dataset relationship, evaluation</data>
  <data key="d8">chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="CMA-ES Algorithm" target="Optuna">
  <data key="d5">14.0</data>
  <data key="d6">The CMA-ES Algorithm is implemented within Optuna for hyperparameter optimization, showing how the two are interconnected in the model optimization process.</data>
  <data key="d7">optimization techniques, software integration</data>
  <data key="d8">chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Optuna" target="Stability AI">
  <data key="d5">17.0</data>
  <data key="d6">Stability AI is associated with the Optuna framework, which is utilized for hyperparameter optimization in AI models.&lt;SEP&gt;Stability AI utilizes the Optuna framework for optimizing its AI models, enhancing their performance and efficiency.</data>
  <data key="d7">AI development, optimization&lt;SEP&gt;AI optimization, framework usage</data>
  <data key="d8">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MGSM" target="Model 1">
  <data key="d5">9.0</data>
  <data key="d6">MGSM provides the test set that evaluates Model 1's performance in Japanese Math tasks.</data>
  <data key="d7">evaluation, performance testing</data>
  <data key="d8">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MGSM" target="Model 2">
  <data key="d5">9.0</data>
  <data key="d6">MGSM is used to assess Model 2's mathematical proficiency in the Japanese context.</data>
  <data key="d7">evaluation, performance testing</data>
  <data key="d8">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MGSM" target="Model 3">
  <data key="d5">9.0</data>
  <data key="d6">MGSM benchmarks Model 3's capabilities, revealing its low scores in Japanese language proficiency.</data>
  <data key="d7">evaluation, performance testing</data>
  <data key="d8">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model 1" target="Model 4">
  <data key="d5">16.0</data>
  <data key="d6">Model 4, as an optimized merged model, is compared against Model 1 to demonstrate performance improvements in the MGSM-JA benchmark.&lt;SEP&gt;Model 4, being an optimized merged model, is compared against Model 1 to demonstrate performance improvements.</data>
  <data key="d7">model comparison, performance enhancement</data>
  <data key="d8">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model 1" target="MGSM's Test Set">
  <data key="d5">9.0</data>
  <data key="d6">MGSM's Test Set is used to evaluate Model 1's performance in Japanese Math tasks, providing a benchmark for its capabilities.</data>
  <data key="d7">evaluation, benchmark</data>
  <data key="d8">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model 2" target="Model 4">
  <data key="d5">16.0</data>
  <data key="d6">Model 4's performance is evaluated against Model 2, showcasing advancements in accuracy and proficiency in Japanese Math tasks.&lt;SEP&gt;Model 4's performance is evaluated in relation to Model 2, showcasing advancements in accuracy.</data>
  <data key="d7">model comparison, performance enhancement</data>
  <data key="d8">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model 2" target="MGSM's Test Set">
  <data key="d5">9.0</data>
  <data key="d6">MGSM's Test Set is utilized to assess Model 2's mathematical abilities in the context of Japanese language tasks.</data>
  <data key="d7">evaluation, benchmark</data>
  <data key="d8">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model 3" target="Model 5">
  <data key="d5">16.0</data>
  <data key="d6">Model 5 shows enhanced performance compared to Model 3, indicating successful merging strategies and improved mathematical capabilities.&lt;SEP&gt;Model 5 shows enhanced performance compared to Model 3, indicating successful merging strategies.</data>
  <data key="d7">model comparison, performance enhancement</data>
  <data key="d8">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model 3" target="MGSM's Test Set">
  <data key="d5">9.0</data>
  <data key="d6">MGSM's Test Set benchmarks Model 3's performance, revealing its limitations in Japanese language proficiency.</data>
  <data key="d7">evaluation, benchmark</data>
  <data key="d8">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model 4" target="Model 6">
  <data key="d5">16.0</data>
  <data key="d6">Model 6 integrates both merging strategies and is compared to Model 4 for further performance improvements on the benchmark tasks.&lt;SEP&gt;Model 6 integrates both merging strategies and is compared to Model 4 for performance improvements.</data>
  <data key="d7">model comparison, performance enhancement</data>
  <data key="d8">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model 6" target="MGSM-JA task">
  <data key="d5">8.0</data>
  <data key="d6">Model 6 shows enhancements in performance when applied to the MGSM-JA task, indicating its effectiveness in mathematical problem-solving.</data>
  <data key="d7">performance enhancement, model evaluation</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MGSM-JA task" target="Ours (DFS)">
  <data key="d5">7.0</data>
  <data key="d6">Ours (DFS) is tested on the MGSM-JA task, demonstrating its capabilities in mathematical problem-solving.</data>
  <data key="d7">task application, model evaluation</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Shisa Gamma 7b v1" target="JP-LMEH">
  <data key="d5">8.0</data>
  <data key="d6">Shisa Gamma 7b v1 serves as a baseline in the JP-LMEH benchmark, highlighting its role in evaluating language models.</data>
  <data key="d7">benchmark comparison, evaluation standard</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="WizardMath 7B V1.1" target="JP-LMEH">
  <data key="d5">7.0</data>
  <data key="d6">WizardMath 7B V1.1 is evaluated in the JP-LMEH tasks, providing insights into its language capabilities.</data>
  <data key="d7">benchmark evaluation, language model</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Abel 7B 002" target="JP-LMEH">
  <data key="d5">6.0</data>
  <data key="d6">Abel 7B 002 is also evaluated within the JP-LMEH framework, contributing to the understanding of its performance.</data>
  <data key="d7">model assessment, language proficiency</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Ours (PS)" target="JP-LMEH">
  <data key="d5">9.0</data>
  <data key="d6">Ours (PS) achieved high scores in the JP-LMEH benchmark, showcasing its effectiveness in Japanese language tasks.</data>
  <data key="d7">evaluation success, model performance</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Ours (PS)" target="Ours (DFS)">
  <data key="d5">18.0</data>
  <data key="d6">Ours (PS) and Ours (DFS) are merged models that compile performance metrics from different sources.".</data>
  <data key="d7">model integration, performance metrics</data>
  <data key="d8">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Ours (PS)" target="Ours (PS+DFS)">
  <data key="d5">18.0</data>
  <data key="d6">Ours (PS) contributes to the data used in Ours (PS+DFS), indicating a relationship in model performance.".</data>
  <data key="d7">model integration, performance metrics</data>
  <data key="d8">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Ours (PS)" target="7B merged models">
  <data key="d5">10.0</data>
  <data key="d6">Ours (PS) is a part of the 7B merged models category, indicating its integration of various 7B models.".</data>
  <data key="d7">category classification, model type</data>
  <data key="d8">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Ours (DFS)" target="CMA-ES Optimization Results">
  <data key="d5">16.0</data>
  <data key="d6">The CMA-ES optimization results are derived from the merging efforts of Ours (DFS), showcasing the effectiveness of the model.")</data>
  <data key="d7">model optimization, performance evaluation</data>
  <data key="d8">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Hybrid Model" target="MGSM Scores">
  <data key="d5">9.0</data>
  <data key="d6">The effectiveness of the Hybrid Model is evaluated using MGSM Scores, which reflect its performance on mathematical tasks.</data>
  <data key="d7">performance evaluation, task assessment</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Hybrid Model" target="Appendix A">
  <data key="d5">7.0</data>
  <data key="d6">Appendix A provides additional details and comparisons relevant to the Hybrid Model's performance and evaluations.</data>
  <data key="d7">additional information, model assessment</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Hybrid Model" target="Appendix C">
  <data key="d5">8.0</data>
  <data key="d6">Appendix C showcases examples that illustrate the capabilities of the Hybrid Model in specific tasks.</data>
  <data key="d7">demonstration, model utility</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese StableLM 70B" target="MGSM Scores">
  <data key="d5">9.0</data>
  <data key="d6">Japanese StableLM 70B serves as a benchmark for comparing the MGSM Scores of other models, indicating its significance in evaluations.</data>
  <data key="d7">benchmark comparison, evaluation standard</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="JSQuAD" target="JP-LMEH">
  <data key="d5">8.0</data>
  <data key="d6">JSQuAD is one of the tasks evaluated within the JP-LMEH benchmark, measuring model performance in question answering.</data>
  <data key="d7">benchmark task, evaluation</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="JNLI" target="JP-LMEH">
  <data key="d5">8.0</data>
  <data key="d6">JNLI is included in the JP-LMEH framework, assessing models' capabilities in natural language inference in Japanese.</data>
  <data key="d7">benchmark task, evaluation</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MARC" target="JP-LMEH">
  <data key="d5">7.0</data>
  <data key="d6">MARC is another task within the JP-LMEH benchmark that evaluates models' reading comprehension abilities.</data>
  <data key="d7">benchmark task, evaluation</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Ours (PS+DFS)" target="CMA-ES Optimization Results">
  <data key="d5">16.0</data>
  <data key="d6">The results of CMA-ES optimization are influenced by the configurations used in Ours (PS+DFS), indicating a connection between the two.")</data>
  <data key="d7">model optimization, performance relationship</data>
  <data key="d8">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Ours (PS+DFS)" target="DARE-TIES">
  <data key="d5">7.0</data>
  <data key="d6">Ours (PS+DFS) utilizes DARE-TIES parameters for evolutionary merging, indicating a methodological relationship.")</data>
  <data key="d7">methodology, parameter utilization</data>
  <data key="d8">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Llama 2 70B" target="Parameter Configuration Post PS Merging">
  <data key="d5">14.0</data>
  <data key="d6">The performance metrics of Llama 2 70B are part of the parameter configuration derived from PS merging efforts.")</data>
  <data key="d7">model performance, parameter analysis</data>
  <data key="d8">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese Stable LM 70B" target="CMA-ES Optimization Results">
  <data key="d5">18.0</data>
  <data key="d6">The Japanese Stable LM 70B is noted for its significant contribution to the CMA-ES optimization results, indicating its importance in the analysis.")</data>
  <data key="d7">model contribution, optimization results</data>
  <data key="d8">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese Stable LM 70B" target="Mistral Base Model">
  <data key="d5">9.0</data>
  <data key="d6">The Japanese Stable LM 70B is built on the Mistral Base Model, indicating a direct lineage in model development.")</data>
  <data key="d7">model lineage, development</data>
  <data key="d8">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Swallow 70B" target="Parameter Configuration Post PS Merging">
  <data key="d5">14.0</data>
  <data key="d6">Swallow 70B's performance metrics are included in the parameter configuration analysis post PS merging.")</data>
  <data key="d7">model performance, parameter analysis</data>
  <data key="d8">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Mistral Base Model" target="Shisa-Gamma-7B-v1">
  <data key="d5">8.0</data>
  <data key="d6">Shisa-Gamma-7B-v1 is based on the Mistral Base Model and benefits from its extensive pretraining.")</data>
  <data key="d7">model foundation, fine-tuning</data>
  <data key="d8">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="DARE-TIES" target="JP-LMEH">
  <data key="d5">6.0</data>
  <data key="d6">DARE-TIES was compared in terms of average performance metrics in the JP-LMEH benchmark.</data>
  <data key="d7">model evaluation, performance</data>
  <data key="d8">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="DARE-TIES" target="WizardMath 7B v1.1">
  <data key="d5">8.0</data>
  <data key="d6">WizardMath 7B v1.1 served as a source model for DARE-TIES, facilitating direct performance comparisons.</data>
  <data key="d7">model source, comparative analysis</data>
  <data key="d8">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="DARE-TIES" target="PS model">
  <data key="d5">7.0</data>
  <data key="d6">DARE-TIES performed better than other baseline models, including the PS model, in specific comparisons.</data>
  <data key="d7">model performance, comparative analysis</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="DARE-TIES" target="Evolutionary Model Merging">
  <data key="d5">8.0</data>
  <data key="d6">DARE-TIES is a method applied in the context of evolutionary model merging to enhance model performance.</data>
  <data key="d7">technique, model integration</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MGSM-JA" target="TIES-Merge">
  <data key="d5">5.0</data>
  <data key="d6">TIES-Merge was evaluated based on its performance in the MGSM-JA task, showing low effectiveness.</data>
  <data key="d7">model evaluation, performance</data>
  <data key="d8">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MGSM-JA" target="PS model">
  <data key="d5">9.0</data>
  <data key="d6">The PS model achieved a higher MGSM-JA score compared to other models, indicating better performance.</data>
  <data key="d7">performance metrics, model evaluation</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MGSM-JA" target="Llama-2-13b">
  <data key="d5">16.0</data>
  <data key="d6">Llama-2-13b's performance is evaluated using the MGSM-JA scoring metric, reflecting its effectiveness in English tasks.</data>
  <data key="d7">performance evaluation, model scoring</data>
  <data key="d8">chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MGSM-JA" target="MetaMath-13B-V1.0">
  <data key="d5">14.0</data>
  <data key="d6">MetaMath-13B-V1.0's performance is also evaluated using the MGSM-JA metric, focusing on its capabilities in math tasks.</data>
  <data key="d7">performance evaluation, model scoring</data>
  <data key="d8">chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MGSM-JA" target="WizardMath-7B-V1.1">
  <data key="d5">20.0</data>
  <data key="d6">WizardMath-7B-V1.1 achieves a high score on the GSM8k benchmark, indicating its strong performance in math-related tasks.</data>
  <data key="d7">performance evaluation, model scoring</data>
  <data key="d8">chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MGSM-JA" target="Performance Comparison">
  <data key="d5">9.0</data>
  <data key="d6">Performance Comparison involves using the MGSM-JA metric to evaluate and compare the effectiveness of various models.</data>
  <data key="d7">evaluation methodology, model assessment</data>
  <data key="d8">chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="JP-LMEH" target="fine-tuning">
  <data key="d5">6.0</data>
  <data key="d6">Fine-tuning often led to significant decreases in JP-LMEH scores, indicating its impact on model performance.</data>
  <data key="d7">fine-tuning effects, performance issues</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="JP-LMEH" target="ELYZA-japanese-Llama-2-13b-instruct">
  <data key="d5">16.0</data>
  <data key="d6">The performance of ELYZA-japanese-Llama-2-13b-instruct is assessed using the JP-LMEH metric, indicating its effectiveness in Japanese tasks.</data>
  <data key="d7">performance evaluation, model scoring</data>
  <data key="d8">chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="TIES-Merge" target="Shisa Gamma 7B v1">
  <data key="d5">8.0</data>
  <data key="d6">Shisa Gamma 7B v1 was used as a source model for the TIES-Merge method, allowing for performance comparison.</data>
  <data key="d7">model source, comparative analysis</data>
  <data key="d8">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="W_{ij}" target="10B model">
  <data key="d5">9.0</data>
  <data key="d6">The scaling parameters W_{ij} are crucial for the performance of the 10B model, as indicated by the study's findings.</data>
  <data key="d7">model performance, parameter significance</data>
  <data key="d8">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="W_{ij}" target="Ablation Studies">
  <data key="d5">8.0</data>
  <data key="d6">Ablation Studies were conducted to assess the impact of the scaling parameters W_{ij} on model performance.</data>
  <data key="d7">parameter analysis, model evaluation</data>
  <data key="d8">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="A and B" target="10B model">
  <data key="d5">8.0</data>
  <data key="d6">Models A and B were merged to create the 10B model, which was evaluated for performance.</data>
  <data key="d7">model merging, evaluation</data>
  <data key="d8">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="PS" target="10B model">
  <data key="d5">7.0</data>
  <data key="d6">The PS method was used in the creation of the 10B model, influencing its performance outcomes.</data>
  <data key="d7">method application, model performance</data>
  <data key="d8">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="DFS" target="10B model">
  <data key="d5">7.0</data>
  <data key="d6">The DFS method also contributed to the development of the 10B model, impacting its evaluation results.</data>
  <data key="d7">method application, model performance</data>
  <data key="d8">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Unoptimized Model Merging" target="Table 3">
  <data key="d5">6.0</data>
  <data key="d6">Unoptimized Model Merging methods are summarized in Table 3, showcasing their performance metrics.</data>
  <data key="d7">performance summary, model comparison</data>
  <data key="d8">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Ablation Studies" target="GPU Memory Requirement">
  <data key="d5">6.0</data>
  <data key="d6">GPU Memory Requirement impacts the feasibility of conducting Ablation Studies, as certain configurations may require more resources.</data>
  <data key="d7">resource constraints, experimental limitations</data>
  <data key="d8">chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="PS model" target="PS+DFS">
  <data key="d5">9.0</data>
  <data key="d6">The PS model is part of the hybrid merging method PS+DFS, which aims to achieve superior performance on target tasks.</data>
  <data key="d7">model enhancement, performance improvement</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="PS model" target="MGSM-JA Score">
  <data key="d5">9.0</data>
  <data key="d6">The PS model achieved a higher MGSM-JA score, indicating its effectiveness compared to other models.</data>
  <data key="d7">performance evaluation, model comparison</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="shisa-gamma-7b-v1" target="HuggingFace">
  <data key="d5">8.0</data>
  <data key="d6">Shisa-gamma-7b-v1 is available on HuggingFace, which hosts various models for public use.</data>
  <data key="d7">model hosting, platform availability</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="shisa-gamma-7b-v1" target="Mistral-7B-v0.1">
  <data key="d5">16.0</data>
  <data key="d6">Both Mistral-7B-v0.1 and shisa-gamma-7b-v1 are categorized as 7B source models, showcasing performance metrics.".</data>
  <data key="d7">model comparison, performance metrics</data>
  <data key="d8">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="shisa-gamma-7b-v1" target="7B source models">
  <data key="d5">10.0</data>
  <data key="d6">shisa-gamma-7b-v1 belongs to the 7B source models category, showcasing similar performance metrics.".</data>
  <data key="d7">category classification, model type</data>
  <data key="d8">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="WizardMath-7B-V1.1" target="EvoLLM-JP">
  <data key="d5">9.0</data>
  <data key="d6">EvoLLM-JP is designed to outperform WizardMath-7B-V1.1 in mathematical reasoning, especially in a Japanese context.</data>
  <data key="d7">model comparison, performance</data>
  <data key="d8">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="WizardMath-7B-V1.1" target="Mistral-7B-v0.1">
  <data key="d5">16.0</data>
  <data key="d6">Mistral-7B-v0.1 and WizardMath-7B-V1.1 are both 7B source models that report performance metrics.".</data>
  <data key="d7">model comparison, performance metrics</data>
  <data key="d8">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Abel-7B-002" target="Mistral-7B-v0.1">
  <data key="d5">16.0</data>
  <data key="d6">Mistral-7B-v0.1 and Abel-7B-002 are part of the 7B source models, indicating similar performance measurement.".</data>
  <data key="d7">model comparison, performance metrics</data>
  <data key="d8">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="HuggingFace" target="EvoLLM-JP">
  <data key="d5">7.0</data>
  <data key="d6">Models used in EvoLLM-JP were found on HuggingFace, indicating collaboration and resource sharing.</data>
  <data key="d7">model sourcing, collaboration</data>
  <data key="d8">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Epochs" target="Learning Rates">
  <data key="d5">8.0</data>
  <data key="d6">Learning rates were tested over a set number of epochs to determine optimal performance during fine-tuning.</data>
  <data key="d7">training parameters, optimization</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Distraction Effect" target="Irrelevant Models">
  <data key="d5">7.0</data>
  <data key="d6">The distraction effect was observed when irrelevant models were included in the selection process, impacting performance.</data>
  <data key="d7">model selection, performance impact</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Distraction Effect" target="German Model">
  <data key="d5">6.0</data>
  <data key="d6">The German model is an example of an irrelevant model that contributed to the distraction effect in the experiments.</data>
  <data key="d7">irrelevant model, experimental impact</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Distraction Effect" target="Spanish Model">
  <data key="d5">6.0</data>
  <data key="d6">The Spanish model is included as an irrelevant model affecting the distraction effect in the experiments.</data>
  <data key="d7">irrelevant model, experimental impact</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Distraction Effect" target="Chinese Model">
  <data key="d5">6.0</data>
  <data key="d6">The Chinese model is classified as an irrelevant model contributing to the distraction effect in the experiments.</data>
  <data key="d7">irrelevant model, experimental impact</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Distraction Effect" target="Biomedical Model">
  <data key="d5">6.0</data>
  <data key="d6">The biomedical model is an irrelevant model included in the distraction experiments, influencing performance outcomes.</data>
  <data key="d7">irrelevant model, experimental impact</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Distraction Effect" target="French Model">
  <data key="d5">6.0</data>
  <data key="d6">The French model is included as an irrelevant model affecting the distraction effect observed in the experiments.</data>
  <data key="d7">irrelevant model, experimental impact</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Distraction Effect" target="Korean Model">
  <data key="d5">6.0</data>
  <data key="d6">The Korean model is an irrelevant model that contributed to the distraction effect in the experiments.</data>
  <data key="d7">irrelevant model, experimental impact</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Distraction Effect" target="Arabic Model">
  <data key="d5">6.0</data>
  <data key="d6">The Arabic model is classified as an irrelevant model impacting the distraction effect in the experiments.</data>
  <data key="d7">irrelevant model, experimental impact</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Distraction Effect" target="Italian Model">
  <data key="d5">6.0</data>
  <data key="d6">The Italian model is included as an irrelevant model affecting the distraction effect observed in the experiments.</data>
  <data key="d7">irrelevant model, experimental impact</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Llama-2-13b" target="Mistral-7B-v0.1">
  <data key="d5">18.0</data>
  <data key="d6">Mistral-7B-v0.1 outperforms Llama-2-13b in basic mathematical abilities, showcasing a comparative analysis between the two models.</data>
  <data key="d7">model comparison, performance analysis</data>
  <data key="d8">chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Llama-2-13b" target="LiveBench">
  <data key="d5">8.0</data>
  <data key="d6">LiveBench evaluates the OOD robustness of the Llama-2-13b model through various tasks and benchmarks.</data>
  <data key="d7">evaluation, model testing</data>
  <data key="d8">chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="ELYZA-japanese-Llama-2-13b-instruct" target="Llama-2-13b-hf">
  <data key="d5">16.0</data>
  <data key="d6">Both Llama-2-13b-hf and ELYZA-japanese-Llama-2-13b-instruct are 13B models with reported performance metrics.".</data>
  <data key="d7">model comparison, performance metrics</data>
  <data key="d8">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MetaMath-13B-V1.0" target="DFS-Merged Model">
  <data key="d5">9.0</data>
  <data key="d6">The DFS-Merged Model utilizes configurations from MetaMath-13B-V1.0, particularly analyzing the effects of skipping certain layers for performance improvement.</data>
  <data key="d7">model analysis, performance improvement</data>
  <data key="d8">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MetaMath-13B-V1.0" target="Layer #30">
  <data key="d5">10.0</data>
  <data key="d6">Layer #30 from MetaMath-13B-V1.0 was identified as redundant, leading to its removal in the DFS-Merged Model for better outcomes.</data>
  <data key="d7">layer optimization, model efficiency</data>
  <data key="d8">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MetaMath-13B-V1.0" target="Llama-2-13b-hf">
  <data key="d5">16.0</data>
  <data key="d6">Llama-2-13b-hf and MetaMath-13B-V1.0 are both categorized as 13B source models, showcasing their performance metrics.".</data>
  <data key="d7">model comparison, performance metrics</data>
  <data key="d8">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Mistral-7B-v0.1" target="7B source models">
  <data key="d5">10.0</data>
  <data key="d6">Mistral-7B-v0.1 is classified under the 7B source models category, indicating its performance metrics.".</data>
  <data key="d7">category classification, model type</data>
  <data key="d8">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="DFS-Merged Model" target="Model A">
  <data key="d5">8.0</data>
  <data key="d6">The DFS-Merged Model incorporates layers from Model A, leveraging its initial performance characteristics during the DFS process.</data>
  <data key="d7">model integration, performance enhancement</data>
  <data key="d8">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="DFS-Merged Model" target="Model B">
  <data key="d5">7.0</data>
  <data key="d6">The performance of the DFS-Merged Model is influenced by layers from Model B, which may have undergone extensive fine-tuning.</data>
  <data key="d7">model integration, fine-tuning impact</data>
  <data key="d8">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="DFS-Merged Model" target="Japanese General Model">
  <data key="d5">9.0</data>
  <data key="d6">The DFS-Merged Model incorporates layers from the Japanese General Model to enhance its capabilities in processing Japanese language tasks.</data>
  <data key="d7">language processing, model enhancement</data>
  <data key="d8">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="DFS-Merged Model" target="Task Scenarios">
  <data key="d5">8.0</data>
  <data key="d6">The Task Scenarios illustrate specific instances where the DFS-Merged Model outperformed its source models in reasoning tasks.</data>
  <data key="d7">performance comparison, model evaluation</data>
  <data key="d8">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model A" target="Model #5">
  <data key="d5">8.0</data>
  <data key="d6">Model #5 incorporates layers from Model A, allowing it to utilize the strengths of this source model in its performance evaluations.</data>
  <data key="d7">model integration, performance enhancement</data>
  <data key="d8">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model B" target="Model #5">
  <data key="d5">7.0</data>
  <data key="d6">The performance of Model #5 is influenced by the layers from Model B, which may have undergone significant changes affecting output quality.</data>
  <data key="d7">model integration, performance evaluation</data>
  <data key="d8">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model #5" target="Scaling Matrix W">
  <data key="d5">8.0</data>
  <data key="d6">The Scaling Matrix W is utilized in Model #5 to control the outputs of layers, impacting the overall performance of the model.</data>
  <data key="d7">layer management, performance optimization</data>
  <data key="d8">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model #5" target="Performance Analysis">
  <data key="d5">9.0</data>
  <data key="d6">Performance Analysis is conducted on Model #5 to evaluate its effectiveness in various tasks and scenarios.</data>
  <data key="d7">model evaluation, performance assessment</data>
  <data key="d8">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model #5" target="Layer Removal">
  <data key="d5">10.0</data>
  <data key="d6">The process of Layer Removal is applied to Model #5 to improve its performance based on findings from performance analysis.</data>
  <data key="d7">model optimization, performance enhancement</data>
  <data key="d8">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model #6" target="Model #7">
  <data key="d5">8.0</data>
  <data key="d6">Model #6 and Model #7 are compared in performance analysis to understand the impact of different configurations on outcomes.</data>
  <data key="d7">model comparison, performance evaluation</data>
  <data key="d8">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="ビリー" target="お客様">
  <data key="d5">16.0</data>
  <data key="d6">ビリーは火曜日に8人のお客様にDVDを販売している。</data>
  <data key="d7">販売活動、顧客</data>
  <data key="d8">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="ビリー" target="DVD">
  <data key="d5">18.0</data>
  <data key="d6">ビリーはDVDを販売することに特化した活動を行っている。</data>
  <data key="d7">販売、商品</data>
  <data key="d8">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="火曜日" target="お客様">
  <data key="d5">14.0</data>
  <data key="d6">お客様は火曜日にビリーからDVDを購入する。</data>
  <data key="d7">顧客、販売日</data>
  <data key="d8">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="アップルパイ" target="ジョーンズおばあちゃん">
  <data key="d5">18.0</data>
  <data key="d6">ジョーンズおばあちゃんは消防士のためにアップルパイを焼いている。</data>
  <data key="d7">料理、イベント</data>
  <data key="d8">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="アップルパイ" target="パイ">
  <data key="d5">7.0</data>
  <data key="d6">アップルパイはジョーンズおばあちゃんが焼いたパイの一種である。</data>
  <data key="d7">料理、種類</data>
  <data key="d8">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="消防士の昼食会" target="ジョーンズおばあちゃん">
  <data key="d5">16.0</data>
  <data key="d6">ジョーンズおばあちゃんは消防士の昼食会のためにアップルパイを用意している。</data>
  <data key="d7">イベント、料理</data>
  <data key="d8">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="消防士の昼食会" target="ゲスト">
  <data key="d5">9.0</data>
  <data key="d6">消防士の昼食会にはゲストが参加し、アップルパイを食べる。</data>
  <data key="d7">イベント、参加者</data>
  <data key="d8">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Grandma Jones" target="Pies">
  <data key="d5">16.0</data>
  <data key="d6">Grandma Jones cut the pies into pieces for the guests to enjoy.</data>
  <data key="d7">food preparation, event hosting</data>
  <data key="d8">chunk-3ca398087b4d5dd48a0c1fe169e9b7fd</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Guests" target="Pieces of Pie">
  <data key="d5">18.0</data>
  <data key="d6">Guests consumed the pieces of pie that were prepared by Grandma Jones.</data>
  <data key="d7">food consumption, event participation</data>
  <data key="d8">chunk-3ca398087b4d5dd48a0c1fe169e9b7fd</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Guests" target="Event of Pie Serving">
  <data key="d5">8.0</data>
  <data key="d6">The event of pie serving involves guests who partake in eating the pies prepared by Grandma Jones.</data>
  <data key="d7">event participation, social gathering</data>
  <data key="d8">chunk-3ca398087b4d5dd48a0c1fe169e9b7fd</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Total Pieces" target="Eaten Pieces">
  <data key="d5">10.0</data>
  <data key="d6">Total pieces minus remaining pieces equals the number of eaten pieces, illustrating consumption during the event.</data>
  <data key="d7">calculation, food consumption</data>
  <data key="d8">chunk-3ca398087b4d5dd48a0c1fe169e9b7fd</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Total Pieces" target="Remaining Pieces">
  <data key="d5">9.0</data>
  <data key="d6">Total pieces and remaining pieces are related through subtraction to determine how many were eaten.</data>
  <data key="d7">mathematical relationship, event outcome</data>
  <data key="d8">chunk-3ca398087b4d5dd48a0c1fe169e9b7fd</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="GPT-4-V" target="LLaVA-1.6-Mistral-7B">
  <data key="d5">16.0</data>
  <data key="d6">GPT-4-V is utilized in conjunction with LLaVA-1.6-Mistral-7B to generate captions and improve model outputs.</data>
  <data key="d7">model collaboration, caption generation</data>
  <data key="d8">chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="LLaVA-1.6-Mistral-7B" target="Japanese Stable VLM">
  <data key="d5">18.0</data>
  <data key="d6">LLaVA-1.6-Mistral-7B serves as a source model for the Japanese Stable VLM, influencing its training process.</data>
  <data key="d7">model training, source influence</data>
  <data key="d8">chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="LLaVA-1.6-Mistral-7B" target="Fasttext">
  <data key="d5">10.0</data>
  <data key="d6">Fasttext is used for language detection in the context of LLaVA-1.6-Mistral-7B experiments.</data>
  <data key="d7">language detection, experimental methodology</data>
  <data key="d8">chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="LLaVA-1.6-Mistral-7B" target="Japanese Culture">
  <data key="d5">8.0</data>
  <data key="d6">LLaVA-1.6-Mistral-7B's performance reflects its understanding of Japanese cultural nuances.</data>
  <data key="d7">cultural understanding, language model performance</data>
  <data key="d8">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese Stable VLM" target="JA-VG-VQA-500">
  <data key="d5">12.0</data>
  <data key="d6">The Japanese Stable VLM cannot be evaluated on the JA-VG-VQA-500 dataset because it was trained on this dataset.</data>
  <data key="d7">evaluation limitation, dataset training</data>
  <data key="d8">chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese Stable VLM" target="JA-VLM-Bench-In-the-Wild">
  <data key="d5">20.0</data>
  <data key="d6">The performance of the Japanese Stable VLM on the JA-VLM-Bench-In-the-Wild demonstrates its ability to handle culturally-specific content.</data>
  <data key="d7">cultural context, model performance</data>
  <data key="d8">chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese Stable VLM" target="Culturally-Specific Content">
  <data key="d5">9.0</data>
  <data key="d6">The Japanese Stable VLM is designed to handle culturally-specific content, enhancing its relevance in Japanese contexts.</data>
  <data key="d7">cultural relevance, model design</data>
  <data key="d8">chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese Stable VLM" target="LM Benchmark">
  <data key="d5">7.0</data>
  <data key="d6">Japanese Stable VLM is related to the LM Benchmark initiative, focusing on multilingual models and benchmarks.</data>
  <data key="d7">research initiative, multilingual models</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Techniques" target="Model Performance Comparison">
  <data key="d5">8.0</data>
  <data key="d6">Evolutionary techniques are applied to discover optimal ways to combine models, which impacts the results of model performance comparison.</data>
  <data key="d7">model optimization, performance evaluation</data>
  <data key="d8">chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Model Merging" target="EvoSDXL">
  <data key="d5">18.0</data>
  <data key="d6">EvoSDXL demonstrates the effectiveness of evolutionary model merging in the domain of image generation.</data>
  <data key="d7">application, model integration</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Model Merging" target="EvoVLM-JP-v2">
  <data key="d5">14.0</data>
  <data key="d6">EvoVLM-JP-v2 showcases the adaptability of evolutionary model merging in developing advanced models.</data>
  <data key="d7">model development, adaptability</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Model Merging" target="EvoUkiyoe">
  <data key="d5">14.0</data>
  <data key="d6">EvoUkiyoe is another example of a model developed through evolutionary model merging, illustrating its potential across different applications.</data>
  <data key="d7">model development, adaptability</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Model Merging" target="MergeKit">
  <data key="d5">16.0</data>
  <data key="d6">MergeKit implements evolutionary model merging, making the method widely available for practical use.</data>
  <data key="d7">software application, accessibility</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Model Merging" target="Optuna Hub">
  <data key="d5">16.0</data>
  <data key="d6">Optuna Hub incorporates evolutionary model merging, facilitating its application in various AI projects.</data>
  <data key="d7">software application, accessibility</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Model Merging" target="SLERP">
  <data key="d5">8.0</data>
  <data key="d6">SLERP is a technique utilized in evolutionary model merging to create effective model combinations.</data>
  <data key="d7">technique, model integration</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Model Merging" target="Foundation Models">
  <data key="d5">9.0</data>
  <data key="d6">Foundation Models are improved through evolutionary model merging, allowing for the development of more capable AI systems.</data>
  <data key="d7">model enhancement, AI development</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Model Merging" target="Image Diffusion Models">
  <data key="d5">9.0</data>
  <data key="d6">Image Diffusion Models benefit from evolutionary model merging techniques, enhancing their generative capabilities.</data>
  <data key="d7">model enhancement, generative AI</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Model Merging" target="Benchmark Tasks">
  <data key="d5">7.0</data>
  <data key="d6">Benchmark Tasks are utilized to assess the effectiveness of models developed through evolutionary model merging.</data>
  <data key="d7">evaluation, performance metrics</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="EvoSDXL" target="SDXL-Lightning">
  <data key="d5">20.0</data>
  <data key="d6">EvoSDXL successfully merges SDXL-Lightning with standard SDXL fine-tunes to enhance image generation capabilities.</data>
  <data key="d7">model integration, performance improvement</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="LLM Experiments" target="Hugging Face">
  <data key="d5">7.0</data>
  <data key="d6">Datasets for LLM Experiments are available on Hugging Face, indicating a collaboration in AI research.</data>
  <data key="d7">dataset access, research collaboration</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="LLM Experiments" target="GitHub">
  <data key="d5">7.0</data>
  <data key="d6">GitHub hosts datasets used in LLM Experiments, facilitating open-source contributions to AI research.</data>
  <data key="d7">dataset access, open-source collaboration</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Hugging Face" target="Datasets">
  <data key="d5">8.0</data>
  <data key="d6">Hugging Face provides datasets that are essential for the LLM experiments conducted in the research.</data>
  <data key="d7">dataset provision, research support</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="GitHub" target="Datasets">
  <data key="d5">8.0</data>
  <data key="d6">GitHub hosts various datasets used for LLM experiments, facilitating open-source collaboration in AI research.</data>
  <data key="d7">open-source collaboration, dataset access</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="GitHub" target="EvoLLM-JP-A">
  <data key="d5">20.0</data>
  <data key="d6">EvoLLM-JP-A will be released on GitHub, making it accessible to the open-source community.</data>
  <data key="d7">public release, accessibility</data>
  <data key="d8">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese Stable LM Beta" target="Stability AI">
  <data key="d5">9.0</data>
  <data key="d6">Stability AI developed the Japanese Stable LM Beta, showcasing their efforts in advancing language models.</data>
  <data key="d7">model development, organizational effort</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Optimization of Model Merging Recipes" target="OpenAI">
  <data key="d5">8.0</data>
  <data key="d6">OpenAI's research and developments contribute to the methodologies explored in the Evolutionary Optimization of Model Merging Recipes project.</data>
  <data key="d7">research contribution, methodology development</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Stability AI" target="JP Language Model Evaluation Harness">
  <data key="d5">9.0</data>
  <data key="d6">Stability AI developed the JP Language Model Evaluation Harness to evaluate the performance of Japanese language models.</data>
  <data key="d7">AI development, language evaluation</data>
  <data key="d8">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Emanuele Aiello" target="Qwen-VL">
  <data key="d5">18.0</data>
  <data key="d6">Emanuele Aiello is involved in the research and development of Qwen-VL, a vision-language model.&lt;SEP&gt;Emanuele Aiello is involved in the research and development of Qwen-VL, contributing to its capabilities in multimodal understanding.</data>
  <data key="d7">research contribution, model development</data>
  <data key="d8">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="shisa-gamma-7b" target="Stable Diffusion WebUI">
  <data key="d5">14.0</data>
  <data key="d6">Shisa-gamma-7b can be integrated with the Stable Diffusion WebUI for generating outputs based on user inputs.&lt;SEP&gt;Shisa-gamma-7b can be utilized within the Stable Diffusion WebUI for generating content based on user inputs.</data>
  <data key="d7">model application, user interface</data>
  <data key="d8">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Qwen-VL" target="Lili Yu">
  <data key="d5">8.0</data>
  <data key="d6">Lili Yu contributes to the development of Qwen-VL, enhancing its performance in vision-language tasks.</data>
  <data key="d7">collaboration, AI research</data>
  <data key="d8">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Qwen-VL" target="Yixin Nie">
  <data key="d5">8.0</data>
  <data key="d6">Yixin Nie's research efforts are directed towards improving the functionalities of Qwen-VL in multimodal applications.</data>
  <data key="d7">research focus, model enhancement</data>
  <data key="d8">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Qwen-VL" target="Armen Aghajanyan">
  <data key="d5">8.0</data>
  <data key="d6">Armen Aghajanyan collaborates on the development of Qwen-VL, focusing on its training methodologies.</data>
  <data key="d7">collaboration, AI model training</data>
  <data key="d8">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Qwen-VL" target="Barlas Oguz">
  <data key="d5">8.0</data>
  <data key="d6">Barlas Oguz is involved in the research for Qwen-VL, contributing to its overall development and performance improvements.</data>
  <data key="d7">research involvement, model development</data>
  <data key="d8">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Generative AI for Math: Abel" target="InstructBLIP">
  <data key="d5">12.0</data>
  <data key="d6">Generative AI for Math: Abel and InstructBLIP are both projects that leverage generative AI technologies for different applications, showcasing the versatility of AI.&lt;SEP&gt;Generative AI for Math: Abel and InstructBLIP are both projects that utilize generative AI technologies for different applications.</data>
  <data key="d7">AI applications, project comparison</data>
  <data key="d8">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Daniel M Roy" target="arXiv preprint">
  <data key="d5">16.0</data>
  <data key="d6">Daniel M Roy's work on generalization bounds for neural networks is published as an arXiv preprint, contributing to the field of AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Adam Gaier" target="arXiv preprint">
  <data key="d5">16.0</data>
  <data key="d6">Adam Gaier's research on weight agnostic neural networks appears as an arXiv preprint, showcasing advancements in neural processing systems.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Leo Gao" target="arXiv preprint">
  <data key="d5">16.0</data>
  <data key="d6">Leo Gao's framework for few-shot language model evaluation is shared as an arXiv preprint, reflecting ongoing research in language models.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Mistral 7B" target="arXiv preprint">
  <data key="d5">7.0</data>
  <data key="d6">Mistral 7B is discussed in the context of a research paper, indicating its relevance in the study of language models.</data>
  <data key="d7">research relevance, language model</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Baber Abbasi">
  <data key="d5">8.0</data>
  <data key="d6">Baber Abbasi's contributions to few-shot language model evaluation are published as an arXiv preprint, indicating involvement in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Stella Biderman">
  <data key="d5">8.0</data>
  <data key="d6">Stella Biderman's work on few-shot language model evaluation is documented in an arXiv preprint, showcasing advancements in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Sid Black">
  <data key="d5">8.0</data>
  <data key="d6">Sid Black's research contributions to few-shot language model evaluation are published as an arXiv preprint, reflecting his involvement in AI advancements.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Anthony DiPofi">
  <data key="d5">8.0</data>
  <data key="d6">Anthony DiPofi's work on few-shot language model evaluation is shared as an arXiv preprint, contributing to the field of AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Charles Foster">
  <data key="d5">8.0</data>
  <data key="d6">Charles Foster's contributions to few-shot language model evaluation are documented in an arXiv preprint, indicating his role in AI advancements.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Laurence Golding">
  <data key="d5">8.0</data>
  <data key="d6">Laurence Golding's research on few-shot language model evaluation is published as an arXiv preprint, showcasing his involvement in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Jeffrey Hsu">
  <data key="d5">8.0</data>
  <data key="d6">Jeffrey Hsu's work on few-shot language model evaluation is documented in an arXiv preprint, contributing to advancements in AI.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Alain Le Noac'h">
  <data key="d5">8.0</data>
  <data key="d6">Alain Le Noac'h's contributions to few-shot language model evaluation are published as an arXiv preprint, indicating his involvement in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Haonan Li">
  <data key="d5">8.0</data>
  <data key="d6">Haonan Li's work on few-shot language model evaluation is shared as an arXiv preprint, showcasing advancements in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Kyle McDonell">
  <data key="d5">8.0</data>
  <data key="d6">Kyle McDonell's contributions to few-shot language model evaluation are documented in an arXiv preprint, reflecting his role in AI advancements.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Niklas Muennighoff">
  <data key="d5">8.0</data>
  <data key="d6">Niklas Muennighoff's work on few-shot language model evaluation is published as an arXiv preprint, indicating his involvement in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Chris Ociepa">
  <data key="d5">8.0</data>
  <data key="d6">Chris Ociepa's contributions to few-shot language model evaluation are documented in an arXiv preprint, showcasing his involvement in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Jason Phang">
  <data key="d5">8.0</data>
  <data key="d6">Jason Phang's work on few-shot language model evaluation is shared as an arXiv preprint, contributing to advancements in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Laria Reynolds">
  <data key="d5">8.0</data>
  <data key="d6">Laria Reynolds' contributions to few-shot language model evaluation are published as an arXiv preprint, indicating her involvement in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Hailey Schoelkopf">
  <data key="d5">8.0</data>
  <data key="d6">Hailey Schoelkopf's work on few-shot language model evaluation is documented in an arXiv preprint, showcasing her role in AI advancements.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Aviya Skowron">
  <data key="d5">8.0</data>
  <data key="d6">Aviya Skowron's contributions to few-shot language model evaluation are published as an arXiv preprint, indicating her involvement in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Lintang Sutawika">
  <data key="d5">8.0</data>
  <data key="d6">Lintang Sutawika's work on few-shot language model evaluation is shared as an arXiv preprint, contributing to advancements in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Eric Tang">
  <data key="d5">8.0</data>
  <data key="d6">Eric Tang's contributions to few-shot language model evaluation are documented in an arXiv preprint, showcasing his involvement in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Anish Thite">
  <data key="d5">8.0</data>
  <data key="d6">Anish Thite's work on few-shot language model evaluation is shared as an arXiv preprint, contributing to advancements in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Ben Wang">
  <data key="d5">8.0</data>
  <data key="d6">Ben Wang's contributions to few-shot language model evaluation are documented in an arXiv preprint, indicating his role in AI advancements.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Kevin Wang">
  <data key="d5">8.0</data>
  <data key="d6">Kevin Wang's work on few-shot language model evaluation is published as an arXiv preprint, showcasing his involvement in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Andy Zou">
  <data key="d5">8.0</data>
  <data key="d6">Andy Zou's contributions to few-shot language model evaluation are documented in an arXiv preprint, indicating his involvement in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Mor Geva">
  <data key="d5">8.0</data>
  <data key="d6">Mor Geva's work on transformer feed-forward layers is published as an arXiv preprint, contributing to AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Avi Caciularu">
  <data key="d5">8.0</data>
  <data key="d6">Avi Caciularu's contributions to transformer models are documented in an arXiv preprint, showcasing advancements in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Kevin Ro Wang">
  <data key="d5">8.0</data>
  <data key="d6">Kevin Ro Wang's work on transformer models is shared as an arXiv preprint, contributing to advancements in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Yoav Goldberg">
  <data key="d5">8.0</data>
  <data key="d6">Yoav Goldberg's contributions to transformer models are documented in an arXiv preprint, showcasing his role in AI advancements.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Gabriel Ilharco">
  <data key="d5">8.0</data>
  <data key="d6">Gabriel Ilharco's work on editing models is published as an arXiv preprint, contributing to advancements in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Marco Tulio Ribeiro">
  <data key="d5">8.0</data>
  <data key="d6">Marco Tulio Ribeiro's contributions to model interpretability are documented in an arXiv preprint, showcasing his role in AI advancements.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Mitchell Wortsman">
  <data key="d5">8.0</data>
  <data key="d6">Mitchell Wortsman's work on model editing is published as an arXiv preprint, contributing to advancements in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Suchin Gururangan">
  <data key="d5">8.0</data>
  <data key="d6">Suchin Gururangan's contributions to AI model editing are documented in an arXiv preprint, showcasing his role in AI advancements.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Ludwig Schmidt">
  <data key="d5">8.0</data>
  <data key="d6">Ludwig Schmidt's work on model editing is shared as an arXiv preprint, contributing to advancements in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Hannaneh Hajishirzi">
  <data key="d5">8.0</data>
  <data key="d6">Hannaneh Hajishirzi's contributions to AI research are documented in an arXiv preprint, showcasing her role in model interpretability.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Ali Farhadi">
  <data key="d5">8.0</data>
  <data key="d6">Ali Farhadi's work on AI models is published as an arXiv preprint, contributing to advancements in the field.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Gabriel Ilharco" target="Marco Tulio Ribeiro">
  <data key="d5">8.0</data>
  <data key="d6">Gabriel Ilharco and Marco Tulio Ribeiro have collaborated on editing models and adversarial techniques in machine learning.</data>
  <data key="d7">collaboration, adversarial techniques</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Gabriel Ilharco" target="Model Soups">
  <data key="d5">8.0</data>
  <data key="d6">Gabriel Ilharco collaborates with Mitchell Wortsman in the research on model soups, contributing to the findings.</data>
  <data key="d7">research collaboration, model improvement</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Mitchell Wortsman" target="Tom White">
  <data key="d5">8.0</data>
  <data key="d6">Tom White and Mitchell Wortsman are involved in research on generative networks, indicating their contributions to AI advancements.</data>
  <data key="d7">collaboration, research</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Mitchell Wortsman" target="Model Soups">
  <data key="d5">18.0</data>
  <data key="d6">Mitchell Wortsman is a lead author discussing the concept of model soups which average weights of multiple models to improve accuracy.&lt;SEP&gt;Mitchell Wortsman is an author of Model Soups, indicating his involvement in the research.</data>
  <data key="d7">authorship, research collaboration&lt;SEP&gt;research contribution, model improvement</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</edge>
<edge source="Mitchell Wortsman" target="Suchin Gururangan">
  <data key="d5">8.0</data>
  <data key="d6">Mitchell Wortsman and Suchin Gururangan are both involved in research focused on adversarial techniques in natural language processing.</data>
  <data key="d7">research collaboration, adversarial learning</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Ludwig Schmidt" target="Hannaneh Hajishirzi">
  <data key="d5">9.0</data>
  <data key="d6">Ludwig Schmidt and Hannaneh Hajishirzi are prominent researchers collaborating on adversarial robustness in machine learning models.</data>
  <data key="d7">collaboration, research focus</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Ali Farhadi" target="Wenxiang Jiao">
  <data key="d5">7.0</data>
  <data key="d6">Ali Farhadi and Wenxiang Jiao are both involved in machine learning evaluation, particularly in translation tasks.</data>
  <data key="d7">research collaboration, evaluation focus</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Ali Farhadi" target="Model Soups">
  <data key="d5">8.0</data>
  <data key="d6">Ali Farhadi collaborates on the research regarding model soups and their effectiveness in machine learning.</data>
  <data key="d7">research collaboration, model improvement</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Neural Information Processing Systems" target="Nitish Shirish Keskar">
  <data key="d5">16.0</data>
  <data key="d6">Nitish Shirish Keskar contributed to the Neural Information Processing Systems conference through his research on deep learning.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Neural Information Processing Systems" target="Dheevatsa Mudigere">
  <data key="d5">16.0</data>
  <data key="d6">Dheevatsa Mudigere's work on large-batch training was presented at the Neural Information Processing Systems conference.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Neural Information Processing Systems" target="Jorge Nocedal">
  <data key="d5">14.0</data>
  <data key="d6">Jorge Nocedal's research on optimization techniques is relevant to the discussions held at the Neural Information Processing Systems conference.</data>
  <data key="d7">research contribution, conference relevance</data>
  <data key="d8">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Neural Information Processing Systems" target="Mikhail Smelyanskiy">
  <data key="d5">16.0</data>
  <data key="d6">Mikhail Smelyanskiy's contributions in AI are showcased at the Neural Information Processing Systems conference.</data>
  <data key="d7">research contribution, conference relevance</data>
  <data key="d8">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Neural Information Processing Systems" target="Ping Tak Peter Tang">
  <data key="d5">16.0</data>
  <data key="d6">Ping Tak Peter Tang's work in deep learning is featured in the Neural Information Processing Systems conference proceedings.</data>
  <data key="d7">research contribution, conference relevance</data>
  <data key="d8">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Neural Information Processing Systems" target="Large-Batch Training">
  <data key="d5">8.0</data>
  <data key="d6">Research on large-batch training is often discussed at the Neural Information Processing Systems conference, indicating its significance in the field.</data>
  <data key="d7">research focus, conference theme</data>
  <data key="d8">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Maxime Labonne" target="International Conference on Learning Representations">
  <data key="d5">18.0</data>
  <data key="d6">Maxime Labonne presented his work at the International Conference on Learning Representations, highlighting advancements in model merging.</data>
  <data key="d7">research presentation, conference participation</data>
  <data key="d8">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Maxime Labonne" target="Hugging Face Blog">
  <data key="d5">16.0</data>
  <data key="d6">Maxime Labonne shared insights on model merging via the Hugging Face Blog, contributing to community knowledge.</data>
  <data key="d7">knowledge sharing, community contribution</data>
  <data key="d8">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Maxime Labonne" target="arXiv">
  <data key="d5">18.0</data>
  <data key="d6">Maxime Labonne's research papers are available on arXiv, contributing to the dissemination of knowledge in AI.</data>
  <data key="d7">research dissemination, open access</data>
  <data key="d8">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv" target="CoRR">
  <data key="d5">14.0</data>
  <data key="d6">CoRR and arXiv are both repositories that facilitate the sharing of computer science research papers.</data>
  <data key="d7">repository comparison, research sharing</data>
  <data key="d8">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Henning Petzka" target="Advances in Neural Information Processing Systems">
  <data key="d5">8.0</data>
  <data key="d6">Henning Petzka is an author of a paper presented at the Advances in Neural Information Processing Systems conference.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Michael Kamp" target="Advances in Neural Information Processing Systems">
  <data key="d5">8.0</data>
  <data key="d6">Michael Kamp is a co-author of a paper presented at the Advances in Neural Information Processing Systems conference.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Advances in Neural Information Processing Systems" target="Colin Raffel">
  <data key="d5">7.0</data>
  <data key="d6">Colin Raffel's work on machine learning models relates to discussions at the Advances in Neural Information Processing Systems conference.</data>
  <data key="d7">thematic relevance, research influence</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Advances in Neural Information Processing Systems" target="Proceedings of the AAAI Conference on Artificial Intelligence">
  <data key="d5">9.0</data>
  <data key="d6">Both are significant publications in the field of artificial intelligence and machine learning research.</data>
  <data key="d7">academic publications, AI research</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Advances in Neural Information Processing Systems" target="Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition">
  <data key="d5">9.0</data>
  <data key="d6">Both are significant publications in the field of computer vision and machine learning research.</data>
  <data key="d7">academic publications, computer vision research</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Advances in Neural Information Processing Systems" target="International Conference on Computational Linguistics">
  <data key="d5">9.0</data>
  <data key="d6">Both are important conferences for presenting research in computational linguistics and AI.</data>
  <data key="d7">academic conferences, AI research</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Colin Raffel" target="LM Benchmark">
  <data key="d5">6.0</data>
  <data key="d6">Colin Raffel's advocacy for open-source methodologies aligns with the goals of the LM Benchmark initiative.</data>
  <data key="d7">methodological alignment, research initiative</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Kigali" target="The Eleventh International Conference on Learning Representations">
  <data key="d5">9.0</data>
  <data key="d6">The Eleventh International Conference on Learning Representations is hosted in Kigali, Rwanda.</data>
  <data key="d7">event location, international conference</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="The Eleventh International Conference on Learning Representations" target="Freda Shi">
  <data key="d5">8.0</data>
  <data key="d6">Freda Shi is a co-author of a paper presented at The Eleventh International Conference on Learning Representations.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="The Eleventh International Conference on Learning Representations" target="Mirac Suzgun">
  <data key="d5">8.0</data>
  <data key="d6">Mirac Suzgun is a co-author of a paper presented at The Eleventh International Conference on Learning Representations.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="The Eleventh International Conference on Learning Representations" target="Markus Freitag">
  <data key="d5">8.0</data>
  <data key="d6">Markus Freitag is a co-author of a paper presented at The Eleventh International Conference on Learning Representations.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="The Eleventh International Conference on Learning Representations" target="Xuezhi Wang">
  <data key="d5">8.0</data>
  <data key="d6">Xuezhi Wang is a co-author of a paper presented at The Eleventh International Conference on Learning Representations.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="The Eleventh International Conference on Learning Representations" target="Suraj Srivats">
  <data key="d5">8.0</data>
  <data key="d6">Suraj Srivats is a co-author of a paper presented at The Eleventh International Conference on Learning Representations.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="The Eleventh International Conference on Learning Representations" target="Soroush Vosoughi">
  <data key="d5">8.0</data>
  <data key="d6">Soroush Vosoughi is a co-author of a paper presented at The Eleventh International Conference on Learning Representations.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="The Eleventh International Conference on Learning Representations" target="Hyung Won Chung">
  <data key="d5">8.0</data>
  <data key="d6">Hyung Won Chung is a co-author of a paper presented at The Eleventh International Conference on Learning Representations.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="The Eleventh International Conference on Learning Representations" target="Yi Tay">
  <data key="d5">8.0</data>
  <data key="d6">Yi Tay is a co-author of a paper presented at The Eleventh International Conference on Learning Representations.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="The Eleventh International Conference on Learning Representations" target="Sebastian Ruder">
  <data key="d5">8.0</data>
  <data key="d6">Sebastian Ruder is a co-author of a paper presented at The Eleventh International Conference on Learning Representations.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="The Eleventh International Conference on Learning Representations" target="Denny Zhou">
  <data key="d5">8.0</data>
  <data key="d6">Denny Zhou is a co-author of a paper presented at The Eleventh International Conference on Learning Representations.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="The Eleventh International Conference on Learning Representations" target="Dipanjan Das">
  <data key="d5">8.0</data>
  <data key="d6">Dipanjan Das is a co-author of a paper presented at The Eleventh International Conference on Learning Representations.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="The Eleventh International Conference on Learning Representations" target="Jason Wei">
  <data key="d5">8.0</data>
  <data key="d6">Jason Wei is a co-author of a paper presented at The Eleventh International Conference on Learning Representations.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Kenneth O Stanley" target="Risto Miikkulainen">
  <data key="d5">8.0</data>
  <data key="d6">Kenneth O Stanley and Risto Miikkulainen co-authored a paper on evolving neural networks, indicating collaboration in AI research.</data>
  <data key="d7">collaboration, research</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Kenneth O Stanley" target="Evolving Neural Networks Through Augmenting Topologies">
  <data key="d5">9.0</data>
  <data key="d6">Kenneth O Stanley is a co-author of the paper Evolving Neural Networks Through Augmenting Topologies, indicating his contribution to the research.</data>
  <data key="d7">authorship, research collaboration</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Risto Miikkulainen" target="Evolving Neural Networks Through Augmenting Topologies">
  <data key="d5">9.0</data>
  <data key="d6">Risto Miikkulainen is a co-author of the paper Evolving Neural Networks Through Augmenting Topologies, indicating his contribution to the research.</data>
  <data key="d7">authorship, research collaboration</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Marc Pickett" target="Transformer Layers as Painters">
  <data key="d5">9.0</data>
  <data key="d6">Marc Pickett is a co-author of the paper Transformer Layers as Painters, indicating his contribution to the research.</data>
  <data key="d7">authorship, research collaboration</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Yi-Lin Sung" target="Linjie Li">
  <data key="d5">8.0</data>
  <data key="d6">Yi-Lin Sung and Linjie Li collaborated on a study of multimodal model merging, showcasing their joint research efforts in AI.</data>
  <data key="d7">collaboration, research</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Yi-Lin Sung" target="An Empirical Study of Multimodal Model Merging">
  <data key="d5">9.0</data>
  <data key="d6">Yi-Lin Sung is an author of the paper An Empirical Study of Multimodal Model Merging, indicating his involvement in the research.</data>
  <data key="d7">authorship, research collaboration</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Linjie Li" target="An Empirical Study of Multimodal Model Merging">
  <data key="d5">9.0</data>
  <data key="d6">Linjie Li is a co-author of the paper An Empirical Study of Multimodal Model Merging, indicating his contribution to the research.</data>
  <data key="d7">authorship, research collaboration</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Tom White" target="Sampling Generative Networks">
  <data key="d5">9.0</data>
  <data key="d6">Tom White is the author of Sampling Generative Networks, indicating his contribution to the research.</data>
  <data key="d7">authorship, research collaboration</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Samir Ya Gadre" target="Model Soups">
  <data key="d5">8.0</data>
  <data key="d6">Samir Ya Gadre is a co-author on the research paper discussing model soups, contributing to its insights.</data>
  <data key="d7">research collaboration, model improvement</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Rebecca Roelofs" target="Model Soups">
  <data key="d5">8.0</data>
  <data key="d6">Rebecca Roelofs is involved in the research on model soups, enhancing the understanding of model averaging.</data>
  <data key="d7">research collaboration, model improvement</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Raphael Gontijo-Lopes" target="Model Soups">
  <data key="d5">8.0</data>
  <data key="d6">Raphael Gontijo-Lopes co-authors the study on model soups, focusing on accuracy improvements.</data>
  <data key="d7">research collaboration, model improvement</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Ari S Morcos" target="Model Soups">
  <data key="d5">8.0</data>
  <data key="d6">Ari S Morcos contributes to the research on model soups, emphasizing its significance in machine learning.</data>
  <data key="d7">research collaboration, model improvement</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Hongseok Namkoong" target="Model Soups">
  <data key="d5">8.0</data>
  <data key="d6">Hongseok Namkoong is a co-author discussing the impact of model soups on model performance.</data>
  <data key="d7">research collaboration, model improvement</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Yair Carmon" target="Model Soups">
  <data key="d5">8.0</data>
  <data key="d6">Yair Carmon is a co-author of the paper on model soups, contributing to the research findings.</data>
  <data key="d7">research collaboration, model improvement</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Simon Kornblith" target="Model Soups">
  <data key="d5">8.0</data>
  <data key="d6">Simon Kornblith is involved in the research on model soups, enhancing the understanding of model merging techniques.</data>
  <data key="d7">research collaboration, model improvement</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Prateek Yadav" target="Leshem Choshen">
  <data key="d5">8.0</data>
  <data key="d6">Prateek Yadav and Leshem Choshen are authors who collaborated on compression techniques for AI models, indicating joint research efforts.</data>
  <data key="d7">collaboration, research</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Prateek Yadav" target="Compeft">
  <data key="d5">9.0</data>
  <data key="d6">Prateek Yadav is the author of Compeft, indicating his contribution to the research.</data>
  <data key="d7">authorship, research collaboration</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="International Conference on Machine Learning" target="Advances in Neural Information Processing Systems 36">
  <data key="d5">9.0</data>
  <data key="d6">Both conferences are significant events for presenting advancements in AI and machine learning research.</data>
  <data key="d7">event significance, research</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="International Conference on Machine Learning" target="Model Soups">
  <data key="d5">9.0</data>
  <data key="d6">The research on model soups is presented at the International Conference on Machine Learning, showcasing its relevance.</data>
  <data key="d7">conference presentation, research dissemination</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="International Conference on Machine Learning" target="PMLR">
  <data key="d5">9.0</data>
  <data key="d6">PMLR publishes research presented at the International Conference on Machine Learning, disseminating findings in the field.</data>
  <data key="d7">publication, research dissemination</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Model Soups" target="mixup">
  <data key="d5">7.0</data>
  <data key="d6">Mixup is a technique that may relate to the concepts discussed in model soups, enhancing model performance.</data>
  <data key="d7">technique application, model improvement</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Model Soups" target="Promptrobust">
  <data key="d5">7.0</data>
  <data key="d6">Promptrobust relates to the evaluation of models, which can be influenced by the techniques discussed in model soups.</data>
  <data key="d7">evaluation framework, model improvement</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Model Soups" target="Promptbench">
  <data key="d5">7.0</data>
  <data key="d6">Promptbench provides a framework for evaluating models, relevant to the discussions on model soups.</data>
  <data key="d7">evaluation framework, model improvement</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Yi: Open Foundation Models by 01.ai" target="Alex Young">
  <data key="d5">9.0</data>
  <data key="d6">Alex Young is an author of Yi: Open Foundation Models by 01.ai, indicating his contribution to the research.</data>
  <data key="d7">authorship, research collaboration</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Language Models are Super Mario" target="Le Yu">
  <data key="d5">9.0</data>
  <data key="d6">Le Yu is an author of Language Models are Super Mario, indicating his involvement in the research.</data>
  <data key="d7">authorship, research collaboration</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Stability AI Japan" target="EvoLLM-JP">
  <data key="d5">16.0</data>
  <data key="d6">Stability AI Japan developed the EvoLLM-JP model, which is based on their evaluation harness.</data>
  <data key="d7">model development, organizational contribution</data>
  <data key="d8">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Stability AI Japan" target="lm-eval-harness">
  <data key="d5">8.0</data>
  <data key="d6">Stability AI Japan developed a fork of the lm-eval-harness for evaluating language models in Japanese.</data>
  <data key="d7">model evaluation, organizational development</data>
  <data key="d8">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="EvoLLM-JP" target="EvoLLM-JP-A">
  <data key="d5">18.0</data>
  <data key="d6">EvoLLM-JP-A is an improved version of EvoLLM-JP, showcasing advancements in language model performance.</data>
  <data key="d7">model improvement, evolution</data>
  <data key="d8">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="EvoLLM-JP" target="Setsubun">
  <data key="d5">14.0</data>
  <data key="d6">EvoLLM-JP demonstrates improved understanding of Japanese culture, as evidenced by its performance on questions about Setsubun.</data>
  <data key="d7">cultural understanding, language capability</data>
  <data key="d8">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="EvoLLM-JP" target="Japanese Language Model Evaluation Harness">
  <data key="d5">8.0</data>
  <data key="d6">EvoLLM-JP is evaluated using the Japanese Language Model Evaluation Harness to measure its proficiency.</data>
  <data key="d7">evaluation, language proficiency</data>
  <data key="d8">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Setsubun" target="Risshun">
  <data key="d5">29.0</data>
  <data key="d6">Setsubun is celebrated the day before Risshun, marking a significant cultural event in Japan.&lt;SEP&gt;Setsubun marks the day before Risshun, which signifies the start of spring according to the lunisolar calendar.</data>
  <data key="d7">cultural significance, seasonal events&lt;SEP&gt;cultural significance, seasonal transition</data>
  <data key="d8">chunk-19ca32b4278ab8247ef043d83b7b65ef&lt;SEP&gt;chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Setsubun" target="February">
  <data key="d5">16.0</data>
  <data key="d6">Setsubun is celebrated in February, marking the transition to spring.</data>
  <data key="d7">cultural event, seasonal celebration</data>
  <data key="d8">chunk-19ca32b4278ab8247ef043d83b7b65ef</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Setsubun" target="2021">
  <data key="d5">9.0</data>
  <data key="d6">Last year's Setsubun was celebrated in 2021 on February 3.</data>
  <data key="d7">historical event, annual celebration</data>
  <data key="d8">chunk-19ca32b4278ab8247ef043d83b7b65ef</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Setsubun" target="2022">
  <data key="d5">9.0</data>
  <data key="d6">This year's Setsubun is celebrated in 2022 on February 4.</data>
  <data key="d7">historical event, annual celebration</data>
  <data key="d8">chunk-19ca32b4278ab8247ef043d83b7b65ef</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Setsubun" target="John">
  <data key="d5">5.0</data>
  <data key="d6">John's problem-solving task involves calculations related to the dimensions of the boxes, which ties into the broader context of Setsubun as a traditional event.</data>
  <data key="d7">cultural context, problem-solving</data>
  <data key="d8">chunk-62a104ca37857a0566f04bc4cd794e8c</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="lm-eval-harness" target="Rinna">
  <data key="d5">9.0</data>
  <data key="d6">The lm-eval-harness is compatible with results from Rinna's leaderboards, allowing for direct comparison of scores.</data>
  <data key="d7">performance comparison, evaluation framework</data>
  <data key="d8">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Llama-2-13b-hf" target="13B source models">
  <data key="d5">10.0</data>
  <data key="d6">Llama-2-13b-hf is categorized as a 13B source model, highlighting its performance metrics.".</data>
  <data key="d7">category classification, model type</data>
  <data key="d8">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Swallow-70b-instruct-hf" target="Swallow-70b-hf">
  <data key="d5">16.0</data>
  <data key="d6">Both models are part of the Swallow family, showcasing high performance in various tasks and evaluations.&lt;SEP&gt;Both models are part of the Swallow family, showcasing high performance in various tasks.</data>
  <data key="d7">model family, performance comparison</data>
  <data key="d8">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Swallow-70b-hf" target="japanese-stablelm-instruct-beta-70b">
  <data key="d5">8.0</data>
  <data key="d6">Both models are high-performance AI models that focus on instruction and task execution, indicating similar applications.</data>
  <data key="d7">task execution, performance metrics</data>
  <data key="d8">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="japanese-stablelm-base-beta-70b" target="nekomata-14b-instruction">
  <data key="d5">18.0</data>
  <data key="d6">Both models are designed for advanced performance in instruction-based tasks, indicating a shared focus on instructional capabilities.</data>
  <data key="d7">instructional focus, model comparison</data>
  <data key="d8">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="nekomata-14b" target="Qwen-14B">
  <data key="d5">8.0</data>
  <data key="d6">Both models are designed to deliver competitive performance metrics across various tasks, highlighting their capability in AI evaluations.</data>
  <data key="d7">performance metrics, AI capabilities</data>
  <data key="d8">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="youri-7b-chat" target="Llama-2-70b-chat-hf">
  <data key="d5">18.0</data>
  <data key="d6">Both models are optimized for chat interactions, suggesting they serve similar purposes in conversational AI applications.&lt;SEP&gt;Both models are optimized for chat interactions, suggesting they serve similar purposes in conversational AI.</data>
  <data key="d7">chat optimization, conversational models</data>
  <data key="d8">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="youri-7b-instruction" target="nekomata-14b-instruction-gguf">
  <data key="d5">9.0</data>
  <data key="d6">Both models are tailored for instruction tasks, indicating their shared focus on enhancing user interaction through effective learning models.</data>
  <data key="d7">instructional focus, model enhancement</data>
  <data key="d8">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Swallow-MX-8x7b-NVE-v0.1" target="youri-7b-chat-gptq">
  <data key="d5">14.0</data>
  <data key="d6">Both models are part of the advanced AI model family, each targeting specific performance metrics in chat and interaction tasks.</data>
  <data key="d7">model family, performance metrics</data>
  <data key="d8">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Llama-2-70b-chat-hf" target="youri-7b-instruction-gptq">
  <data key="d5">16.0</data>
  <data key="d6">Both models are designed for natural language processing tasks and contribute to advancements in AI language understanding.&lt;SEP&gt;Both models are developed for natural language processing tasks, contributing to advancements in AI language understanding and instruction following.</data>
  <data key="d7">language models, AI development</data>
  <data key="d8">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="japanese-stablelm-base-gamma-7b" target="Swallow-13b-instruct-hf">
  <data key="d5">14.0</data>
  <data key="d6">Both models focus on language processing and instruction-following capabilities, enhancing their effectiveness in applications.&lt;SEP&gt;Both models focus on language processing and instruction-following capabilities, enhancing their effectiveness in various applications.</data>
  <data key="d7">language processing, instruction following</data>
  <data key="d8">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Swallow-13b-instruct-hf" target="llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0">
  <data key="d5">8.0</data>
  <data key="d6">Both models are designed for instruction-following tasks, highlighting advancements in AI's ability to understand and generate language based on user instructions.</data>
  <data key="d7">instruction-following, AI advancements</data>
  <data key="d8">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="nekomata-14b-gguf" target="Swallow-MS-7b-v0.1">
  <data key="d5">16.0</data>
  <data key="d6">Both models are part of the growing field of advanced language models, emphasizing performance and multi-task learning capabilities.&lt;SEP&gt;Both models are part of the growing field of advanced language models, emphasizing performance and multi-task learning.</data>
  <data key="d7">model performance, multi-task learning</data>
  <data key="d8">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Swallow-7b-instruct-hf" target="japanese-stablelm-instruct-beta-7b">
  <data key="d5">18.0</data>
  <data key="d6">Both models are designed for instruction-based tasks, indicating a trend towards improving user interaction with AI technologies.&lt;SEP&gt;Both models are designed for instruction-based tasks, indicating a trend towards improving user interaction with AI.</data>
  <data key="d7">instruction-based models, user interaction</data>
  <data key="d8">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Qwen-7B" target="youri-7b-gptq">
  <data key="d5">14.0</data>
  <data key="d6">Both models are examples of 7 billion parameter language models, showcasing the trend of scaling AI capabilities in natural language processing.&lt;SEP&gt;Both models are examples of 7 billion parameter language models, showcasing the trend of scaling AI capabilities.</data>
  <data key="d7">model scaling, AI capabilities</data>
  <data key="d8">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="youri-7b-gptq" target="youri-7b">
  <data key="d5">28.0</data>
  <data key="d6">youri-7b-gptq and youri-7b are related as they represent different versions or configurations of the same model family.&lt;SEP&gt;youri-7b-gptq is a variant of youri-7b, indicating a development in the same model lineage for improved performance in language tasks.&lt;SEP&gt;youri-7b-gptq is a variant of youri-7b, indicating a development in the same model lineage for improved performance.</data>
  <data key="d7">model variants&lt;SEP&gt;model variants, performance improvement</data>
  <data key="d8">chunk-de4fd08ec74cc6d0a33238a38b1bdb52&lt;SEP&gt;chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="youri-7b-gptq" target="nekomata-7b-gguf">
  <data key="d5">7.0</data>
  <data key="d6">nekomata-7b-gguf and youri-7b-gptq are related as they are both models with performance metrics reflecting their effectiveness in AI applications.</data>
  <data key="d7">model effectiveness</data>
  <data key="d8">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="youri-7b" target="stockmark-13b">
  <data key="d5">7.0</data>
  <data key="d6">stockmark-13b and youri-7b are both models that have performance metrics indicating their capabilities in AI tasks.</data>
  <data key="d7">model capabilities</data>
  <data key="d8">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="ELYZA-japanese-Llama-2-7b-instruct" target="ELYZA-japanese-Llama-2-7b">
  <data key="d5">18.0</data>
  <data key="d6">Both ELYZA models are related as they belong to the same family of instruction-based models with performance metrics.</data>
  <data key="d7">model family</data>
  <data key="d8">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="japanese-stablelm-instruct-ja_vocab-beta-7b" target="japanese-stablelm-base-ja_vocab-beta-7b">
  <data key="d5">16.0</data>
  <data key="d6">Both Japanese StableLM models are related as they are part of the same series with varying performance metrics.</data>
  <data key="d7">model series</data>
  <data key="d8">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Llama-2-7b-hf" target="calm2-7b">
  <data key="d5">6.0</data>
  <data key="d6">Both calm2-7b and Llama-2-7b-hf are models that focus on performance metrics in various AI tasks.</data>
  <data key="d7">model comparison</data>
  <data key="d8">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="plamo-13b" target="plamo-13b-instruct">
  <data key="d5">14.0</data>
  <data key="d6">plamo-13b and plamo-13b-instruct are related as they represent different configurations of the same model family.</data>
  <data key="d7">model variants</data>
  <data key="d8">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="EvoLLM-JP-v1-7B" target="Koi-nobori">
  <data key="d5">16.0</data>
  <data key="d6">EvoLLM-JP-v1-7B's performance in understanding Koi-nobori reflects its grasp of Japanese cultural traditions.</data>
  <data key="d7">cultural understanding, language model performance</data>
  <data key="d8">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="EvoLLM-JP-v1-7B" target="Japanese Culture">
  <data key="d5">18.0</data>
  <data key="d6">EvoLLM-JP-v1-7B demonstrated knowledge of Japanese culture, affecting its performance on tasks related to cultural context.</data>
  <data key="d7">cultural fluency, language model</data>
  <data key="d8">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="EvoLLM-JP-v1-7B" target="Traffic Lights">
  <data key="d5">14.0</data>
  <data key="d6">EvoLLM-JP-v1-7B's understanding of traffic lights in Japan shows its ability to interpret local terminology and cultural nuances.</data>
  <data key="d7">language understanding, cultural context</data>
  <data key="d8">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="EvoLLM-JP-v1-7B" target="MGSM Test Set">
  <data key="d5">8.0</data>
  <data key="d6">EvoLLM-JP-v1-7B was tested using the MGSM Test Set to evaluate its mathematical reasoning capabilities.</data>
  <data key="d7">evaluation, performance testing</data>
  <data key="d8">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Koi-nobori" target="Japanese-Stable-VLM">
  <data key="d5">9.0</data>
  <data key="d6">Japanese-Stable-VLM successfully identified Koi-nobori, indicating its cultural knowledge.</data>
  <data key="d7">cultural understanding, language model performance</data>
  <data key="d8">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese Culture" target="EvoVLM-JP">
  <data key="d5">9.0</data>
  <data key="d6">EvoVLM-JP's fluency in Japanese expression demonstrates its deep understanding of Japanese culture.</data>
  <data key="d7">cultural fluency, language model</data>
  <data key="d8">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Traffic Lights" target="EvoVLM-JP">
  <data key="d5">10.0</data>
  <data key="d6">EvoVLM-JP accurately identifies the color of traffic lights in Japan, showcasing its cultural knowledge.</data>
  <data key="d7">cultural understanding, language model performance</data>
  <data key="d8">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="2021" target="2022">
  <data key="d5">12.0</data>
  <data key="d6">The years 2021 and 2022 are relevant for the calculations related to the dates of Setsubun.</data>
  <data key="d7">temporal relationship, event sequencing</data>
  <data key="d8">chunk-19ca32b4278ab8247ef043d83b7b65ef</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="2021" target="2023">
  <data key="d5">14.0</data>
  <data key="d6">The year 2021 is used to calculate the combined date for Setsubun in 2023.</data>
  <data key="d7">temporal relationship, event calculation</data>
  <data key="d8">chunk-19ca32b4278ab8247ef043d83b7b65ef</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="2021" target="Last Year's Setsubun">
  <data key="d5">17.0</data>
  <data key="d6">Last year's Setsubun corresponds to the year 2021, marking the date of celebration.&lt;SEP&gt;Last year's Setsubun was celebrated in 2021 on February 2.</data>
  <data key="d7">event year, annual tradition&lt;SEP&gt;historical event, annual celebration</data>
  <data key="d8">chunk-19ca32b4278ab8247ef043d83b7b65ef</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="2022" target="2023">
  <data key="d5">14.0</data>
  <data key="d6">The year 2022 is used to calculate the combined date for Setsubun in 2023.</data>
  <data key="d7">temporal relationship, event calculation</data>
  <data key="d8">chunk-19ca32b4278ab8247ef043d83b7b65ef</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="2022" target="This Year's Setsubun">
  <data key="d5">17.0</data>
  <data key="d6">This year's Setsubun corresponds to the year 2022, marking the date of celebration.&lt;SEP&gt;This year's Setsubun is celebrated in 2022 on February 3.</data>
  <data key="d7">event year, annual tradition&lt;SEP&gt;historical event, annual celebration</data>
  <data key="d8">chunk-19ca32b4278ab8247ef043d83b7b65ef</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="John" target="Boxes">
  <data key="d5">16.0</data>
  <data key="d6">John calculates the inner volume of the boxes, indicating his direct involvement with them.&lt;SEP&gt;John owns three boxes, which are the subject of the volume calculation problem he is tasked with solving.</data>
  <data key="d7">measurement, calculation&lt;SEP&gt;ownership, problem-solving</data>
  <data key="d8">chunk-30dbd5c2c163db07de6f5e159458d713&lt;SEP&gt;chunk-62a104ca37857a0566f04bc4cd794e8c</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="John" target="Volume Calculation">
  <data key="d5">9.0</data>
  <data key="d6">John is directly involved in the Volume Calculation event as he needs to determine the total inner volume of the boxes he owns.</data>
  <data key="d7">mathematical problem, involvement</data>
  <data key="d8">chunk-62a104ca37857a0566f04bc4cd794e8c</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="John" target="72 cubic inches">
  <data key="d5">9.0</data>
  <data key="d6">John arrives at the total inner volume of 72 cubic inches as a result of his calculations.</data>
  <data key="d7">result, calculation outcome</data>
  <data key="d8">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Boxes" target="Volume Calculation">
  <data key="d5">10.0</data>
  <data key="d6">The Boxes are the objects for which the Volume Calculation is being performed, making them central to the event.</data>
  <data key="d7">object of calculation, mathematical focus</data>
  <data key="d8">chunk-62a104ca37857a0566f04bc4cd794e8c</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Boxes" target="Dimensions">
  <data key="d5">8.0</data>
  <data key="d6">The boxes have specific dimensions that are crucial for calculating their inner volume.</data>
  <data key="d7">measurement, physical properties</data>
  <data key="d8">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Dimensions" target="Inner Volume">
  <data key="d5">9.0</data>
  <data key="d6">The inner volume is calculated based on the dimensions of the box after accounting for wall thickness.</data>
  <data key="d7">calculation, geometry</data>
  <data key="d8">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Dimensions" target="1 inch">
  <data key="d5">7.0</data>
  <data key="d6">The 1 inch thickness of the walls affects the overall dimensions of the box and subsequently the inner volume.</data>
  <data key="d7">measurement impact, wall thickness</data>
  <data key="d8">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Dimensions" target="5 inches">
  <data key="d5">8.0</data>
  <data key="d6">The original length of 5 inches is one of the dimensions used to determine the inner volume of the box.</data>
  <data key="d7">measurement, physical properties</data>
  <data key="d8">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Dimensions" target="6 inches">
  <data key="d5">8.0</data>
  <data key="d6">The original width of 6 inches is one of the dimensions used to determine the inner volume of the box.</data>
  <data key="d7">measurement, physical properties</data>
  <data key="d8">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Dimensions" target="4 inches">
  <data key="d5">8.0</data>
  <data key="d6">The original height of 4 inches is one of the dimensions used to determine the inner volume of the box.</data>
  <data key="d7">measurement, physical properties</data>
  <data key="d8">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Inner Volume" target="3 boxes">
  <data key="d5">10.0</data>
  <data key="d6">The inner volume of one box is multiplied by three to find the total volume for all boxes.</data>
  <data key="d7">aggregation, total volume</data>
  <data key="d8">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Peace Tower" target="August 15, 1945">
  <data key="d5">9.0</data>
  <data key="d6">The Peace Tower was destroyed on August 15, 1945, during World War II, marking a significant historical event.</data>
  <data key="d7">historical event, destruction</data>
  <data key="d8">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Peace Tower" target="1964">
  <data key="d5">8.0</data>
  <data key="d6">The reconstruction of the Peace Tower in 1964 represents its rebirth after World War II's destruction.</data>
  <data key="d7">reconstruction, historical significance</data>
  <data key="d8">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="World War II" target="Hiroshima">
  <data key="d5">20.0</data>
  <data key="d6">Hiroshima was significantly impacted during World War II, particularly by the atomic bombing that led to massive destruction.&lt;SEP&gt;Hiroshima's historical significance is tied to World War II, particularly the atomic bombing that occurred there.</data>
  <data key="d7">historical impact, conflict</data>
  <data key="d8">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Hiroshima" target="Atomic Bomb Dome">
  <data key="d5">9.0</data>
  <data key="d6">The Atomic Bomb Dome is located in Hiroshima and serves as a memorial for the events that took place during World War II.</data>
  <data key="d7">location, memorial</data>
  <data key="d8">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Atomic Bomb Dome" target="August 6, 1945">
  <data key="d5">10.0</data>
  <data key="d6">The Atomic Bomb Dome stands as a reminder of the destruction that occurred on August 6, 1945, when the atomic bomb was dropped on Hiroshima.</data>
  <data key="d7">historical significance, memorial</data>
  <data key="d8">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Atomic Bomb Dome" target="Hiroshima Prefectural Industrial Promotion Hall">
  <data key="d5">8.0</data>
  <data key="d6">The Hiroshima Prefectural Industrial Promotion Hall was the original structure that became the Atomic Bomb Dome after the bombing.</data>
  <data key="d7">historical transformation, identity</data>
  <data key="d8">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Atomic Bomb Dome" target="UNESCO World Heritage Site">
  <data key="d5">9.0</data>
  <data key="d6">The Atomic Bomb Dome was designated as a UNESCO World Heritage Site in recognition of its cultural and historical importance.</data>
  <data key="d7">cultural significance, preservation</data>
  <data key="d8">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Yue Zhou" target="Jilin University">
  <data key="d5">18.0</data>
  <data key="d6">Yue Zhou is affiliated with Jilin University, where he conducts research in artificial intelligence and model merging techniques.</data>
  <data key="d7">affiliation, research</data>
  <data key="d8">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Yi Chang" target="Engineering Research Center of Knowledge-Driven Human-Machine Intelligence">
  <data key="d5">16.0</data>
  <data key="d6">Yi Chang works at the Engineering Research Center, contributing to the study of human-machine intelligence and model merging.</data>
  <data key="d7">affiliation, research</data>
  <data key="d8">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Yi Chang" target="Nuo Xu">
  <data key="d5">8.0</data>
  <data key="d6">Nuo Xu and Yi Chang are both researchers working on evaluation datasets, contributing to health-related assessments in language models.</data>
  <data key="d7">research collaboration, dataset focus</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Yi Chang" target="Yuan Wu">
  <data key="d5">7.0</data>
  <data key="d6">Yi Chang and Yuan Wu are involved in similar research areas, focusing on evaluating language models and their health applications.</data>
  <data key="d7">research alignment, evaluation focus</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Yi Chang" target="Rethinking data selection at scale">
  <data key="d5">8.0</data>
  <data key="d6">Yi Chang is a co-author contributing to the research on data selection methods in machine learning.</data>
  <data key="d7">research collaboration, data selection</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Yuan Wu" target="Jilin University">
  <data key="d5">18.0</data>
  <data key="d6">Yuan Wu is associated with Jilin University, contributing to research in model merging and AI technologies.</data>
  <data key="d7">affiliation, research</data>
  <data key="d8">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Yuan Wu" target="Yupeng Chang">
  <data key="d5">6.0</data>
  <data key="d6">Yupeng Chang and Yuan Wu are co-authors of a paper on bias-alleviating low-rank adaptation.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Yuan Wu" target="Rethinking data selection at scale">
  <data key="d5">8.0</data>
  <data key="d6">Yuan Wu is involved in the research on effective data selection methods in machine learning.</data>
  <data key="d7">research collaboration, data selection</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Jilin University" target="International Center of Future Science">
  <data key="d5">14.0</data>
  <data key="d6">Jilin University hosts the International Center of Future Science, which focuses on innovative research in various scientific fields.</data>
  <data key="d7">institutional relationship, research</data>
  <data key="d8">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Mixup Model Merge (M3)" target="Large Language Models (LLMs)">
  <data key="d5">20.0</data>
  <data key="d6">M3 is a method designed to enhance the performance of LLMs by improving their merging capabilities.</data>
  <data key="d7">methodology, AI advancement</data>
  <data key="d8">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Natural Language Processing (NLP)" target="Large Language Models (LLMs)">
  <data key="d5">18.0</data>
  <data key="d6">NLP utilizes LLMs to perform complex language tasks, showcasing their capabilities in the field.</data>
  <data key="d7">field application, technology</data>
  <data key="d8">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Large Language Models (LLMs)" target="Supervised Fine-Tuning (SFT)">
  <data key="d5">8.0</data>
  <data key="d6">SFT is a technique used to refine the performance of LLMs for specific tasks, making them more effective in application.</data>
  <data key="d7">training technique, model performance</data>
  <data key="d8">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="WizardLM-13B">
  <data key="d5">16.0</data>
  <data key="d6">WizardLM-13B benefits from the M&lt;sup&gt;3&lt;/sup&gt; method which helps in improving its instruction-following capabilities.&lt;SEP&gt;WizardLM-13B was one of the fine-tuned models evaluated to assess the effectiveness of the M&lt;sup&gt;3&lt;/sup&gt; technique.</data>
  <data key="d7">model evaluation, technique application&lt;SEP&gt;model improvement, instruction-following</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="WizardMath-13B">
  <data key="d5">34.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; is used to improve the performance of the merged model involving WizardMath-13B, enhancing its effectiveness in code generation.&lt;SEP&gt;WizardMath-13B utilizes the M&lt;sup&gt;3&lt;/sup&gt; method to enhance its performance in mathematical reasoning tasks.&lt;SEP&gt;WizardMath-13B was tested in conjunction with M&lt;sup&gt;3&lt;/sup&gt; to evaluate its impact on mathematical reasoning models.</data>
  <data key="d7">model evaluation, technique application&lt;SEP&gt;model improvement, mathematical reasoning&lt;SEP&gt;performance enhancement, model merging</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="llama-2-13b-code-alpaca">
  <data key="d5">16.0</data>
  <data key="d6">llama-2-13b-code-alpaca employs the M&lt;sup&gt;3&lt;/sup&gt; method to optimize its capabilities in code generation tasks.&lt;SEP&gt;llama-2-13b-code-alpaca was included in the experiments to determine the effectiveness of the M&lt;sup&gt;3&lt;/sup&gt; technique in code generation.</data>
  <data key="d7">model evaluation, technique application&lt;SEP&gt;model improvement, code generation</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="LiveBench">
  <data key="d5">18.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; enhances performance in LiveBench evaluations, indicating its effectiveness in various tasks."|&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; was validated through extensive evaluations on LiveBench to check the robustness of merged models.</data>
  <data key="d7">evaluation framework, robustness testing&lt;SEP&gt;performance improvement, evaluation</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="PromptBench">
  <data key="d5">25.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; is validated using PromptBench to assess its effectiveness in improving model robustness.&lt;SEP&gt;The effectiveness of M&lt;sup&gt;3&lt;/sup&gt; was also tested on PromptBench, further assessing its impact on model performance.</data>
  <data key="d7">evaluation framework, robustness testing&lt;SEP&gt;validation, performance assessment</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Yang et al.">
  <data key="d5">9.0</data>
  <data key="d6">Yang et al. contributed to the development of the M&lt;sup&gt;3&lt;/sup&gt; technique, which enhances model merging processes.</data>
  <data key="d7">research contribution, technique development</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Akiba et al.">
  <data key="d5">8.0</data>
  <data key="d6">Akiba et al. are cited regarding advancements in model merging, influencing the M&lt;sup&gt;3&lt;/sup&gt; technique.</data>
  <data key="d7">research contribution, technique development</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Wortsman et al.">
  <data key="d5">8.0</data>
  <data key="d6">Wortsman et al. are referenced for their work on parameter fusion strategies that M&lt;sup&gt;3&lt;/sup&gt; aims to improve upon.</data>
  <data key="d7">research contribution, technique development</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Ilharco et al.">
  <data key="d5">8.0</data>
  <data key="d6">Ilharco et al. are mentioned in the context of limitations that M&lt;sup&gt;3&lt;/sup&gt; seeks to address in model merging.</data>
  <data key="d7">research contribution, technique improvement</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Zhang">
  <data key="d5">9.0</data>
  <data key="d6">Zhang's Mixup technique inspired the M&lt;sup&gt;3&lt;/sup&gt; method, indicating a direct influence on its development.</data>
  <data key="d7">inspiration, technique development</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Zhu et al.">
  <data key="d5">8.0</data>
  <data key="d6">Zhu et al. contributed to the development of M&lt;sup&gt;3&lt;/sup&gt; for enhancing model robustness.</data>
  <data key="d7">research contribution, model robustness</data>
  <data key="d8">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Yu et al.">
  <data key="d5">8.0</data>
  <data key="d6">Yu et al. have investigated the role of LLMs in enhancing model merging capabilities, relating to M&lt;sup&gt;3&lt;/sup&gt;.</data>
  <data key="d7">research exploration, model enhancement</data>
  <data key="d8">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="SFT">
  <data key="d5">8.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; utilizes SFT models to create a merged model by interpolating their parameters.</data>
  <data key="d7">model merging, parameter adjustment</data>
  <data key="d8">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Beta Distribution">
  <data key="d5">9.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; employs the Beta distribution to determine the linear interpolation ratio for merging models.</data>
  <data key="d7">statistical method, interpolation</data>
  <data key="d8">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Vicinal Risk Minimization (VRM)">
  <data key="d5">7.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; extends the VRM principle by applying interpolation in the model parameter space for improved performance.</data>
  <data key="d7">theoretical framework, model enhancement</data>
  <data key="d8">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Task-Specific Fine-Tuned LLMs">
  <data key="d5">9.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; is a method that enhances the performance of task-specific fine-tuned LLMs by merging their parameters through linear interpolation.</data>
  <data key="d7">model merging, performance enhancement</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="θ_{SFT}^{t_1}">
  <data key="d5">8.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; uses θ_{SFT}^{t_1} as one of the parameters for linear interpolation in model training.</data>
  <data key="d7">parameter usage, model training</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="θ_{SFT}^{t_2}">
  <data key="d5">8.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; incorporates θ_{SFT}^{t_2} in the parameter merging process to enhance model performance.</data>
  <data key="d7">parameter usage, model training</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="λ_m">
  <data key="d5">9.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; employs λ_m as the interpolation ratio to effectively combine parameters from different models.</data>
  <data key="d7">parameter combination, model merging</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="P_{ν}(θ_M)">
  <data key="d5">9.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; results in the formation of P_{ν}(θ_M) as the new distribution representing merged model parameters.</data>
  <data key="d7">distribution formation, model merging</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Dirac Delta Function">
  <data key="d5">8.0</data>
  <data key="d6">The Dirac delta function is used in M&lt;sup&gt;3&lt;/sup&gt; to ensure that the interpolated parameters satisfy the linear interpolation rule.</data>
  <data key="d7">mathematical enforcement, model merging</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Neighborhood Distribution">
  <data key="d5">9.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; creates a neighborhood distribution that integrates knowledge from different tasks, enhancing model performance.</data>
  <data key="d7">knowledge integration, model performance</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Decision Boundaries">
  <data key="d5">9.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; aims to create smoother decision boundaries between different tasks, improving task adaptability.</data>
  <data key="d7">task adaptability, performance improvement</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Occam's Razor">
  <data key="d5">8.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; aligns with Occam's Razor by promoting simpler solutions that generalize better across tasks.</data>
  <data key="d7">principle application, generalization</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Task Conflicts">
  <data key="d5">9.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; addresses task conflicts by balancing conflicting parameter values for improved performance.</data>
  <data key="d7">conflict mitigation, performance enhancement</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Experiments">
  <data key="d5">10.0</data>
  <data key="d6">Experiments are conducted to evaluate the effectiveness of the M&lt;sup&gt;3&lt;/sup&gt; method in various tasks.</data>
  <data key="d7">evaluation, method effectiveness</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Average Merging">
  <data key="d5">25.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; is integrated into Average Merging to enhance the performance of merging fine-tuned LLMs."|&lt;SEP&gt;When Average Merging is combined with M&lt;sup&gt;3&lt;/sup&gt;, it contributes to performance improvements in model evaluations.</data>
  <data key="d7">method integration, performance boost&lt;SEP&gt;model merging, performance improvement</data>
  <data key="d8">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Matthews Correlation Coefficient (MCC)">
  <data key="d5">8.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; is assessed using Matthews Correlation Coefficient (MCC) as a performance metric for models on the CoLA dataset."|</data>
  <data key="d7">performance metric, evaluation</data>
  <data key="d8">chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="HumanEval">
  <data key="d5">9.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; shows improvements in performance on HumanEval, linking the merging technique with code generation evaluation."|</data>
  <data key="d7">performance improvement, evaluation</data>
  <data key="d8">chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="MBPP">
  <data key="d5">29.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; is evaluated against MBPP, demonstrating its impact on coding task performance."|&lt;SEP&gt;The M&lt;sup&gt;3&lt;/sup&gt; approach shows a measurable improvement in the pass@1 score on the MBPP benchmark for the merged model.</data>
  <data key="d7">benchmark improvement, evaluation&lt;SEP&gt;performance improvement, evaluation</data>
  <data key="d8">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="llama-2-13b-codealpaca">
  <data key="d5">16.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; addresses the suboptimal results of merging with llama-2-13b-codealpaca, aiming to improve its performance in code generation.</data>
  <data key="d7">performance enhancement, model optimization</data>
  <data key="d8">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="LiveBench-Instruction">
  <data key="d5">14.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; enhances the win rate of the Math &amp; Code model on the LiveBench-Instruction dataset.</data>
  <data key="d7">dataset evaluation, performance improvement</data>
  <data key="d8">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="LiveBench-Coding">
  <data key="d5">14.0</data>
  <data key="d6">The LM &amp; Math model achieves better results on the LiveBench-Coding dataset through the application of M&lt;sup&gt;3&lt;/sup&gt;.</data>
  <data key="d7">dataset evaluation, performance improvement</data>
  <data key="d8">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="LiveBench-TypoFixing">
  <data key="d5">16.0</data>
  <data key="d6">The LM &amp; Code model shows significant accuracy improvements on the LiveBench-TypoFixing dataset when using M&lt;sup&gt;3&lt;/sup&gt;.</data>
  <data key="d7">dataset evaluation, performance improvement</data>
  <data key="d8">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Math &amp; Code">
  <data key="d5">36.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; enhances the performance of the Math &amp; Code model across various tasks and merging strategies.&lt;SEP&gt;The Math &amp; Code model achieves significant performance improvements when merged with M&lt;sup&gt;3&lt;/sup&gt;, demonstrating the effectiveness of this approach.&lt;SEP&gt;The Math &amp; Code model achieves significant performance improvements when merged with M&lt;sup&gt;3&lt;/sup&gt;.</data>
  <data key="d7">model performance, merging technique&lt;SEP&gt;performance enhancement, model improvement</data>
  <data key="d8">chunk-070652f4f36e86620289fcafaaa29599&lt;SEP&gt;chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="LM &amp; Math">
  <data key="d5">18.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; contributes to the performance improvements seen in the LM &amp; Math model on diverse tasks.</data>
  <data key="d7">performance enhancement, model improvement</data>
  <data key="d8">chunk-070652f4f36e86620289fcafaaa29599</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="LM &amp; Code">
  <data key="d5">36.0</data>
  <data key="d6">LM &amp; Code sees notable accuracy increases when enhanced with M&lt;sup&gt;3&lt;/sup&gt; during merging, indicating a strong relationship between the model and the merging technique.&lt;SEP&gt;LM &amp; Code sees notable accuracy increases when enhanced with M&lt;sup&gt;3&lt;/sup&gt; during merging.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; improves the robustness and performance of the LM &amp; Code model on various benchmarks.</data>
  <data key="d7">model performance, merging technique&lt;SEP&gt;performance enhancement, model improvement</data>
  <data key="d8">chunk-070652f4f36e86620289fcafaaa29599&lt;SEP&gt;chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Model Merging Techniques">
  <data key="d5">8.0</data>
  <data key="d6">Various model merging techniques are evaluated alongside M&lt;sup&gt;3&lt;/sup&gt;, indicating that M&lt;sup&gt;3&lt;/sup&gt; offers a novel approach compared to traditional methods.</data>
  <data key="d7">comparison, merging techniques</data>
  <data key="d8">chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Parameter Linear Interpolation Process">
  <data key="d5">9.0</data>
  <data key="d6">The parameter linear interpolation process is a core component of M&lt;sup&gt;3&lt;/sup&gt;, enabling dynamic adjustments that lead to improved model merging outcomes.</data>
  <data key="d7">core technique, model merging</data>
  <data key="d8">chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Performance Metrics">
  <data key="d5">8.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; aims to enhance performance metrics such as accuracy and MCC, demonstrating its impact on model evaluation.</data>
  <data key="d7">evaluation criteria, performance improvement</data>
  <data key="d8">chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Adversarial Robustness">
  <data key="d5">9.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; is designed to improve adversarial robustness in merged models, indicating its relevance to model reliability.</data>
  <data key="d7">model reliability, performance under stress</data>
  <data key="d8">chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="BERTAttack">
  <data key="d5">14.0</data>
  <data key="d6">BERTAttack is utilized in the context of evaluating models that may be merged using the M&lt;sup&gt;3&lt;/sup&gt; technique, highlighting its relevance in adversarial robustness studies.</data>
  <data key="d7">adversarial evaluation, model merging</data>
  <data key="d8">chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="WizardLM-13B" target="AlpacaEval">
  <data key="d5">7.0</data>
  <data key="d6">AlpacaEval is used to evaluate the performance of the WizardLM-13B model in instruction-following tasks.</data>
  <data key="d7">evaluation benchmark, instruction-following</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="WizardLM-13B" target="Instruction Following">
  <data key="d5">9.0</data>
  <data key="d6">The WizardLM-13B model is specifically designed for instruction following tasks, showcasing its specialized capabilities.</data>
  <data key="d7">task specialization, model design</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="WizardLM-13B" target="LiveBench-Coding">
  <data key="d5">8.0</data>
  <data key="d6">WizardLM-13B was evaluated on the LiveBench-Coding event, showcasing its performance in coding tasks.</data>
  <data key="d7">performance evaluation, coding tasks</data>
  <data key="d8">chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="WizardLM-13B" target="Pass@1">
  <data key="d5">9.0</data>
  <data key="d6">WizardLM-13B's performance on LiveBench-Coding is evaluated using the Pass@1 metric to determine its accuracy.</data>
  <data key="d7">evaluation metric, performance assessment</data>
  <data key="d8">chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="WizardLM-13B" target="Evol-Instruct">
  <data key="d5">9.0</data>
  <data key="d6">Evol-Instruct is a method used in the development of WizardLM-13B to generate effective instruction data.</data>
  <data key="d7">method application, model improvement</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="WizardMath-13B" target="GSM8K">
  <data key="d5">16.0</data>
  <data key="d6">GSM8K is a dataset used to train WizardMath-13B for mathematical reasoning tasks.&lt;SEP&gt;GSM8K serves as a testing dataset for evaluating the performance of the WizardMath-13B model in mathematical reasoning tasks.</data>
  <data key="d7">dataset utilization, model training&lt;SEP&gt;evaluation dataset, mathematical reasoning</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="WizardMath-13B" target="MATH">
  <data key="d5">16.0</data>
  <data key="d6">MATH is a dataset utilized for enhancing the capabilities of WizardMath-13B in mathematical reasoning.&lt;SEP&gt;MATH is utilized to assess the capabilities of the WizardMath-13B model in mathematical reasoning tasks.</data>
  <data key="d7">dataset utilization, model training&lt;SEP&gt;evaluation dataset, mathematical reasoning</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="WizardMath-13B" target="Mathematical Reasoning">
  <data key="d5">9.0</data>
  <data key="d6">The WizardMath-13B model is specifically fine-tuned for mathematical reasoning tasks, highlighting its focus area.</data>
  <data key="d7">task specialization, model design</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="WizardMath-13B" target="LiveBench-TypoFixing">
  <data key="d5">9.0</data>
  <data key="d6">WizardMath-13B demonstrated a 4% enhancement in accuracy during the LiveBench-TypoFixing event.</data>
  <data key="d7">performance improvement, typo correction</data>
  <data key="d8">chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="WizardMath-13B" target="Accuracy Improvement">
  <data key="d5">8.0</data>
  <data key="d6">WizardMath-13B achieved an accuracy improvement of 4% in LiveBench-TypoFixing, indicating its enhanced performance.</data>
  <data key="d7">performance enhancement, accuracy</data>
  <data key="d8">chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="llama-2-13b-code-alpaca" target="HumanEval">
  <data key="d5">7.0</data>
  <data key="d6">HumanEval is a benchmark used to evaluate the code generation performance of the llama-2-13b-code-alpaca model.</data>
  <data key="d7">evaluation benchmark, code generation</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="llama-2-13b-code-alpaca" target="MBPP">
  <data key="d5">7.0</data>
  <data key="d6">MBPP is a benchmark that assesses the performance of the llama-2-13b-code-alpaca model in code generation tasks.</data>
  <data key="d7">evaluation benchmark, code generation</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="llama-2-13b-code-alpaca" target="Code Generation">
  <data key="d5">9.0</data>
  <data key="d6">The llama-2-13b-code-alpaca model is tailored for code generation tasks, demonstrating its specialized function.</data>
  <data key="d7">task specialization, model design</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="LiveBench" target="White et al.">
  <data key="d5">9.0</data>
  <data key="d6">White et al. conducted evaluations on LiveBench to test the robustness of models, including those improved by M&lt;sup&gt;3&lt;/sup&gt;.</data>
  <data key="d7">evaluation, model robustness</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="LiveBench" target="Appendix A.1">
  <data key="d5">8.0</data>
  <data key="d6">LiveBench is elaborated upon in Appendix A.1, which provides details on its evaluation methods and frameworks."|</data>
  <data key="d7">evaluation framework, documentation</data>
  <data key="d8">chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="LiveBench" target="Out-of-Distribution Dataset Selection">
  <data key="d5">9.0</data>
  <data key="d6">Out-of-Distribution Dataset Selection is a crucial part of LiveBench's evaluation framework for assessing model robustness.</data>
  <data key="d7">evaluation framework, model testing</data>
  <data key="d8">chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="LiveBench" target="Instruction Following">
  <data key="d5">8.0</data>
  <data key="d6">Instruction Following is a task category featured in LiveBench to evaluate the ability of models to follow instructions.</data>
  <data key="d7">task evaluation, model capability</data>
  <data key="d8">chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="LiveBench" target="Coding">
  <data key="d5">8.0</data>
  <data key="d6">Coding is another task category in LiveBench that tests the coding abilities of language models.</data>
  <data key="d7">task evaluation, model capability</data>
  <data key="d8">chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="LiveBench" target="Language Comprehension">
  <data key="d5">8.0</data>
  <data key="d6">Language Comprehension is a task category in LiveBench focusing on a model's understanding of language intricacies.</data>
  <data key="d7">task evaluation, model capability</data>
  <data key="d8">chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="PromptBench" target="Zhu et al.">
  <data key="d5">17.0</data>
  <data key="d6">Zhu et al. developed the Adversarial Prompt Attacks module within PromptBench, linking their research directly to the evaluation of LLMs."|&lt;SEP&gt;Zhu et al. evaluated the effectiveness of M&lt;sup&gt;3&lt;/sup&gt; using PromptBench, assessing model performance.</data>
  <data key="d7">development, evaluation&lt;SEP&gt;evaluation, model effectiveness</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="PromptBench" target="Adversarial Prompt Attacks">
  <data key="d5">9.0</data>
  <data key="d6">PromptBench includes an Adversarial Prompt Attacks module to evaluate the robustness of LLMs against adversarial prompts.</data>
  <data key="d7">robustness evaluation, testing framework</data>
  <data key="d8">chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="PromptBench" target="Adversarial Robustness Evaluation">
  <data key="d5">9.0</data>
  <data key="d6">Adversarial Robustness Evaluation is conducted using the PromptBench framework to assess how models handle adversarial prompts.</data>
  <data key="d7">evaluation method, model robustness</data>
  <data key="d8">chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Ding et al." target="Brown et al.">
  <data key="d5">6.0</data>
  <data key="d6">Ding et al. and Brown et al. are both referenced in discussions regarding model merging and computational resources.</data>
  <data key="d7">research collaboration, citation</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Xia et al." target="Chang et al.">
  <data key="d5">6.0</data>
  <data key="d6">Xia et al. and Chang et al. are both cited in the context of advancements in model merging techniques.</data>
  <data key="d7">research collaboration, citation</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="White et al." target="Austin et al.">
  <data key="d5">7.0</data>
  <data key="d6">Austin et al. and White et al. are both involved in the evaluation of LLMs and their datasets, indicating collaboration or related research."|</data>
  <data key="d7">research collaboration, evaluation</data>
  <data key="d8">chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Fisher Merging" target="Matena and Raffel">
  <data key="d5">8.0</data>
  <data key="d6">Matena and Raffel's work focuses on Fisher Merging, which is a specific technique within model merging.</data>
  <data key="d7">specific technique, research focus</data>
  <data key="d8">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Mixup" target="Empirical Risk Minimization (ERM)">
  <data key="d5">18.0</data>
  <data key="d6">Mixup serves as an enhancement over traditional ERM by providing a new way to train models through virtual examples.</data>
  <data key="d7">training enhancement, model generalization</data>
  <data key="d8">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Mixup" target="Vicinal Risk Minimization (VRM)">
  <data key="d5">16.0</data>
  <data key="d6">Mixup is rooted in the principles of VRM, aiming to improve generalization in machine learning models.</data>
  <data key="d7">theoretical foundation, model improvement</data>
  <data key="d8">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Mixup" target="LLMs">
  <data key="d5">9.0</data>
  <data key="d6">Mixup can be applied to LLMs to improve their robustness and performance by augmenting the training dataset.</data>
  <data key="d7">data augmentation, model training</data>
  <data key="d8">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Mixup Model Merge" target="Fine-Tuned LLMs">
  <data key="d5">8.0</data>
  <data key="d6">Mixup Model Merge (M&lt;sup&gt;3&lt;/sup&gt;) utilizes fine-tuned LLMs as the basis for its parameter merging process.</data>
  <data key="d7">model merging, machine learning</data>
  <data key="d8">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Mixup Model Merge" target="Average Merging">
  <data key="d5">7.0</data>
  <data key="d6">Mixup Model Merge incorporates concepts from Average Merging to enhance its model fusion approach.</data>
  <data key="d7">model merging techniques, innovation</data>
  <data key="d8">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Fine-Tuned LLMs" target="Llama 2">
  <data key="d5">9.0</data>
  <data key="d6">Fine-tuned LLMs often utilize Llama 2 as their underlying architecture to improve task performance.</data>
  <data key="d7">model architecture, performance enhancement</data>
  <data key="d8">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Beta Distribution" target="Hyperparameter">
  <data key="d5">10.0</data>
  <data key="d6">The interpolation ratio in Mixup is determined by a hyperparameter sampled from a Beta distribution, influencing the model merging process.</data>
  <data key="d7">statistical modeling, machine learning</data>
  <data key="d8">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Llama 2" target="Instruction-following llama model">
  <data key="d5">18.0</data>
  <data key="d6">Llama 2 is part of the advancements in instruction-following models, indicating ongoing research in AI.</data>
  <data key="d7">AI research, model development</data>
  <data key="d8">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Interpolation Ratio" target="Synthetic Sample">
  <data key="d5">10.0</data>
  <data key="d6">The Interpolation Ratio directly influences how Synthetic Samples are generated during the Mixup process.</data>
  <data key="d7">data generation, sample creation</data>
  <data key="d8">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Parameters" target="Fine-Tuned Models">
  <data key="d5">9.0</data>
  <data key="d6">The Parameters of Fine-Tuned Models are adjusted to enhance performance on specific tasks after initial training.</data>
  <data key="d7">model training, performance optimization</data>
  <data key="d8">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Fine-Tuned Models" target="Pre-Trained Backbone">
  <data key="d5">10.0</data>
  <data key="d6">Fine-Tuned Models are built upon a Pre-Trained Backbone, leveraging its learned features for improved task performance.</data>
  <data key="d7">model architecture, training efficiency</data>
  <data key="d8">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="$\theta^t_{SFT}$" target="$\theta_{PRE}$">
  <data key="d5">9.0</data>
  <data key="d6">$\theta^t_{SFT}$ is derived from $\theta_{PRE}$ through the process of Supervised Fine-Tuning, indicating a progression in model training.</data>
  <data key="d7">model training, parameter evolution</data>
  <data key="d8">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="$\theta^t_{SFT}$" target="$\delta^t$">
  <data key="d5">8.0</data>
  <data key="d6">$\delta^t$ quantifies the change in parameters from $\theta_{PRE}$ to $\theta^t_{SFT}$, illustrating the effect of fine-tuning.</data>
  <data key="d7">parameter change, fine-tuning effect</data>
  <data key="d8">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="$\lambda_m$" target="$\alpha$">
  <data key="d5">7.0</data>
  <data key="d6">$\lambda_m$ is sampled from a Beta distribution where $\alpha$ influences the shape and behavior of the distribution.</data>
  <data key="d7">sampling method, distribution characteristics</data>
  <data key="d8">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="$\alpha$" target="Figure 3">
  <data key="d5">6.0</data>
  <data key="d6">Figure 3 provides a visual representation of how different values of $\alpha$ affect the Beta distribution used for $\lambda_m$.</data>
  <data key="d7">visualization, statistical analysis</data>
  <data key="d8">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="AlpacaEval" target="GSM8K">
  <data key="d5">23.0</data>
  <data key="d6">AlpacaEval is used to evaluate models on the GSM8K dataset for instruction-following tasks.&lt;SEP&gt;AlpacaEval is utilized to evaluate models on the GSM8K dataset for instruction-following tasks.".&lt;SEP&gt;GSM8K is used to evaluate zero-shot accuracy in AlpacaEval, indicating a connection between the two benchmarks."|</data>
  <data key="d7">benchmark relationship, evaluation&lt;SEP&gt;evaluation metric, dataset usage</data>
  <data key="d8">chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-9fbd0c0622e4d5fb84be83781e1c4521</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="GSM8K" target="MATH">
  <data key="d5">18.0</data>
  <data key="d6">Both GSM8K and MATH datasets are used to evaluate mathematical reasoning abilities of language models.&lt;SEP&gt;Both GSM8K and MATH datasets are used to evaluate mathematical reasoning abilities of language models.".</data>
  <data key="d7">dataset comparison, evaluation</data>
  <data key="d8">chunk-9fbd0c0622e4d5fb84be83781e1c4521</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="HumanEval" target="MBPP">
  <data key="d5">16.0</data>
  <data key="d6">HumanEval and MBPP are datasets used to assess different aspects of code generation models.&lt;SEP&gt;HumanEval and MBPP are datasets used to assess different aspects of code generation models.".</data>
  <data key="d7">dataset comparison, evaluation</data>
  <data key="d8">chunk-9fbd0c0622e4d5fb84be83781e1c4521</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="MBPP" target="Math &amp; Code">
  <data key="d5">16.0</data>
  <data key="d6">Math &amp; Code achieves specific performance scores on the MBPP dataset when merged with DARE, showcasing the effectiveness of combining different techniques.&lt;SEP&gt;Math &amp; Code achieves specific performance scores on the MBPP dataset when merged with DARE.</data>
  <data key="d7">dataset performance, model evaluation</data>
  <data key="d8">chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="SST2" target="Math &amp; Code">
  <data key="d5">22.0</data>
  <data key="d6">Math &amp; Code was evaluated using the SST2 dataset to measure its performance in sentiment analysis.&lt;SEP&gt;The performance of Math &amp; Code is evaluated using the SST2 dataset, showing improvements with M&lt;sup&gt;3&lt;/sup&gt;, which highlights the practical applications of the merging technique.&lt;SEP&gt;The performance of Math &amp; Code is evaluated using the SST2 dataset, showing improvements with M&lt;sup&gt;3&lt;/sup&gt;.</data>
  <data key="d7">dataset evaluation, performance testing&lt;SEP&gt;dataset evaluation, sentiment analysis</data>
  <data key="d8">chunk-407b385e429f3495cdcf3f23f7e80f73&lt;SEP&gt;chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="SST2" target="LM &amp; Math">
  <data key="d5">8.0</data>
  <data key="d6">LM &amp; Math was tested on the SST2 dataset to analyze its effectiveness in sentiment classification.</data>
  <data key="d7">dataset evaluation, performance testing</data>
  <data key="d8">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="SST2" target="LM &amp; Code">
  <data key="d5">8.0</data>
  <data key="d6">LM &amp; Code was also evaluated on the SST2 dataset to determine its adversarial robustness.</data>
  <data key="d7">dataset evaluation, performance testing</data>
  <data key="d8">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="SST2" target="Sentiment Analysis Dataset">
  <data key="d5">9.0</data>
  <data key="d6">SST2 is a specific sentiment analysis dataset used to evaluate models' abilities to classify sentiments in text.</data>
  <data key="d7">dataset classification, evaluation</data>
  <data key="d8">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="SST2" target="CoLA">
  <data key="d5">16.0</data>
  <data key="d6">CoLA and SST2 are both datasets used in natural language processing for different purposes, with CoLA focusing on linguistic acceptability and SST2 on sentiment analysis.</data>
  <data key="d7">dataset comparison, NLP evaluation</data>
  <data key="d8">chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="CoLA" target="Math &amp; Code">
  <data key="d5">22.0</data>
  <data key="d6">Math &amp; Code was tested on the CoLA dataset to evaluate its grammar correctness capability.&lt;SEP&gt;Math &amp; Code's performance metrics are assessed using the CoLA dataset, emphasizing the model's capabilities in grammar correctness when enhanced with M&lt;sup&gt;3&lt;/sup&gt;.&lt;SEP&gt;Math &amp; Code's performance metrics are assessed using the CoLA dataset, highlighting improvements with M&lt;sup&gt;3&lt;/sup&gt;.</data>
  <data key="d7">dataset evaluation, grammar correctness&lt;SEP&gt;dataset evaluation, performance testing</data>
  <data key="d8">chunk-407b385e429f3495cdcf3f23f7e80f73&lt;SEP&gt;chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="CoLA" target="LM &amp; Math">
  <data key="d5">8.0</data>
  <data key="d6">LM &amp; Math was assessed on the CoLA dataset to determine its performance in grammar correctness.</data>
  <data key="d7">dataset evaluation, performance testing</data>
  <data key="d8">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="CoLA" target="LM &amp; Code">
  <data key="d5">8.0</data>
  <data key="d6">LM &amp; Code was evaluated on the CoLA dataset to assess its grammar correctness performance.</data>
  <data key="d7">dataset evaluation, performance testing</data>
  <data key="d8">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="CoLA" target="Grammar Correctness Dataset">
  <data key="d5">9.0</data>
  <data key="d6">CoLA serves as a grammar correctness dataset that assesses the grammaticality of sentences, used in model evaluations.</data>
  <data key="d7">dataset classification, evaluation</data>
  <data key="d8">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Adversarial Prompt Attacks" target="DeepWordBug">
  <data key="d5">16.0</data>
  <data key="d6">DeepWordBug is one of the attack methods used within the Adversarial Prompt Attacks module to assess LLM robustness."|&lt;SEP&gt;DeepWordBug is one of the methods employed in Adversarial Prompt Attacks to test language model robustness.</data>
  <data key="d7">attack method, evaluation&lt;SEP&gt;evaluation method, adversarial testing</data>
  <data key="d8">chunk-b650b88d4bebbc45d20b8908b045eff8&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Adversarial Prompt Attacks" target="BERTAttack">
  <data key="d5">16.0</data>
  <data key="d6">BERTAttack is another attack method featured in the Adversarial Prompt Attacks module for evaluating language model resilience."|&lt;SEP&gt;BERTAttack is another method used to evaluate language models within the Adversarial Prompt Attacks framework.</data>
  <data key="d7">attack method, evaluation&lt;SEP&gt;evaluation method, adversarial testing</data>
  <data key="d8">chunk-b650b88d4bebbc45d20b8908b045eff8&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Adversarial Prompt Attacks" target="StressTest">
  <data key="d5">16.0</data>
  <data key="d6">StressTest is included in the Adversarial Prompt Attacks module as a method to evaluate LLMs against adversarial prompts."|&lt;SEP&gt;StressTest is included in the Adversarial Prompt Attacks to assess how models handle distractions and maintain performance.</data>
  <data key="d7">attack method, evaluation&lt;SEP&gt;evaluation method, distraction testing</data>
  <data key="d8">chunk-b650b88d4bebbc45d20b8908b045eff8&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="DeepWordBug" target="Math &amp; Code">
  <data key="d5">23.0</data>
  <data key="d6">DeepWordBug is used to evaluate the adversarial robustness of the Math &amp; Code model, assessing its vulnerability to attacks.&lt;SEP&gt;DeepWordBug was applied to evaluate the adversarial robustness of the Math &amp; Code model on the SST2 and CoLA datasets.</data>
  <data key="d7">adversarial evaluation, model robustness&lt;SEP&gt;adversarial testing, robustness evaluation</data>
  <data key="d8">chunk-070652f4f36e86620289fcafaaa29599&lt;SEP&gt;chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="DeepWordBug" target="LM &amp; Math">
  <data key="d5">9.0</data>
  <data key="d6">DeepWordBug was used to assess the robustness of the LM &amp; Math model against adversarial attacks on the datasets.</data>
  <data key="d7">adversarial testing, robustness evaluation</data>
  <data key="d8">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="DeepWordBug" target="LM &amp; Code">
  <data key="d5">9.0</data>
  <data key="d6">DeepWordBug was utilized to evaluate the LM &amp; Code model's performance under adversarial conditions on the datasets.</data>
  <data key="d7">adversarial testing, robustness evaluation</data>
  <data key="d8">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="BERTAttack" target="LM &amp; Math">
  <data key="d5">14.0</data>
  <data key="d6">BERTAttack assesses the robustness of the LM &amp; Math model, helping to identify weaknesses against adversarial inputs.</data>
  <data key="d7">adversarial evaluation, model robustness</data>
  <data key="d8">chunk-070652f4f36e86620289fcafaaa29599</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="BERTAttack" target="Performance Drop Rate (PDR)">
  <data key="d5">18.0</data>
  <data key="d6">Performance Drop Rate (PDR) is a metric evaluated through the BERTAttack method to assess model robustness against adversarial attacks.</data>
  <data key="d7">evaluation metric, adversarial robustness</data>
  <data key="d8">chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="StressTest" target="LM &amp; Code">
  <data key="d5">14.0</data>
  <data key="d6">StressTest is employed to evaluate the adversarial performance of the LM &amp; Code model, measuring its robustness under attack.</data>
  <data key="d7">adversarial evaluation, model robustness</data>
  <data key="d8">chunk-070652f4f36e86620289fcafaaa29599</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="llama-2-13b-codealpaca" target="LiveBench-Instruction">
  <data key="d5">7.0</data>
  <data key="d6">llama-2-13b-codealpaca showed improved performance in the LiveBench-Instruction event through the application of TIES-Merging.</data>
  <data key="d7">performance enhancement, instruction following</data>
  <data key="d8">chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="llama-2-13b-codealpaca" target="Win Rate">
  <data key="d5">7.0</data>
  <data key="d6">The application of TIES-Merging led to a 1.1% improvement in the win rate of llama-2-13b-codealpaca on LiveBench-Instruction.</data>
  <data key="d7">performance improvement, competitive success</data>
  <data key="d8">chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="LiveBench-Instruction" target="Win Rate">
  <data key="d5">9.0</data>
  <data key="d6">The win rate of models is determined based on their performance during the LiveBench-Instruction event, reflecting their ability to follow instructions.</data>
  <data key="d7">event evaluation, performance metrics</data>
  <data key="d8">chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="LiveBench-Instruction" target="LM &amp; Math">
  <data key="d5">16.0</data>
  <data key="d6">The LM &amp; Math model achieves better performance metrics on the LiveBench-Instruction benchmark, showcasing its capabilities in instruction tasks.</data>
  <data key="d7">benchmark performance, model effectiveness</data>
  <data key="d8">chunk-070652f4f36e86620289fcafaaa29599</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="LiveBench-Coding" target="Accuracy Improvement">
  <data key="d5">9.0</data>
  <data key="d6">The accuracy improvement of models is measured during the LiveBench-Coding event, highlighting their effectiveness in coding tasks.</data>
  <data key="d7">event evaluation, performance metrics</data>
  <data key="d8">chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="LiveBench-Coding" target="LM &amp; Code">
  <data key="d5">16.0</data>
  <data key="d6">The LM &amp; Code model demonstrates enhanced performance on the LiveBench-Coding benchmark, reflecting its coding capabilities.</data>
  <data key="d7">benchmark performance, model effectiveness</data>
  <data key="d8">chunk-070652f4f36e86620289fcafaaa29599</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="LiveBench-TypoFixing" target="Accuracy Improvement">
  <data key="d5">8.0</data>
  <data key="d6">The results from LiveBench-TypoFixing provide insights into the accuracy improvements of models in typo correction tasks.</data>
  <data key="d7">event evaluation, performance metrics</data>
  <data key="d8">chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="LiveBench-TypoFixing" target="Math &amp; Code">
  <data key="d5">16.0</data>
  <data key="d6">The Math &amp; Code model shows improved accuracy on the LiveBench-TypoFixing benchmark, indicating its effectiveness in typo fixing tasks.</data>
  <data key="d7">benchmark performance, model effectiveness</data>
  <data key="d8">chunk-070652f4f36e86620289fcafaaa29599</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="OOD Datasets" target="Model Robustness">
  <data key="d5">9.0</data>
  <data key="d6">Evaluating models on OOD datasets is essential for assessing their robustness and generalization capabilities.</data>
  <data key="d7">robustness evaluation, generalization</data>
  <data key="d8">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Model Robustness" target="Merging Ratio">
  <data key="d5">8.0</data>
  <data key="d6">The merging ratio impacts the robustness of the model, influencing how well it performs on out-of-distribution data.</data>
  <data key="d7">performance factors, model evaluation</data>
  <data key="d8">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Math &amp; Code" target="Performance Drop Rate (PDR)">
  <data key="d5">8.0</data>
  <data key="d6">PDR is a metric used to evaluate the robustness of the Math &amp; Code model under adversarial conditions.</data>
  <data key="d7">metric evaluation, model robustness</data>
  <data key="d8">chunk-070652f4f36e86620289fcafaaa29599</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="LM &amp; Math" target="Performance Drop Rate (PDR)">
  <data key="d5">8.0</data>
  <data key="d6">PDR is a metric used to assess the robustness of the LM &amp; Math model when subjected to adversarial attacks.</data>
  <data key="d7">metric evaluation, model robustness</data>
  <data key="d8">chunk-070652f4f36e86620289fcafaaa29599</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="LM &amp; Code" target="Performance Drop Rate (PDR)">
  <data key="d5">8.0</data>
  <data key="d6">PDR is a critical measure for evaluating the adversarial robustness of the LM &amp; Code model.</data>
  <data key="d7">metric evaluation, model robustness</data>
  <data key="d8">chunk-070652f4f36e86620289fcafaaa29599</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Performance Drop Rate (PDR)" target="Metric_no attack">
  <data key="d5">8.0</data>
  <data key="d6">Metric_no attack is used in the calculation of Performance Drop Rate (PDR) to evaluate model performance without attacks.</data>
  <data key="d7">performance evaluation, metric calculation</data>
  <data key="d8">chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Performance Drop Rate (PDR)" target="Metric_attack">
  <data key="d5">8.0</data>
  <data key="d6">Metric_attack is considered in the calculation of Performance Drop Rate (PDR) to assess the impact of attacks on model performance.</data>
  <data key="d7">performance evaluation, metric calculation</data>
  <data key="d8">chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Performance Drop Rate (PDR)" target="adversarial robustness">
  <data key="d5">9.0</data>
  <data key="d6">Adversarial robustness is assessed through the Performance Drop Rate (PDR), indicating how well a model withstands attacks.</data>
  <data key="d7">robustness assessment, adversarial evaluation</data>
  <data key="d8">chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Adversarial Robustness" target="Evaluation">
  <data key="d5">9.0</data>
  <data key="d6">Adversarial Robustness is evaluated through systematic assessments that determine how well a model performs under adversarial conditions.</data>
  <data key="d7">model evaluation, robustness</data>
  <data key="d8">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Jacob Austin" target="Augustus Odena">
  <data key="d5">6.0</data>
  <data key="d6">Jacob Austin and Augustus Odena are co-authors of a paper on program synthesis with large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Augustus Odena" target="Maxwell Nye">
  <data key="d5">6.0</data>
  <data key="d6">Augustus Odena and Maxwell Nye are co-authors of a paper on program synthesis with large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Maxwell Nye" target="Maarten Bosma">
  <data key="d5">6.0</data>
  <data key="d6">Maxwell Nye and Maarten Bosma are co-authors of a paper on program synthesis with large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Maarten Bosma" target="Henryk Michalewski">
  <data key="d5">6.0</data>
  <data key="d6">Maarten Bosma and Henryk Michalewski are co-authors of a paper on program synthesis with large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Henryk Michalewski" target="David Dohan">
  <data key="d5">6.0</data>
  <data key="d6">Henryk Michalewski and David Dohan are co-authors of a paper on program synthesis with large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="David Dohan" target="Ellen Jiang">
  <data key="d5">6.0</data>
  <data key="d6">David Dohan and Ellen Jiang are co-authors of a paper on program synthesis with large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Ellen Jiang" target="Carrie Cai">
  <data key="d5">6.0</data>
  <data key="d6">Ellen Jiang and Carrie Cai are co-authors of a paper on program synthesis with large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Carrie Cai" target="Michael Terry">
  <data key="d5">6.0</data>
  <data key="d6">Carrie Cai and Michael Terry are co-authors of a paper on program synthesis with large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Michael Terry" target="Quoc Le">
  <data key="d5">6.0</data>
  <data key="d6">Michael Terry and Quoc Le are co-authors of a paper on program synthesis with large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Yupeng Chang" target="Xu Wang">
  <data key="d5">6.0</data>
  <data key="d6">Yupeng Chang and Xu Wang are co-authors of a paper on evaluation of large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Edward Beeching" target="Clémentine Fourrier">
  <data key="d5">6.0</data>
  <data key="d6">Edward Beeching and Clémentine Fourrier are co-authors of a paper on the Open LLM leaderboard.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Clémentine Fourrier" target="Nathan Habib">
  <data key="d5">6.0</data>
  <data key="d6">Clémentine Fourrier and Nathan Habib are co-authors of a paper on the Open LLM leaderboard.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Nathan Habib" target="Sheon Han">
  <data key="d5">6.0</data>
  <data key="d6">Nathan Habib and Sheon Han are co-authors of a paper on the Open LLM leaderboard.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Sheon Han" target="Nathan Lambert">
  <data key="d5">6.0</data>
  <data key="d6">Sheon Han and Nathan Lambert are co-authors of a paper on the Open LLM leaderboard.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Nathan Lambert" target="Nazneen Rajani">
  <data key="d5">6.0</data>
  <data key="d6">Nathan Lambert and Nazneen Rajani are co-authors of a paper on the Open LLM leaderboard.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Nazneen Rajani" target="Omar Sanseviero">
  <data key="d5">6.0</data>
  <data key="d6">Nazneen Rajani and Omar Sanseviero are co-authors of a paper on the Open LLM leaderboard.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Omar Sanseviero" target="Lewis Tunstall">
  <data key="d5">6.0</data>
  <data key="d6">Omar Sanseviero and Lewis Tunstall are co-authors of a paper on the Open LLM leaderboard.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Lewis Tunstall" target="Thomas Wolf">
  <data key="d5">6.0</data>
  <data key="d6">Lewis Tunstall and Thomas Wolf are co-authors of a paper on the Open LLM leaderboard.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Tom Brown" target="Benjamin Mann&quot;&lt;||&gt;&quot;Tom Brown and Benjamin Mann are co-authors of a paper discussing language models as few-shot learners.">
  <data key="d5">6.0</data>
  <data key="d6">co-authorship, research</data>
  <data key="d7">6</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Benjamin Mann" target="Nick Ryder">
  <data key="d5">6.0</data>
  <data key="d6">Benjamin Mann and Nick Ryder are co-authors of a paper discussing language models as few-shot learners.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Nick Ryder" target="Melanie Subbiah">
  <data key="d5">6.0</data>
  <data key="d6">Nick Ryder and Melanie Subbiah are co-authors of a paper discussing language models as few-shot learners.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Melanie Subbiah" target="Jared D Kaplan&quot;&lt;||&quot;Melanie Subbiah and Jared D Kaplan are co-authors of a paper discussing language models as few-shot learners.">
  <data key="d5">6.0</data>
  <data key="d6">co-authorship, research</data>
  <data key="d7">6</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Jared D Kaplan" target="Prafulla Dhariwal">
  <data key="d5">6.0</data>
  <data key="d6">Jared D Kaplan and Prafulla Dhariwal are co-authors of a paper discussing language models as few-shot learners.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Prafulla Dhariwal" target="Arvind Neelakantan">
  <data key="d5">6.0</data>
  <data key="d6">Prafulla Dhariwal and Arvind Neelakantan are co-authors of a paper discussing language models as few-shot learners.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Arvind Neelakantan" target="Pranav Shyam">
  <data key="d5">6.0</data>
  <data key="d6">Arvind Neelakantan and Pranav Shyam are co-authors of a paper discussing language models as few-shot learners.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Pranav Shyam" target="Girish Sastry">
  <data key="d5">6.0</data>
  <data key="d6">Pranav Shyam and Girish Sastry are co-authors of a paper discussing language models as few-shot learners.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Girish Sastry" target="Amanda Askell">
  <data key="d5">6.0</data>
  <data key="d6">Girish Sastry and Amanda Askell are co-authors of a paper discussing language models as few-shot learners.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Xu Wang" target="Jindong Wang">
  <data key="d5">6.0</data>
  <data key="d6">Xu Wang and Jindong Wang are co-authors of a paper on evaluation of large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Jindong Wang" target="Linyi Yang">
  <data key="d5">6.0</data>
  <data key="d6">Jindong Wang and Linyi Yang are co-authors of a paper on evaluation of large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Linyi Yang" target="Kaijie Zhu">
  <data key="d5">6.0</data>
  <data key="d6">Linyi Yang and Kaijie Zhu are co-authors of a paper on evaluation of large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Kaijie Zhu" target="Hao Chen">
  <data key="d5">6.0</data>
  <data key="d6">Kaijie Zhu and Hao Chen are co-authors of a paper on evaluation of large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Hao Chen" target="Xiaoyuan Yi">
  <data key="d5">6.0</data>
  <data key="d6">Hao Chen and Xiaoyuan Yi are co-authors of a paper on evaluation of large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Xiaoyuan Yi" target="Cunxiang Wang">
  <data key="d5">6.0</data>
  <data key="d6">Xiaoyuan Yi and Cunxiang Wang are co-authors of a paper on evaluation of large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Cunxiang Wang" target="Yidong Wang">
  <data key="d5">6.0</data>
  <data key="d6">Cunxiang Wang and Yidong Wang are co-authors of a paper on evaluation of large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Karl Cobbe" target="Vineet Kosaraju">
  <data key="d5">6.0</data>
  <data key="d6">Karl Cobbe and Vineet Kosaraju are co-authors of a paper on training verifiers for math word problems.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Vineet Kosaraju" target="Mohammad Bavarian">
  <data key="d5">6.0</data>
  <data key="d6">Vineet Kosaraju and Mohammad Bavarian are co-authors of a paper on training verifiers for math word problems.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Mohammad Bavarian" target="Mark Chen">
  <data key="d5">6.0</data>
  <data key="d6">Mohammad Bavarian and Mark Chen are co-authors of a paper on training verifiers for math word problems.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Mark Chen" target="Heewoo Jun">
  <data key="d5">6.0</data>
  <data key="d6">Mark Chen and Heewoo Jun are co-authors of a paper on training verifiers for math word problems.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Heewoo Jun" target="Lukasz Kaiser">
  <data key="d5">6.0</data>
  <data key="d6">Heewoo Jun and Lukasz Kaiser are co-authors of a paper on training verifiers for math word problems.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Lukasz Kaiser" target="Matthias Plappert">
  <data key="d5">6.0</data>
  <data key="d6">Lukasz Kaiser and Matthias Plappert are co-authors of a paper on training verifiers for math word problems.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Matthias Plappert" target="Jerry Tworek">
  <data key="d5">6.0</data>
  <data key="d6">Matthias Plappert and Jerry Tworek are co-authors of a paper on training verifiers for math word problems.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Jerry Tworek" target="Jacob Hilton">
  <data key="d5">6.0</data>
  <data key="d6">Jerry Tworek and Jacob Hilton are co-authors of a paper on training verifiers for math word problems.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Jacob Hilton" target="Reiichiro Nakano">
  <data key="d5">6.0</data>
  <data key="d6">Jacob Hilton and Reiichiro Nakano are co-authors of a paper on training verifiers for math word problems.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Ronald A Fisher" target="Philosophical Transactions of the Royal">
  <data key="d5">17.0</data>
  <data key="d6">Ronald A Fisher's work on theoretical statistics was published in Philosophical Transactions of the Royal, establishing foundational concepts in statistics.&lt;SEP&gt;Ronald A Fisher's work on theoretical statistics was published in Philosophical Transactions of the Royal.</data>
  <data key="d7">publication, historical significance</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Ronald A Fisher" target="Philosophical Transactions of the Royal Society of London">
  <data key="d5">8.0</data>
  <data key="d6">Ronald A Fisher's work was published in the Philosophical Transactions, showcasing his contributions to statistics.</data>
  <data key="d7">publication, academic contribution</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Reinforcement Learning with Human Feedback" target="arXiv preprint arXiv:2403.13187">
  <data key="d5">8.0</data>
  <data key="d6">The paper discusses the application of Reinforcement Learning with Human Feedback in optimizing model merging recipes.</data>
  <data key="d7">research methodology, model optimization</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="arXiv preprint arXiv:2403.13187" target="UCI Machine Learning Repository">
  <data key="d5">7.0</data>
  <data key="d6">The paper utilizes datasets from the UCI Machine Learning Repository for its experiments and evaluations.</data>
  <data key="d7">data source, research application</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="arXiv preprint arXiv:2403.13187" target="ACM Transactions on Intelligent Systems and Technology">
  <data key="d5">6.0</data>
  <data key="d6">The research on model merging recipes contributes to the body of knowledge published in ACM Transactions on Intelligent Systems and Technology.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="UCI Machine Learning Repository" target="arXiv preprint arXiv:2108.07732">
  <data key="d5">7.0</data>
  <data key="d6">The program synthesis research utilizes datasets from the UCI Machine Learning Repository for its evaluations.</data>
  <data key="d7">data source, research application</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="arXiv preprint arXiv:2408.04556" target="Nature Machine Intelligence">
  <data key="d5">8.0</data>
  <data key="d6">The research on bias-alleviating low-rank adaptation is published in Nature Machine Intelligence, contributing to the field of AI ethics and performance.</data>
  <data key="d7">publication, AI ethics</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="arXiv preprint arXiv:2408.04556" target="Code Alpaca">
  <data key="d5">7.0</data>
  <data key="d6">The instruction-following LLaMA model, Code Alpaca, is related to advancements in language models discussed in the paper on bias-alleviating techniques.</data>
  <data key="d7">model development, research application</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="arXiv preprint arXiv:2404.04475" target="IEEE Security and Privacy Workshops (SPW)">
  <data key="d5">7.0</data>
  <data key="d6">The arXiv preprint discusses topics relevant to security and privacy, which are central themes at the IEEE SPW.</data>
  <data key="d7">research relevance, thematic connection</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="arXiv preprint arXiv:2404.04475" target="Lora">
  <data key="d5">9.0</data>
  <data key="d6">Lora's methodology for adapting language models is discussed as part of advancements in LLM fine-tuning in the arXiv preprint.</data>
  <data key="d7">methodology, model adaptation</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="arXiv preprint arXiv:2404.04475" target="Math Dataset">
  <data key="d5">8.0</data>
  <data key="d6">The Math Dataset is referenced in the context of evaluating mathematical problem-solving capabilities of language models.</data>
  <data key="d7">evaluation, problem-solving</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="arXiv preprint arXiv:2404.04475" target="Wizardmath">
  <data key="d5">9.0</data>
  <data key="d6">Wizardmath's focus on enhancing mathematical reasoning aligns with the advancements discussed in the arXiv preprint.</data>
  <data key="d7">research alignment, reasoning enhancement</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Ji Gao" target="Jack Lanchantin">
  <data key="d5">8.0</data>
  <data key="d6">Ji Gao and Jack Lanchantin collaborated on research involving adversarial text generation techniques for deep learning classifiers.</data>
  <data key="d7">collaboration, research focus</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Jack Lanchantin" target="Mary Lou Soffa">
  <data key="d5">8.0</data>
  <data key="d6">Jack Lanchantin and Mary Lou Soffa have worked together on adversarial techniques in machine learning, focusing on text generation.</data>
  <data key="d7">collaboration, adversarial techniques</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Mary Lou Soffa" target="Yanjun Qi">
  <data key="d5">7.0</data>
  <data key="d6">Mary Lou Soffa and Yanjun Qi are both involved in research related to adversarial machine learning, contributing to similar projects.</data>
  <data key="d7">research alignment, adversarial learning</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Chenlu Guo" target="Nuo Xu">
  <data key="d5">9.0</data>
  <data key="d6">Chenlu Guo and Nuo Xu collaborated on the development of a dataset for evaluating health in large language models.</data>
  <data key="d7">dataset development, collaboration</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Dan Hendrycks" target="Collin Burns">
  <data key="d5">9.0</data>
  <data key="d6">Dan Hendrycks and Collin Burns collaborated on the Math Dataset, which evaluates mathematical problem-solving abilities in machine learning models.</data>
  <data key="d7">dataset collaboration, evaluation</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Collin Burns" target="Saurav Kadavath">
  <data key="d5">8.0</data>
  <data key="d6">Collin Burns and Saurav Kadavath worked together on projects related to evaluating mathematical capabilities in machine learning models.</data>
  <data key="d7">collaboration, evaluation focus</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Saurav Kadavath" target="Akul Arora">
  <data key="d5">8.0</data>
  <data key="d6">Saurav Kadavath and Akul Arora collaborated on evaluating machine learning models, particularly in mathematical contexts.</data>
  <data key="d7">research collaboration, evaluation focus</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Akul Arora" target="Steven Basart">
  <data key="d5">7.0</data>
  <data key="d6">Akul Arora and Steven Basart are involved in similar research projects focused on evaluating mathematical problem-solving in machine learning.</data>
  <data key="d7">collaboration, evaluation focus</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Steven Basart" target="Eric Tang&quot;&lt;||&quot;Steven Basart and Eric Tang have worked together on projects evaluating machine learning models, particularly in mathematics.">
  <data key="d5">7.0</data>
  <data key="d6">research collaboration, evaluation focus</data>
  <data key="d7">7</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Dawn Song" target="Jacob Steinhardt&quot;&lt;||&quot;Dawn Song and Jacob Steinhardt are both prominent researchers in machine learning, collaborating on evaluation methodologies.">
  <data key="d5">8.0</data>
  <data key="d6">collaboration, research focus</data>
  <data key="d7">8</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Edward J Hu" target="Yelong Shen">
  <data key="d5">9.0</data>
  <data key="d6">Edward J Hu and Yelong Shen are involved in research focused on low-rank adaptation techniques for language models.</data>
  <data key="d7">research collaboration, model adaptation</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Phillip Wallis" target="Zeyuan Allen-Zhu">
  <data key="d5">8.0</data>
  <data key="d6">Phillip Wallis and Zeyuan Allen-Zhu have collaborated on developing techniques for adapting large language models.</data>
  <data key="d7">collaboration, model adaptation</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Yuanzhi Li" target="Shean Wang">
  <data key="d5">7.0</data>
  <data key="d6">Yuanzhi Li and Shean Wang are both focused on enhancing the performance of language models through adaptation techniques.</data>
  <data key="d7">research alignment, performance enhancement</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Lu Wang" target="Weizhu Chen">
  <data key="d5">8.0</data>
  <data key="d6">Lu Wang and Weizhu Chen are involved in similar research areas, focusing on adapting large language models for improved efficiency.</data>
  <data key="d7">research collaboration, model adaptation</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Wenxuan Wang" target="Jen-tse Huang">
  <data key="d5">8.0</data>
  <data key="d6">Wenxuan Wang and Jen-tse Huang have collaborated on evaluating language models, particularly in translation contexts.</data>
  <data key="d7">collaboration, evaluation focus</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Xing Wang" target="Zhaopeng Tu">
  <data key="d5">7.0</data>
  <data key="d6">Xing Wang and Zhaopeng Tu are both focused on evaluating machine learning models, particularly in translation tasks.</data>
  <data key="d7">research collaboration, evaluation focus</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Xisen Jin" target="Xiang Ren">
  <data key="d5">8.0</data>
  <data key="d6">Xisen Jin and Xiang Ren are researchers collaborating on knowledge fusion techniques in language models.</data>
  <data key="d7">research collaboration, knowledge integration</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Daniel Preotiuc-Pietro" target="Pengxiang Cheng">
  <data key="d5">9.0</data>
  <data key="d6">Daniel Preotiuc-Pietro and Pengxiang Cheng are involved in developing techniques for knowledge integration in language models.</data>
  <data key="d7">research collaboration, knowledge integration</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Alex Krizhevsky" target="Geoffrey Hinton">
  <data key="d5">9.0</data>
  <data key="d6">Alex Krizhevsky and Geoffrey Hinton are both prominent figures in deep learning, collaborating on various research projects.</data>
  <data key="d7">collaboration, deep learning</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Xuechen Li" target="Tianyi Zhang">
  <data key="d5">8.0</data>
  <data key="d6">Xuechen Li and Tianyi Zhang are involved in evaluating instruction-following models, contributing to similar research areas.</data>
  <data key="d7">research collaboration, evaluation focus</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Yann Dubois" target="Rohan Taori">
  <data key="d5">7.0</data>
  <data key="d6">Yann Dubois and Rohan Taori are both focused on evaluating machine learning models, particularly in instruction-following contexts.</data>
  <data key="d7">research collaboration, evaluation focus</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Ishaan Gulrajani" target="Carlos Guestrin">
  <data key="d5">8.0</data>
  <data key="d6">Ishaan Gulrajani and Carlos Guestrin are involved in research on machine learning evaluation techniques, contributing to similar projects.</data>
  <data key="d7">research collaboration, evaluation focus</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Percy Liang" target="Tatsunori B Hashimoto">
  <data key="d5">9.0</data>
  <data key="d6">Percy Liang and Tatsunori B Hashimoto are both researchers focused on evaluating instruction-following models and their applications.</data>
  <data key="d7">research collaboration, evaluation focus</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Norman Sadeh" target="Stress test evaluation for natural language inference">
  <data key="d5">16.0</data>
  <data key="d6">Norman Sadeh contributed to the research on stress test evaluation in natural language inference, indicating his involvement in the study.</data>
  <data key="d7">research contribution, academic collaboration</data>
  <data key="d8">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Carolyn Rose" target="Stress test evaluation for natural language inference">
  <data key="d5">16.0</data>
  <data key="d6">Carolyn Rose co-authored the research on stress test evaluation for natural language inference, showcasing her collaboration with Norman Sadeh.</data>
  <data key="d7">research contribution, academic collaboration</data>
  <data key="d8">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Graham Neubig" target="Stress test evaluation for natural language inference">
  <data key="d5">16.0</data>
  <data key="d6">Graham Neubig is a co-author of the study on stress test evaluation for natural language inference, indicating his expertise in the field.</data>
  <data key="d7">research contribution, academic collaboration</data>
  <data key="d8">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Daye Nam" target="Using an llm to help with code understanding">
  <data key="d5">18.0</data>
  <data key="d6">Daye Nam is a contributor to research that explores the use of large language models for code understanding, indicating his role in software engineering research.</data>
  <data key="d7">research contribution, software engineering</data>
  <data key="d8">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Andrew Macvean" target="Using an llm to help with code understanding">
  <data key="d5">18.0</data>
  <data key="d6">Andrew Macvean co-authored the research on using language models for code understanding, reflecting his involvement in the project.</data>
  <data key="d7">research contribution, software engineering</data>
  <data key="d8">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Vincent Hellendoorn" target="Using an llm to help with code understanding">
  <data key="d5">18.0</data>
  <data key="d6">Vincent Hellendoorn's contribution to the research indicates his focus on enhancing code comprehension using AI tools.</data>
  <data key="d7">research contribution, software engineering</data>
  <data key="d8">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Bogdan Vasilescu" target="Using an llm to help with code understanding">
  <data key="d5">18.0</data>
  <data key="d6">Bogdan Vasilescu's involvement in the research highlights his expertise in software engineering and AI applications.</data>
  <data key="d7">research contribution, software engineering</data>
  <data key="d8">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Brad Myers" target="Using an llm to help with code understanding">
  <data key="d5">18.0</data>
  <data key="d6">Brad Myers' contribution to the research indicates his commitment to advancing software engineering methodologies using AI.</data>
  <data key="d7">research contribution, software engineering</data>
  <data key="d8">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="R OpenAI" target="GPT-4 Technical Report">
  <data key="d5">20.0</data>
  <data key="d6">R OpenAI published the GPT-4 Technical Report, detailing their advancements in AI language models.</data>
  <data key="d7">organization output, AI development</data>
  <data key="d8">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Stanford Alpaca" target="Instruction-following llama model">
  <data key="d5">18.0</data>
  <data key="d6">Stanford Alpaca is a project that focuses on instruction-following models, showcasing advancements in AI technology.</data>
  <data key="d7">AI research, model development</data>
  <data key="d8">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Stanford Alpaca" target="Llama-2-13b-code-alpaca">
  <data key="d5">14.0</data>
  <data key="d6">Llama-2-13b-code-alpaca is fine-tuned from Stanford Alpaca, focusing on code-related tasks.&lt;SEP&gt;Llama-2-13b-code-alpaca is fine-tuned from Stanford Alpaca, focusing on code-related tasks.".</data>
  <data key="d7">model development, fine-tuning</data>
  <data key="d8">chunk-9fbd0c0622e4d5fb84be83781e1c4521</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Imagenet Challenge" target="Large scale visual recognition challenge">
  <data key="d5">18.0</data>
  <data key="d6">The Imagenet Challenge assesses large-scale visual recognition capabilities, indicating its significance in computer vision research.</data>
  <data key="d7">competition, visual recognition</data>
  <data key="d8">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Speech Commands Dataset" target="Launching the speech commands dataset">
  <data key="d5">18.0</data>
  <data key="d6">The Speech Commands Dataset was launched by R OpenAI, indicating their role in advancing speech recognition technology.</data>
  <data key="d7">dataset launch, AI research</data>
  <data key="d8">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Chain-of-Thought Prompting" target="Chain-of-thought prompting elicits reasoning in large language models">
  <data key="d5">20.0</data>
  <data key="d6">Chain-of-Thought Prompting is a technique used to enhance reasoning in AI, indicating its significance in model training.</data>
  <data key="d7">AI training technique, reasoning enhancement</data>
  <data key="d8">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="arXiv preprint arXiv:2410.09335" target="Rethinking data selection at scale">
  <data key="d5">8.0</data>
  <data key="d6">The preprint discusses data selection, a critical aspect of machine learning, reflecting ongoing research in the field.</data>
  <data key="d7">research dissemination, data selection</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="ACM Transactions on Management Information Systems" target="Designing heterogeneous llm agents for financial sentiment analysis">
  <data key="d5">9.0</data>
  <data key="d6">The research published in this journal includes advancements in designing agents for financial analysis.</data>
  <data key="d7">research publication, financial analysis</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="The Twelfth International Conference on Learning Representations" target="Wizardlm: Empowering large pre-trained language models">
  <data key="d5">9.0</data>
  <data key="d6">This conference features research on empowering language models, including the Wizardlm project.</data>
  <data key="d7">conference presentation, language models</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="arXiv preprint arXiv:2306.01708" target="Resolving interference when merging models">
  <data key="d5">8.0</data>
  <data key="d6">The preprint addresses challenges in model merging, relevant to the ongoing research in the field.</data>
  <data key="d7">research dissemination, model merging</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="arXiv preprint arXiv:2408.07666" target="Model merging in llms, mllms, and beyond">
  <data key="d5">8.0</data>
  <data key="d6">This preprint discusses model merging techniques, contributing to the understanding of large language models.</data>
  <data key="d7">research dissemination, model merging</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Forty-first International Conference on Machine Learning" target="Language models are super mario">
  <data key="d5">9.0</data>
  <data key="d6">The conference presents research on innovative techniques for language models, including the Super Mario analogy.</data>
  <data key="d7">conference presentation, language models</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Tingyu Xia" target="Rethinking data selection at scale">
  <data key="d5">9.0</data>
  <data key="d6">Tingyu Xia is a co-author of the paper discussing effective data selection methods in machine learning.</data>
  <data key="d7">research contribution, data selection</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Bowen Yu" target="Rethinking data selection at scale">
  <data key="d5">8.0</data>
  <data key="d6">Bowen Yu collaborates on the research paper focusing on data selection techniques in machine learning.</data>
  <data key="d7">research collaboration, data selection</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Kai Dang" target="Rethinking data selection at scale">
  <data key="d5">8.0</data>
  <data key="d6">Kai Dang is a co-author contributing to the study of data selection methods in machine learning.</data>
  <data key="d7">research collaboration, data selection</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="An Yang" target="Rethinking data selection at scale">
  <data key="d5">8.0</data>
  <data key="d6">An Yang contributes to the research discussing random selection in machine learning.</data>
  <data key="d7">research collaboration, data selection</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Yuan Tian" target="Rethinking data selection at scale">
  <data key="d5">8.0</data>
  <data key="d6">Yuan Tian is a co-author of the paper focusing on data selection techniques in machine learning.</data>
  <data key="d7">research collaboration, data selection</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Junyang Lin" target="Rethinking data selection at scale">
  <data key="d5">8.0</data>
  <data key="d6">Junyang Lin is a co-author of the research discussing effective data selection in machine learning.</data>
  <data key="d7">research collaboration, data selection</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Frank Xing" target="Designing heterogeneous llm agents for financial sentiment analysis">
  <data key="d5">9.0</data>
  <data key="d6">Frank Xing authored a paper discussing the design of language model agents for financial analysis.</data>
  <data key="d7">research contribution, financial sentiment analysis</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Can Xu" target="Wizardlm: Empowering large pre-trained language models">
  <data key="d5">9.0</data>
  <data key="d6">Can Xu is a co-author of the research on enhancing large language models for complex tasks.</data>
  <data key="d7">research collaboration, language models</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Qingfeng Sun" target="Wizardlm: Empowering large pre-trained language models">
  <data key="d5">9.0</data>
  <data key="d6">Qingfeng Sun collaborates on the research discussing large pre-trained language models.</data>
  <data key="d7">research collaboration, language models</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Kai Zheng" target="Wizardlm: Empowering large pre-trained language models">
  <data key="d5">9.0</data>
  <data key="d6">Kai Zheng is a co-author of the paper on empowering large language models for complex instructions.</data>
  <data key="d7">research collaboration, language models</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Xiubo Geng" target="Wizardlm: Empowering large pre-trained language models">
  <data key="d5">9.0</data>
  <data key="d6">Xiubo Geng contributes to the research on enhancing language models for instruction following tasks.</data>
  <data key="d7">research collaboration, language models</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Pu Zhao" target="Wizardlm: Empowering large pre-trained language models">
  <data key="d5">9.0</data>
  <data key="d6">Pu Zhao is involved in the research discussing advancements in large language models.</data>
  <data key="d7">research collaboration, language models</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Jiazhan Feng" target="Wizardlm: Empowering large pre-trained language models">
  <data key="d5">9.0</data>
  <data key="d6">Jiazhan Feng co-authors the research on empowering language models for complex tasks.</data>
  <data key="d7">research collaboration, language models</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Chongyang Tao" target="Wizardlm: Empowering large pre-trained language models">
  <data key="d5">9.0</data>
  <data key="d6">Chongyang Tao is a contributor to the research on enhancing language models for instruction following tasks.</data>
  <data key="d7">research collaboration, language models</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Qingwei Lin" target="Wizardlm: Empowering large pre-trained language models">
  <data key="d5">9.0</data>
  <data key="d6">Qingwei Lin collaborates on the research discussing large language models and their capabilities.</data>
  <data key="d7">research collaboration, language models</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Daxin Jiang" target="Wizardlm: Empowering large pre-trained language models">
  <data key="d5">9.0</data>
  <data key="d6">Daxin Jiang is involved in the study of empowering language models for better performance.</data>
  <data key="d7">research collaboration, language models</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Proceedings of the 1st ACM Workshop on Large AI Systems and Models with Privacy and Safety Analysis" target="large AI systems">
  <data key="d5">8.0</data>
  <data key="d6">This event focuses on the implications of large AI systems, discussing privacy and safety analysis.</data>
  <data key="d7">event focus, AI systems</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Journal of Machine Learning Research" target="Designing heterogeneous llm agents for financial sentiment analysis">
  <data key="d5">9.0</data>
  <data key="d6">The journal publishes significant research findings, including those on financial sentiment analysis agents.</data>
  <data key="d7">research publication, financial analysis</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="WizardMath-Mistral 7B" target="WizardMath 70B">
  <data key="d5">16.0</data>
  <data key="d6">Both WizardMath-Mistral 7B and WizardMath 70B are mathematical models evaluated for their performance in solving math problems.&lt;SEP&gt;Both WizardMath-Mistral 7B and WizardMath 70B are mathematical models evaluated for their performance in solving math problems.".</data>
  <data key="d7">model comparison, performance evaluation</data>
  <data key="d8">chunk-9fbd0c0622e4d5fb84be83781e1c4521</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="WizardMath 70B" target="GPT-3.5-Turbo">
  <data key="d5">18.0</data>
  <data key="d6">WizardMath 70B is compared to GPT-3.5-Turbo in terms of mathematical reasoning capabilities.&lt;SEP&gt;WizardMath 70B is compared to GPT-3.5-Turbo in terms of mathematical reasoning capabilities.".</data>
  <data key="d7">model comparison, reasoning tasks</data>
  <data key="d8">chunk-9fbd0c0622e4d5fb84be83781e1c4521</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="WizardMath 70B" target="Claude 2">
  <data key="d5">16.0</data>
  <data key="d6">WizardMath 70B is evaluated alongside Claude 2 for mathematical reasoning tasks.&lt;SEP&gt;WizardMath 70B is evaluated alongside Claude 2 for mathematical reasoning tasks.".</data>
  <data key="d7">model comparison, evaluation</data>
  <data key="d8">chunk-9fbd0c0622e4d5fb84be83781e1c4521</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Task Vectors" target="Merged Task Vector">
  <data key="d5">8.0</data>
  <data key="d6">Merged Task Vector is derived from the combination of two or more Task Vectors, enhancing the model's ability to learn from data.</data>
  <data key="d7">model enhancement, data representation</data>
  <data key="d8">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Parameter Value" target="Model Performance Metrics">
  <data key="d5">8.0</data>
  <data key="d6">Parameter Value settings influence Model Performance Metrics, affecting how well a model performs on various tasks.</data>
  <data key="d7">model configuration, performance</data>
  <data key="d8">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Extraneous Information" target="Model Performance Metrics">
  <data key="d5">7.0</data>
  <data key="d6">Extraneous Information can negatively impact Model Performance Metrics by introducing noise and confusion during evaluation.</data>
  <data key="d7">data quality, evaluation impact</data>
  <data key="d8">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Marthe Ballon" target="Andres Algaba">
  <data key="d5">12.0</data>
  <data key="d6">Marthe Ballon and Andres Algaba are both researchers contributing to the study of language models, affiliated with the same institutions.</data>
  <data key="d7">collaboration, research affiliation</data>
  <data key="d8">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Marthe Ballon" target="Vincent Ginis">
  <data key="d5">12.0</data>
  <data key="d6">Marthe Ballon and Vincent Ginis work together in the Data Analytics Lab, focusing on language model research.</data>
  <data key="d7">collaboration, research affiliation</data>
  <data key="d8">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Marthe Ballon" target="Data Analytics Lab">
  <data key="d5">8.0</data>
  <data key="d6">Marthe Ballon conducts research at the Data Analytics Lab, contributing to advancements in language models.</data>
  <data key="d7">research, institutional affiliation</data>
  <data key="d8">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Marthe Ballon" target="Vrije Universiteit Brussel">
  <data key="d5">9.0</data>
  <data key="d6">Marthe Ballon is affiliated with Vrije Universiteit Brussel, where she conducts her research.</data>
  <data key="d7">affiliation, academic research</data>
  <data key="d8">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Andres Algaba" target="Vincent Ginis">
  <data key="d5">12.0</data>
  <data key="d6">Andres Algaba and Vincent Ginis are researchers at different institutions but collaborate on language model studies.</data>
  <data key="d7">collaboration, research affiliation</data>
  <data key="d8">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Andres Algaba" target="Data Analytics Lab">
  <data key="d5">8.0</data>
  <data key="d6">Andres Algaba is associated with the Data Analytics Lab, focusing on machine learning and language models.</data>
  <data key="d7">research, institutional affiliation</data>
  <data key="d8">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Andres Algaba" target="Vrije Universiteit Brussel">
  <data key="d5">9.0</data>
  <data key="d6">Andres Algaba is affiliated with Vrije Universiteit Brussel, contributing to research initiatives.</data>
  <data key="d7">affiliation, academic research</data>
  <data key="d8">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Vincent Ginis" target="Data Analytics Lab">
  <data key="d5">8.0</data>
  <data key="d6">Vincent Ginis is part of the Data Analytics Lab, contributing to research on large language models.</data>
  <data key="d7">research, institutional affiliation</data>
  <data key="d8">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Vincent Ginis" target="Harvard University">
  <data key="d5">9.0</data>
  <data key="d6">Vincent Ginis is affiliated with Harvard University, where he collaborates on research related to language models.</data>
  <data key="d7">affiliation, academic research</data>
  <data key="d8">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
</graph></graphml>