<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd"><key id="d9" for="edge" attr.name="file_path" attr.type="string"/>
<key id="d8" for="edge" attr.name="source_id" attr.type="string"/>
<key id="d7" for="edge" attr.name="keywords" attr.type="string"/>
<key id="d6" for="edge" attr.name="description" attr.type="string"/>
<key id="d5" for="edge" attr.name="weight" attr.type="double"/>
<key id="d4" for="node" attr.name="file_path" attr.type="string"/>
<key id="d3" for="node" attr.name="source_id" attr.type="string"/>
<key id="d2" for="node" attr.name="description" attr.type="string"/>
<key id="d1" for="node" attr.name="entity_type" attr.type="string"/>
<key id="d0" for="node" attr.name="entity_id" attr.type="string"/>
<graph edgedefault="undirected"><node id="Takuya Akiba">
  <data key="d0">Takuya Akiba</data>
  <data key="d1">person</data>
  <data key="d2">Takuya Akiba is a researcher involved in the development of evolutionary optimization methods for model merging in machine learning.&lt;SEP&gt;Takuya Akiba is a researcher known for contributions to hyperparameter optimization frameworks and machine learning.&lt;SEP&gt;Takuya Akiba is the initiator of the 'Evolutionary Optimization of Model Merging Recipes' project and has contributed significantly to its design and methodology.&lt;SEP&gt;Takuya Akiba is the initiator of the 'Evolutionary Optimization of Model Merging Recipes' project, contributing significantly to the research.&lt;SEP&gt;Takuya Akiba is one of the authors of a referenced paper on evolutionary optimization of model merging recipes.</data>
  <data key="d3">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b&lt;SEP&gt;chunk-3d9244715091995dd6afa5bd49e8dc52&lt;SEP&gt;chunk-605e26052d578b60e0e6849cb0d635d9&lt;SEP&gt;chunk-72a25142a6a0a013507aa055b73531c4&lt;SEP&gt;chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Makoto Shing">
  <data key="d0">Makoto Shing</data>
  <data key="d1">person</data>
  <data key="d2">Makoto Shing expanded the parameter space model merging to include vision-language models and diffusion models in the research.&lt;SEP&gt;Makoto Shing is a researcher contributing to the study of evolutionary optimization techniques in the context of model merging.&lt;SEP&gt;Makoto Shing is a co-author of a paper discussing the optimization of model merging recipes.</data>
  <data key="d3">chunk-72a25142a6a0a013507aa055b73531c4&lt;SEP&gt;chunk-3d9244715091995dd6afa5bd49e8dc52&lt;SEP&gt;chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Yujin Tang">
  <data key="d0">Yujin Tang</data>
  <data key="d1">person</data>
  <data key="d2">Yujin Tang directed efforts in data flow space model merging and incorporated ideas from neural architecture search.&lt;SEP&gt;Yujin Tang is a researcher focused on evolutionary approaches to combining models for enhanced performance in machine learning.&lt;SEP&gt;Yujin Tang is an author focused on hardware-accelerated neuroevolution.&lt;SEP&gt;Yujin Tang is a contributor to the research on evolutionary optimization of model merging recipes.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f&lt;SEP&gt;chunk-72a25142a6a0a013507aa055b73531c4&lt;SEP&gt;chunk-3d9244715091995dd6afa5bd49e8dc52&lt;SEP&gt;chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Qi Sun">
  <data key="d0">Qi Sun</data>
  <data key="d1">person</data>
  <data key="d2">Qi Sun contributed to the implementation of the parameter space model merging framework and assisted in model evaluation.&lt;SEP&gt;Qi Sun is a researcher participating in the exploration of effective model merging strategies using evolutionary algorithms.&lt;SEP&gt;Qi Sun is an author who has explored the application of transformer layers in computational models.&lt;SEP&gt;Qi Sun is involved in the study of evolutionary optimization of model merging recipes.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f&lt;SEP&gt;chunk-72a25142a6a0a013507aa055b73531c4&lt;SEP&gt;chunk-3d9244715091995dd6afa5bd49e8dc52&lt;SEP&gt;chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="David Ha">
  <data key="d0">David Ha</data>
  <data key="d1">person</data>
  <data key="d2">David Ha is a co-author of the paper on evolutionary optimization of model merging recipes.&lt;SEP&gt;David Ha is a researcher contributing to the field of AI and model merging, particularly through evolutionary methods.&lt;SEP&gt;David Ha is an author known for work in neuroevolution and AI.&lt;SEP&gt;David Ha is an author who has published research on hypernetworks and neural networks, contributing to the field of artificial intelligence.&lt;SEP&gt;David Ha provided guidance and technical insight for the research project, including feedback and writing.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f&lt;SEP&gt;chunk-3d9244715091995dd6afa5bd49e8dc52&lt;SEP&gt;chunk-144f32db3d945848c593404dfe014f4f&lt;SEP&gt;chunk-72a25142a6a0a013507aa055b73531c4&lt;SEP&gt;chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Sakana AI">
  <data key="d0">Sakana AI</data>
  <data key="d1">organization</data>
  <data key="d2">Sakana AI is an organization based in Tokyo, Japan, focused on advancing AI technologies and methodologies.&lt;SEP&gt;Sakana AI is an organization that focuses on developing advanced AI models and provides datasets for research.</data>
  <data key="d3">chunk-72a25142a6a0a013507aa055b73531c4&lt;SEP&gt;chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Tokyo">
  <data key="d0">Tokyo</data>
  <data key="d1">geo</data>
  <data key="d2">Tokyo is mentioned as a location in the context of the document but does not have a direct relevance to the entities identified.&lt;SEP&gt;Tokyo is the capital city of Japan and the location where Sakana AI is based.&lt;SEP&gt;Tokyo is the capital city of Japan, located in the Kanto region, and is known for its modern architecture and historical sites.</data>
  <data key="d3">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a&lt;SEP&gt;chunk-72a25142a6a0a013507aa055b73531c4&lt;SEP&gt;chunk-3ca398087b4d5dd48a0c1fe169e9b7fd</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Evolutionary Model Merge">
  <data key="d0">Evolutionary Model Merge</data>
  <data key="d1">category</data>
  <data key="d2">Evolutionary Model Merge is an approach that evolves models through merging, aiming to create new models capable of handling complex tasks across different domains.&lt;SEP&gt;Evolutionary Model Merge refers to a method developed to automatically discover effective combinations of existing models for creating new foundation models.</data>
  <data key="d3">chunk-72a25142a6a0a013507aa055b73531c4&lt;SEP&gt;chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Japanese LLM">
  <data key="d0">Japanese LLM</data>
  <data key="d1">organization</data>
  <data key="d2">The Japanese LLM is a language model designed for mathematical reasoning and culturally-specific content, achieving state-of-the-art results in benchmarks.&lt;SEP&gt;The Japanese LLM is a language model designed to process and generate text in Japanese, utilized in the model merging experiments.&lt;SEP&gt;The Japanese LLM is a large language model specifically designed to understand and generate Japanese text, showcasing advanced capabilities in various tasks.&lt;SEP&gt;The Japanese LLM refers to a language model specifically designed for understanding and processing the Japanese language.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344&lt;SEP&gt;chunk-72a25142a6a0a013507aa055b73531c4&lt;SEP&gt;chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b&lt;SEP&gt;chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Japanese VLM">
  <data key="d0">Japanese VLM</data>
  <data key="d1">organization</data>
  <data key="d2">The Japanese VLM is a vision-language model that integrates visual and textual information, tailored for understanding Japanese culture-specific content.&lt;SEP&gt;The Japanese VLM is a vision-language model that is culturally aware and performs well on various benchmarks.</data>
  <data key="d3">chunk-72a25142a6a0a013507aa055b73531c4&lt;SEP&gt;chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model Merging">
  <data key="d0">Model Merging</data>
  <data key="d1">category</data>
  <data key="d2">Model Merging is a technique in AI that combines multiple pre-trained models into a single model to leverage their strengths for better performance.&lt;SEP&gt;Model Merging is a technique in machine learning that combines multiple pre-trained models into a single unified model to leverage their strengths across various tasks.&lt;SEP&gt;Model Merging is a technique that combines existing models to create new models, aiming to enhance performance without extensive retraining.&lt;SEP&gt;Model Merging refers to the process of combining multiple models to enhance performance and efficiency in machine learning tasks.&lt;SEP&gt;Model Merging refers to the process of integrating multiple models into a unified architecture, aiming to enhance performance by leveraging strengths of individual models.&lt;SEP&gt;Model merging is a process in machine learning where multiple models are combined to improve performance or efficiency.&lt;SEP&gt;Model merging is a technique in machine learning that combines parameters from multiple models to create a new, optimized model.&lt;SEP&gt;Model Merging is a technique that fuses parameters from multiple fine-tuned LLMs into a single model, enhancing capabilities without additional training costs.&lt;SEP&gt;Model Merging refers to the process of combining parameters from different models to create a unified model capable of performing multiple tasks.&lt;SEP&gt;Model merging is a process that integrates the parameters of multiple models into a unified model, enhancing their combined capabilities.&lt;SEP&gt;Model merging is a technique that integrates parameters from multiple models to create a unified model with enhanced capabilities.&lt;SEP&gt;Model merging is a technique that integrates the parameters of multiple models to create a unified model with enhanced capabilities.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-c8ff785e8102dfde59dbc9309abb6b6a&lt;SEP&gt;chunk-979d654e8dd7d60ff9ad975c80ca86bb&lt;SEP&gt;chunk-689927e59bcb702105a164ca12f7f6b1&lt;SEP&gt;chunk-67c2717229a96dfe2b74a6bceef933f8&lt;SEP&gt;chunk-5fcb50e3e44399e7bec0bc28c47a74f7&lt;SEP&gt;chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7&lt;SEP&gt;chunk-72a25142a6a0a013507aa055b73531c4&lt;SEP&gt;chunk-3df998a85b2dff618feb2e2ce206b62c&lt;SEP&gt;chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Large Language Models">
  <data key="d0">Large Language Models</data>
  <data key="d1">category</data>
  <data key="d2">Large Language Models (LLMs) are advanced AI systems capable of understanding and generating human-like text, often requiring significant computational resources for development.</data>
  <data key="d3">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Open LLM Leaderboard">
  <data key="d0">Open LLM Leaderboard</data>
  <data key="d1">organization</data>
  <data key="d2">The Open LLM Leaderboard is a platform for evaluating and comparing large language models, hosted by HuggingFace.&lt;SEP&gt;The Open LLM Leaderboard is a ranking system that showcases the performance of various large language models, highlighting the effectiveness of model merging.&lt;SEP&gt;The Open LLM Leaderboard is a ranking system that showcases the top language models, many of which are developed through community-driven merging efforts.&lt;SEP&gt;The Open LLM Leaderboard ranks language models based on their performance on various benchmarks, influencing model selection and development.&lt;SEP&gt;The Open LLM Leaderboard is a platform that ranks various language models based on their performance across different benchmarks.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f&lt;SEP&gt;chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b&lt;SEP&gt;chunk-f21dc353d188ca08cba26298157ff09d&lt;SEP&gt;chunk-72a25142a6a0a013507aa055b73531c4&lt;SEP&gt;chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Japanese Math LLM">
  <data key="d0">Japanese Math LLM</data>
  <data key="d1">category</data>
  <data key="d2">The Japanese Math LLM is a specialized large language model designed to understand and perform mathematical reasoning in the Japanese language.</data>
  <data key="d3">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Culturally-Aware Japanese VLM">
  <data key="d0">Culturally-Aware Japanese VLM</data>
  <data key="d1">category</data>
  <data key="d2">The Culturally-Aware Japanese VLM is a vision-language model that integrates visual and textual information, tailored for understanding and generating culturally relevant content in Japanese.</data>
  <data key="d3">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Evolutionary Algorithms">
  <data key="d0">Evolutionary Algorithms</data>
  <data key="d1">category</data>
  <data key="d2">Evolutionary Algorithms are computational methods inspired by natural selection, used to optimize solutions by exploring a vast space of possibilities.&lt;SEP&gt;Evolutionary Algorithms are optimization techniques inspired by natural selection, used to discover effective solutions in complex problem spaces.</data>
  <data key="d3">chunk-72a25142a6a0a013507aa055b73531c4&lt;SEP&gt;chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Foundation Model Development">
  <data key="d0">Foundation Model Development</data>
  <data key="d1">category</data>
  <data key="d2">Foundation Model Development refers to the process of creating robust AI models that serve as the basis for various applications, often requiring innovative approaches like model merging.</data>
  <data key="d3">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Pre-Trained Models">
  <data key="d0">Pre-Trained Models</data>
  <data key="d1">category</data>
  <data key="d2">Pre-Trained Models are machine learning models that have been previously trained on a large dataset and can be fine-tuned for specific tasks.</data>
  <data key="d3">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Transfer Learning">
  <data key="d0">Transfer Learning</data>
  <data key="d1">category</data>
  <data key="d2">Transfer Learning is a machine learning approach where a pre-trained model is further fine-tuned for a new, related task.</data>
  <data key="d3">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Fine-Tuning">
  <data key="d0">Fine-Tuning</data>
  <data key="d1">category</data>
  <data key="d2">Fine-Tuning is the process of training an existing model further on a specialized dataset to improve its performance for a specific task.</data>
  <data key="d3">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Machine Learning Community">
  <data key="d0">Machine Learning Community</data>
  <data key="d1">organization</data>
  <data key="d2">The Machine Learning Community encompasses researchers, developers, and enthusiasts who collaborate on developing and improving machine learning models and techniques.</data>
  <data key="d3">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Open Source Software Development">
  <data key="d0">Open Source Software Development</data>
  <data key="d1">category</data>
  <data key="d2">Open Source Software Development refers to the collaborative approach of creating software where the source code is made freely available for modification and enhancement.</data>
  <data key="d3">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Stable Diffusion">
  <data key="d0">Stable Diffusion</data>
  <data key="d1">organization</data>
  <data key="d2">Stable Diffusion is a generative model that has been widely adopted for creating images and has spawned various specialized fine-tuned versions for different styles.</data>
  <data key="d3">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Task Arithmetic">
  <data key="d0">Task Arithmetic</data>
  <data key="d1">category</data>
  <data key="d2">Task Arithmetic is a merging technique that allows varying merging ratios to improve LLM performance.&lt;SEP&gt;Task Arithmetic is a method for combining language model parameters by adjusting them based on the differences between tasks.&lt;SEP&gt;Task Arithmetic is a method for merging language models by creating task vectors that manipulate model weights to adjust the merged model's behavior.&lt;SEP&gt;Task Arithmetic is a method used in model merging that allows for linear combinations of task-specific models to create a new model.&lt;SEP&gt;Task Arithmetic is a method used in conjunction with M&lt;sup&gt;3&lt;/sup&gt; to improve model performance on specific tasks.&lt;SEP&gt;Task Arithmetic is a method used in model performance evaluation, particularly in the context of model merging.&lt;SEP&gt;Task Arithmetic is a model merging method that utilizes scaling terms to adjust model parameters during merging.&lt;SEP&gt;Task Arithmetic is a model merging technique that combines the parameters of pre-trained and fine-tuned models using a scaling factor and deltas.</data>
  <data key="d3">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-9fbd0c0622e4d5fb84be83781e1c4521&lt;SEP&gt;chunk-689927e59bcb702105a164ca12f7f6b1&lt;SEP&gt;chunk-6c3c559e2fcc2c84ba368ddf5b7db117&lt;SEP&gt;chunk-5fcb50e3e44399e7bec0bc28c47a74f7&lt;SEP&gt;chunk-3df998a85b2dff618feb2e2ce206b62c&lt;SEP&gt;chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Model Soup">
  <data key="d0">Model Soup</data>
  <data key="d1">category</data>
  <data key="d2">Model Soup is a method of merging multiple models by averaging their weights to improve performance in machine learning tasks.</data>
  <data key="d3">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Weighted Model Averaging">
  <data key="d0">Weighted Model Averaging</data>
  <data key="d1">category</data>
  <data key="d2">Weighted Model Averaging is a technique used to combine the weights of multiple models, allowing for a balanced contribution of each model in the merged output.</data>
  <data key="d3">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Latent Diffusion Models">
  <data key="d0">Latent Diffusion Models</data>
  <data key="d1">category</data>
  <data key="d2">Latent Diffusion Models are a type of generative model that operates in a latent space, allowing for efficient image generation and manipulation.</data>
  <data key="d3">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="TIES-Merging">
  <data key="d0">TIES-Merging</data>
  <data key="d1">category</data>
  <data key="d2">TIES-Merging is a merging technique that has shown significant improvements in performance for LLMs.&lt;SEP&gt;TIES-Merging is a method designed to improve model merging performance by addressing parameter interference and information loss during the merging process.&lt;SEP&gt;TIES-Merging is a method used for merging different models in the parameter space to enhance performance.&lt;SEP&gt;TIES-Merging is a method used in the optimization process of models, particularly in merging experiments to enhance performance.&lt;SEP&gt;TIES-Merging is a model merging technique that enhances performance by optimizing configurations for specific tasks in neural networks.&lt;SEP&gt;TIES-Merging is a paper that addresses challenges in merging models and resolving interference in AI systems.&lt;SEP&gt;TIES-Merging is a technique that enhances model merging by analyzing task vectors to optimize the integration of different model parameters.&lt;SEP&gt;TIES-Merging is a method used for merging different model outputs in machine learning, specifically for large language models.&lt;SEP&gt;TIES-Merging is a model merging approach that resolves task conflicts by aligning parameters according to their signs.&lt;SEP&gt;TIES-Merging is a model merging method that facilitates the integration of different models while maintaining their capabilities.&lt;SEP&gt;TIES-Merging is a model merging method that involves scaling terms and a ratio for retaining parameters with the largest-magnitude values.&lt;SEP&gt;TIES-Merging is a technique used to enhance the performance of task-specific language models.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149&lt;SEP&gt;chunk-b650b88d4bebbc45d20b8908b045eff8&lt;SEP&gt;chunk-c8ff785e8102dfde59dbc9309abb6b6a&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-9fbd0c0622e4d5fb84be83781e1c4521&lt;SEP&gt;chunk-689927e59bcb702105a164ca12f7f6b1&lt;SEP&gt;chunk-77432ea4d82a2648008d6d26c50ccdc9&lt;SEP&gt;chunk-67c2717229a96dfe2b74a6bceef933f8&lt;SEP&gt;chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7&lt;SEP&gt;chunk-17c651f0ad506eb37b0f1742513e2650&lt;SEP&gt;chunk-568ea9e09f85a7b95cfe61d48b0ba893&lt;SEP&gt;chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="DARE">
  <data key="d0">DARE</data>
  <data key="d1">category</data>
  <data key="d2">DARE (Drop And REscale) is a model sparsification method designed to reduce redundancy in model parameters while preserving task capabilities.&lt;SEP&gt;DARE is a method employed in the optimization process of machine learning models, particularly in merging experiments.&lt;SEP&gt;DARE is a method that enhances model merging by zeroing out small differences between models while amplifying significant differences, facilitating effective model integration.&lt;SEP&gt;DARE is a method that works in conjunction with TIES-Merging to provide more granular merging capabilities in neural network layers.&lt;SEP&gt;DARE is a specific technique employed in conjunction with TIES-Merging to optimize model parameters during merging experiments.&lt;SEP&gt;DARE is a technique utilized for merging models in the data flow space, aiming to improve model capabilities.&lt;SEP&gt;DARE is a model sparsification method proposed to enhance model merging performance, particularly when combined with M&lt;sup&gt;3&lt;/sup&gt;.&lt;SEP&gt;DARE is a model sparsification method that works in conjunction with M&lt;sup&gt;3&lt;/sup&gt; to improve model merging performance.&lt;SEP&gt;DARE is a sparsification technique that can be combined with model merging methods to improve performance.&lt;SEP&gt;DARE is a sparsification technique used in conjunction with model merging to enhance performance by managing delta parameters.&lt;SEP&gt;DARE is a versatile plug-in designed for merging multiple homologous fine-tuned models while reducing parameter interference, enhancing task performance across benchmarks.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149&lt;SEP&gt;chunk-c8ff785e8102dfde59dbc9309abb6b6a&lt;SEP&gt;chunk-f21dc353d188ca08cba26298157ff09d&lt;SEP&gt;chunk-67c2717229a96dfe2b74a6bceef933f8&lt;SEP&gt;chunk-77432ea4d82a2648008d6d26c50ccdc9&lt;SEP&gt;chunk-47449b4eaab59a26168fa9d821973678&lt;SEP&gt;chunk-568ea9e09f85a7b95cfe61d48b0ba893&lt;SEP&gt;chunk-3df998a85b2dff618feb2e2ce206b62c&lt;SEP&gt;chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="mergekit">
  <data key="d0">mergekit</data>
  <data key="d1">organization</data>
  <data key="d2">Mergekit is a toolkit that provides various recipes for merging language models, making model merging techniques accessible to practitioners.</data>
  <data key="d3">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Mistral">
  <data key="d0">Mistral</data>
  <data key="d1">organization</data>
  <data key="d2">Mistral is a family of models that serves as a popular base for merging techniques in the language model community.</data>
  <data key="d3">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Frankenmerging">
  <data key="d0">Frankenmerging</data>
  <data key="d1">category</data>
  <data key="d2">Frankenmerging is a method that allows users to experiment with stacking layers from different models to create new architectures, distinct from traditional weight merging.&lt;SEP&gt;Frankenmerging is a model merging approach noted for its tendency to produce poor performance outcomes.&lt;SEP&gt;Frankenmerging is another model merging technique that showed decreased performance in the experiments conducted.&lt;SEP&gt;Frankenmerging refers to a specific merging technique used in model building that combines elements from different models to create new architectures.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a&lt;SEP&gt;chunk-d8ed04169bf3a7ed0de2249060ee26fb&lt;SEP&gt;chunk-689927e59bcb702105a164ca12f7f6b1&lt;SEP&gt;chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Parameter Interference">
  <data key="d0">Parameter Interference</data>
  <data key="d1">category</data>
  <data key="d2">Parameter interference refers to the issue of conflicting parameter values and signs across models during the merging process, leading to performance degradation.</data>
  <data key="d3">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="ComPEFT">
  <data key="d0">ComPEFT</data>
  <data key="d1">category</data>
  <data key="d2">ComPEFT is a method that investigates the compression of fine-tuned weight parameter updates in model merging, aiming for improved performance.</data>
  <data key="d3">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Multimodal Model Development">
  <data key="d0">Multimodal Model Development</data>
  <data key="d1">category</data>
  <data key="d2">Multimodal model development involves creating models that can process and integrate multiple types of data, such as text and images.</data>
  <data key="d3">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Image Generation">
  <data key="d0">Image Generation</data>
  <data key="d1">category</data>
  <data key="d2">Image generation refers to the process of creating new images using machine learning models, often enhanced by merging techniques.</data>
  <data key="d3">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="LLM Communities">
  <data key="d0">LLM Communities</data>
  <data key="d1">category</data>
  <data key="d2">LLM communities consist of researchers and engineers focused on developing and merging large language models for various applications.</data>
  <data key="d3">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Evolutionary Neural Architecture Search">
  <data key="d0">Evolutionary Neural Architecture Search</data>
  <data key="d1">category</data>
  <data key="d2">Evolutionary Neural Architecture Search refers to a systematic approach that utilizes evolutionary algorithms to optimize model merging and architecture discovery in deep learning.</data>
  <data key="d3">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Neural Architecture Search (NAS)">
  <data key="d0">Neural Architecture Search (NAS)</data>
  <data key="d1">category</data>
  <data key="d2">Neural Architecture Search (NAS) is a method that employs evolutionary techniques to discover new neural network architectures, often requiring substantial computational resources.</data>
  <data key="d3">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Transformer Blocks">
  <data key="d0">Transformer Blocks</data>
  <data key="d1">category</data>
  <data key="d2">Transformer Blocks are components of neural networks that can be mixed and matched in model architectures, essential for enhancing performance in deep learning tasks.</data>
  <data key="d3">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="NEAT">
  <data key="d0">NEAT</data>
  <data key="d1">organization</data>
  <data key="d2">NEAT (NeuroEvolution of Augmenting Topologies) is an evolutionary algorithm that evolves neural network structures without the need for gradient descent training.</data>
  <data key="d3">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Weight Agnostic Neural Networks">
  <data key="d0">Weight Agnostic Neural Networks</data>
  <data key="d1">category</data>
  <data key="d2">Weight Agnostic Neural Networks are a type of neural network that evolves structures with task-specific biases without training weight parameters.</data>
  <data key="d3">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="SMASH">
  <data key="d0">SMASH</data>
  <data key="d1">category</data>
  <data key="d2">SMASH (Single Model Architecture Search) is a NAS method that avoids costly training through the application of Hypernetworks to estimate weights of architectural candidates.</data>
  <data key="d3">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Deep Learning">
  <data key="d0">Deep Learning</data>
  <data key="d1">category</data>
  <data key="d2">Deep Learning is a subset of machine learning that utilizes neural networks with many layers to analyze various forms of data.</data>
  <data key="d3">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model Building Process">
  <data key="d0">Model Building Process</data>
  <data key="d1">category</data>
  <data key="d2">The Model Building Process encompasses the methodologies and techniques used to create machine learning models, including merging and optimization strategies.</data>
  <data key="d3">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Hypernetwork">
  <data key="d0">Hypernetwork</data>
  <data key="d1">category</data>
  <data key="d2">A Hypernetwork is a neural network that generates weights for another network, enabling efficient training and architecture search.</data>
  <data key="d3">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Parameter Space (PS)">
  <data key="d0">Parameter Space (PS)</data>
  <data key="d1">category</data>
  <data key="d2">Parameter Space refers to the configuration space where model weights are adjusted to create a unified model from multiple sources.</data>
  <data key="d3">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Data Flow Space (DFS)">
  <data key="d0">Data Flow Space (DFS)</data>
  <data key="d1">category</data>
  <data key="d2">Data Flow Space is a configuration space that focuses on how data moves through layers in a neural network architecture.&lt;SEP&gt;Data Flow Space refers to a conceptual space where model merging preserves original weights and optimizes the inference path for tokens in neural networks.</data>
  <data key="d3">chunk-67c2717229a96dfe2b74a6bceef933f8&lt;SEP&gt;chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Merging Configuration Parameters">
  <data key="d0">Merging Configuration Parameters</data>
  <data key="d1">category</data>
  <data key="d2">Merging Configuration Parameters are settings established for sparsification and weight mixing at each layer of a neural network.&lt;SEP&gt;Merging Configuration Parameters are the settings that determine how models are combined, influencing the performance of the resulting architecture.</data>
  <data key="d3">chunk-67c2717229a96dfe2b74a6bceef933f8&lt;SEP&gt;chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="CMA-ES">
  <data key="d0">CMA-ES</data>
  <data key="d1">category</data>
  <data key="d2">CMA-ES is an evolutionary algorithm used to optimize merging configuration parameters in neural networks based on critical task-specific metrics.&lt;SEP&gt;CMA-ES is an optimization algorithm used in the EvoJAX framework for model parameter optimization.&lt;SEP&gt;CMA-ES is an optimization algorithm utilized within the EvoJAX framework to optimize model parameters through evolutionary strategies.&lt;SEP&gt;CMA-ES stands for Covariance Matrix Adaptation Evolution Strategy, a method used for optimization in the context of the models discussed.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149&lt;SEP&gt;chunk-67c2717229a96dfe2b74a6bceef933f8&lt;SEP&gt;chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Evolutionary Search">
  <data key="d0">Evolutionary Search</data>
  <data key="d1">category</data>
  <data key="d2">Evolutionary Search is a method used to explore a large search space of layer arrangements in model merging.</data>
  <data key="d3">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Layer-wise Merging">
  <data key="d0">Layer-wise Merging</data>
  <data key="d1">category</data>
  <data key="d2">Layer-wise Merging is a specific approach within model merging that focuses on optimizing individual layers of neural networks for improved performance.</data>
  <data key="d3">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Input/Output Embedding Layers">
  <data key="d0">Input/Output Embedding Layers</data>
  <data key="d1">category</data>
  <data key="d2">Input/Output Embedding Layers are components of neural networks that transform input data into a format suitable for processing and vice versa.</data>
  <data key="d3">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Transformer Block">
  <data key="d0">Transformer Block</data>
  <data key="d1">category</data>
  <data key="d2">Transformer Block is a building block of transformer models that allows for efficient processing of sequential data in machine learning.</data>
  <data key="d3">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Granular Merging">
  <data key="d0">Granular Merging</data>
  <data key="d1">category</data>
  <data key="d2">Granular Merging refers to the detailed and specific merging of model components to optimize performance for targeted tasks.</data>
  <data key="d3">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Sparsification">
  <data key="d0">Sparsification</data>
  <data key="d1">category</data>
  <data key="d2">Sparsification is a technique used in neural networks to reduce the number of active parameters, improving efficiency and speed.</data>
  <data key="d3">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Weight Mixing">
  <data key="d0">Weight Mixing</data>
  <data key="d1">category</data>
  <data key="d2">Weight Mixing involves combining the weights of different models or layers to create a more robust model.</data>
  <data key="d3">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Inference Path">
  <data key="d0">Inference Path</data>
  <data key="d1">category</data>
  <data key="d2">Inference Path refers to the route that data takes through a neural network during the prediction phase.</data>
  <data key="d3">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Search Space">
  <data key="d0">Search Space</data>
  <data key="d1">category</data>
  <data key="d2">Search Space is the theoretical space of all possible configurations or arrangements of a model that can be explored during optimization.</data>
  <data key="d3">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Layer Indices">
  <data key="d0">Layer Indices</data>
  <data key="d1">category</data>
  <data key="d2">Layer Indices are numerical representations of the positions of layers within a neural network model.</data>
  <data key="d3">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Sequential Order">
  <data key="d0">Sequential Order</data>
  <data key="d1">category</data>
  <data key="d2">Sequential Order refers to the arrangement of layers in a specific sequence in a model merging context.</data>
  <data key="d3">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Indicator Array">
  <data key="d0">Indicator Array</data>
  <data key="d1">category</data>
  <data key="d2">Indicator Array is a data structure used to manage the inclusion or exclusion of layers during model merging.&lt;SEP&gt;The Indicator Array is a data structure initialized within the models, used to track the state of layers and their configurations during processing.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890&lt;SEP&gt;chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Distribution Shift">
  <data key="d0">Distribution Shift</data>
  <data key="d1">category</data>
  <data key="d2">Distribution Shift refers to the changes in data distribution that can occur when inputs are processed by different layers of a model.</data>
  <data key="d3">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="PS Merging">
  <data key="d0">PS Merging</data>
  <data key="d1">category</data>
  <data key="d2">PS Merging is a strategy used within the hybrid model to improve task performance by merging different source models.&lt;SEP&gt;PS Merging is a technique used to combine multiple models in a way that optimizes for various objectives, enhancing the model's performance.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344&lt;SEP&gt;chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="DFS Merging">
  <data key="d0">DFS Merging</data>
  <data key="d1">category</data>
  <data key="d2">DFS Merging is a method applied after PS Merging to further refine the models, often utilizing multi-objective genetic algorithms like NSGA-II.&lt;SEP&gt;DFS Merging is a technique used to combine multiple models while maintaining a modest size for efficient performance on hardware.&lt;SEP&gt;DFS Merging is an approach used in the study to combine different models for improved performance.&lt;SEP&gt;DFS Merging is another strategy utilized in the hybrid model that focuses on enhancing performance through a different integration approach.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344&lt;SEP&gt;chunk-48d7bed100b753d40cd980d00870f149&lt;SEP&gt;chunk-77300cedbd95cdfcbafc5dec2d0ec643&lt;SEP&gt;chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Math LLMs">
  <data key="d0">Math LLMs</data>
  <data key="d1">organization</data>
  <data key="d2">Math LLMs are language models specifically trained to solve mathematical problems, contributing to the merging process for enhanced capabilities.</data>
  <data key="d3">chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="MGSM Dataset">
  <data key="d0">MGSM Dataset</data>
  <data key="d1">organization</data>
  <data key="d2">The MGSM Dataset is a multilingual translation dataset used for evaluating the performance of language models on math problems in Japanese.</data>
  <data key="d3">chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="GSM8k Dataset">
  <data key="d0">GSM8k Dataset</data>
  <data key="d1">organization</data>
  <data key="d2">The GSM8k Dataset is a benchmark dataset for evaluating mathematical reasoning in language models, providing a test set for model accuracy.</data>
  <data key="d3">chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Mistral-7B">
  <data key="d0">Mistral-7B</data>
  <data key="d1">organization</data>
  <data key="d2">Mistral-7B is a foundational model from which other models are fine-tuned, including the Japanese LLM and Math LLMs.</data>
  <data key="d3">chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="CMA-ES Algorithm">
  <data key="d0">CMA-ES Algorithm</data>
  <data key="d1">category</data>
  <data key="d2">CMA-ES is an optimization algorithm used in the PS Merging process to enhance model parameters for better performance.</data>
  <data key="d3">chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Optuna">
  <data key="d0">Optuna</data>
  <data key="d1">organization</data>
  <data key="d2">Optuna is a next-generation hyperparameter optimization framework used in machine learning to enhance model performance.&lt;SEP&gt;Optuna is a next-generation hyperparameter optimization framework used in machine learning.&lt;SEP&gt;Optuna is a software framework used for hyperparameter optimization, particularly in the context of the CMA-ES algorithm.</data>
  <data key="d3">chunk-605e26052d578b60e0e6849cb0d635d9&lt;SEP&gt;chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Section 3.1">
  <data key="d0">Section 3.1</data>
  <data key="d1">event</data>
  <data key="d2">Section 3.1 discusses the experimental setup for evolving a Japanese Math LLM, detailing the models and datasets used.&lt;SEP&gt;Section 3.1 refers to a specific part of the paper that discusses LLM experiments and their methodologies.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52&lt;SEP&gt;chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Section 3.3">
  <data key="d0">Section 3.3</data>
  <data key="d1">event</data>
  <data key="d2">Section 3.3 describes the process of merging a Japanese LLM with an English VLM to create a Japanese VLM.&lt;SEP&gt;Section 3.3 refers to a part of the paper that focuses on VLM experiments and their methodologies.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52&lt;SEP&gt;chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="MGSM">
  <data key="d0">MGSM</data>
  <data key="d1">organization</data>
  <data key="d2">MGSM is an organization that provides a test set used for evaluating language models, particularly in the context of Japanese Math.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="EvoJAX">
  <data key="d0">EvoJAX</data>
  <data key="d1">category</data>
  <data key="d2">EvoJAX is a framework designed for evolutionary optimization in machine learning, specifically for optimizing models through genetic algorithms.&lt;SEP&gt;EvoJAX is a framework utilized for evolutionary optimization in machine learning, particularly in the context of model merging.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model 1">
  <data key="d0">Model 1</data>
  <data key="d1">category</data>
  <data key="d2">Model 1 is a Japanese language model evaluated for its mathematical proficiency, scoring low on the MGSM-JA benchmark.&lt;SEP&gt;Model 1 is a language model evaluated for its performance in Japanese Math tasks, showing limited proficiency in mathematics.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model 2">
  <data key="d0">Model 2</data>
  <data key="d1">category</data>
  <data key="d2">Model 2 is a Math model that demonstrates mathematical proficiency but lacks command of the Japanese language.&lt;SEP&gt;Model 2 is a language model focused on mathematics, demonstrating mathematical proficiency but lacking in Japanese language skills.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model 3">
  <data key="d0">Model 3</data>
  <data key="d1">category</data>
  <data key="d2">Model 3 is another Math model that, while mathematically adept, also scores low in Japanese language capabilities.&lt;SEP&gt;Model 3 is another mathematical language model that showcases mathematical abilities but scores poorly in the Japanese language proficiency.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model 4">
  <data key="d0">Model 4</data>
  <data key="d1">category</data>
  <data key="d2">Model 4 is an optimized merged model that achieved a significant performance score on the MGSM-JA benchmark.&lt;SEP&gt;Model 4 is an optimized merged model that significantly improves performance on the MGSM-JA benchmark compared to the source models.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model 5">
  <data key="d0">Model 5</data>
  <data key="d1">category</data>
  <data key="d2">Model 5 is a DFS-merged model that exhibits performance enhancements over the original source models.&lt;SEP&gt;Model 5 is a DFS-merged model that shows performance enhancement compared to the source models.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model 6">
  <data key="d0">Model 6</data>
  <data key="d1">category</data>
  <data key="d2">Model 6 is a hybrid model that integrates both merging strategies and achieves further performance improvements.&lt;SEP&gt;Model 6 is a hybrid model that integrates both merging strategies, achieving further improvements in performance metrics.&lt;SEP&gt;Model 6 is a hybrid model that integrates multiple merging strategies to enhance performance on mathematical tasks.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149&lt;SEP&gt;chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model 7">
  <data key="d0">Model 7</data>
  <data key="d1">category</data>
  <data key="d2">Model 7 is a general English model evaluated for its performance on Japanese Math tasks.&lt;SEP&gt;Model 7 is an English general model that is evaluated in the context of Japanese Math tasks.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model 8">
  <data key="d0">Model 8</data>
  <data key="d1">category</data>
  <data key="d2">Model 8 is a Japanese general model assessed for its performance on the benchmark tasks.&lt;SEP&gt;Model 8 is a general Japanese model assessed in the context of the benchmark tasks.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model 9">
  <data key="d0">Model 9</data>
  <data key="d1">category</data>
  <data key="d2">Model 9 is a Japanese general model evaluated in the performance comparison.&lt;SEP&gt;Model 9 is another Japanese general model evaluated for its capabilities in the performance comparison.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model 10">
  <data key="d0">Model 10</data>
  <data key="d1">category</data>
  <data key="d2">Model 10 is a commercial model, GPT-3.5, evaluated for its performance on the benchmarks.&lt;SEP&gt;Model 10 is a commercial model, GPT-3.5, that is assessed for its performance on various language tasks.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model 11">
  <data key="d0">Model 11</data>
  <data key="d1">category</data>
  <data key="d2">Model 11 is a commercial model, GPT-4, recognized for its high performance across language tasks.&lt;SEP&gt;Model 11 is another commercial model, GPT-4, known for its high performance on language tasks.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="MGSM's Test Set">
  <data key="d0">MGSM's Test Set</data>
  <data key="d1">organization</data>
  <data key="d2">MGSM's Test Set is a dataset used for evaluating language models, particularly in Japanese Math tasks, providing a benchmark for performance.</data>
  <data key="d3">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="MGSM-JA task">
  <data key="d0">MGSM-JA task</data>
  <data key="d1">event</data>
  <data key="d2">The MGSM-JA task evaluates the performance of models on mathematical problems with a focus on Japanese language understanding.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Japanese Language Model Evaluation Harness (JP-LMEH)">
  <data key="d0">Japanese Language Model Evaluation Harness (JP-LMEH)</data>
  <data key="d1">organization</data>
  <data key="d2">JP-LMEH is a benchmark suite used to evaluate Japanese language proficiency across various tasks.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Shisa Gamma 7b v1">
  <data key="d0">Shisa Gamma 7b v1</data>
  <data key="d1">category</data>
  <data key="d2">Shisa Gamma 7b v1 is a Japanese language model that serves as a baseline for comparison in language proficiency evaluations.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="WizardMath 7B V1.1">
  <data key="d0">WizardMath 7B V1.1</data>
  <data key="d1">category</data>
  <data key="d2">WizardMath 7B V1.1 is another Japanese language model evaluated against the JP-LMEH tasks.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Abel 7B 002">
  <data key="d0">Abel 7B 002</data>
  <data key="d1">category</data>
  <data key="d2">Abel 7B 002 is a Japanese language model included in the evaluation of language proficiency.&lt;SEP&gt;Abel 7B 002 is a source model involved in the performance comparisons of merging methods.</data>
  <data key="d3">chunk-d8ed04169bf3a7ed0de2249060ee26fb&lt;SEP&gt;chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Ours (PS)">
  <data key="d0">Ours (PS)</data>
  <data key="d1">organization</data>
  <data key="d2">Ours (PS) is a merged model that combines different performance metrics from 7B and 10B sources.&lt;SEP&gt;Ours (PS) refers to the model developed by the authors that shows high scores in the JP-LMEH benchmark.</data>
  <data key="d3">chunk-08f49a80ddeee72c7972766e2571c12b&lt;SEP&gt;chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Ours (DFS)">
  <data key="d0">Ours (DFS)</data>
  <data key="d1">organization</data>
  <data key="d2">Ours (DFS) is a merged model that presents performance data from various 7B and 10B sources.&lt;SEP&gt;Ours (DFS) is an organization involved in model merging and optimization, showcasing various performance metrics.&lt;SEP&gt;Ours (DFS) is another model developed by the authors that integrates different merging strategies for evaluation.</data>
  <data key="d3">chunk-08f49a80ddeee72c7972766e2571c12b&lt;SEP&gt;chunk-f96b71f942f04756e590fdfa1786ae98&lt;SEP&gt;chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Hybrid Model">
  <data key="d0">Hybrid Model</data>
  <data key="d1">category</data>
  <data key="d2">The Hybrid Model refers to a model that integrates multiple merging strategies to enhance performance on various tasks, particularly in mathematical problem-solving.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Japanese StableLM 70B">
  <data key="d0">Japanese StableLM 70B</data>
  <data key="d1">category</data>
  <data key="d2">Japanese StableLM 70B is a previous state-of-the-art Japanese language model with 70 billion parameters, serving as a benchmark for comparison.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="MGSM Scores">
  <data key="d0">MGSM Scores</data>
  <data key="d1">category</data>
  <data key="d2">MGSM Scores are performance metrics used to evaluate the effectiveness of models on mathematical tasks, specifically within the MGSM-JA task.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="JSQuAD">
  <data key="d0">JSQuAD</data>
  <data key="d1">category</data>
  <data key="d2">JSQuAD is a task within the JP-LMEH benchmark suite that assesses models' abilities in question answering in Japanese.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="JAQKET">
  <data key="d0">JAQKET</data>
  <data key="d1">category</data>
  <data key="d2">JAQKET is another task in the JP-LMEH benchmark suite, evaluating models on their ability to handle Japanese questions.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="JNLI">
  <data key="d0">JNLI</data>
  <data key="d1">category</data>
  <data key="d2">JNLI is a task included in the JP-LMEH benchmark that tests models on their understanding of natural language inference in Japanese.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="MARC">
  <data key="d0">MARC</data>
  <data key="d1">category</data>
  <data key="d2">MARC is a task in the JP-LMEH framework that evaluates models on their performance in reading comprehension and reasoning.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Appendix A">
  <data key="d0">Appendix A</data>
  <data key="d1">event</data>
  <data key="d2">Appendix A contains additional details and extensive comparisons of different models and their performances.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Appendix C">
  <data key="d0">Appendix C</data>
  <data key="d1">event</data>
  <data key="d2">Appendix C showcases examples demonstrating the utility of the merged models in answering questions related to Japanese culture and math.</data>
  <data key="d3">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Ours (PS+DFS)">
  <data key="d0">Ours (PS+DFS)</data>
  <data key="d1">organization</data>
  <data key="d2">Ours (PS+DFS) is a model that integrates performance metrics from both PS and DFS categories.&lt;SEP&gt;Ours (PS+DFS) is an organization that focuses on merging models with specific parameters for enhanced performance.</data>
  <data key="d3">chunk-08f49a80ddeee72c7972766e2571c12b&lt;SEP&gt;chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Llama 2 70B">
  <data key="d0">Llama 2 70B</data>
  <data key="d1">organization</data>
  <data key="d2">Llama 2 70B refers to a specific model that is part of the analysis, showcasing performance metrics in the context of model evaluation.</data>
  <data key="d3">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Japanese Stable LM 70B">
  <data key="d0">Japanese Stable LM 70B</data>
  <data key="d1">organization</data>
  <data key="d2">Japanese Stable LM 70B is a language model that plays a critical role in the analysis, particularly noted for its contributions.</data>
  <data key="d3">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Swallow 70B">
  <data key="d0">Swallow 70B</data>
  <data key="d1">organization</data>
  <data key="d2">Swallow 70B is another model referenced in the performance analysis, contributing metrics to the overall evaluation.</data>
  <data key="d3">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="CMA-ES Optimization Results">
  <data key="d0">CMA-ES Optimization Results</data>
  <data key="d1">event</data>
  <data key="d2">CMA-ES optimization results refer to the findings from the optimization process that highlight the importance of model contributions in merging.</data>
  <data key="d3">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Parameter Configuration Post PS Merging">
  <data key="d0">Parameter Configuration Post PS Merging</data>
  <data key="d1">event</data>
  <data key="d2">Parameter configuration post PS merging refers to the evolved settings derived from the merging experiments.</data>
  <data key="d3">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Mistral Base Model">
  <data key="d0">Mistral Base Model</data>
  <data key="d1">organization</data>
  <data key="d2">Mistral Base Model is a foundational model that serves as the basis for further fine-tuning in the Japanese language models.</data>
  <data key="d3">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Shisa-Gamma-7B-v1">
  <data key="d0">Shisa-Gamma-7B-v1</data>
  <data key="d1">organization</data>
  <data key="d2">Shisa-Gamma-7B-v1 is a Japanese language model that undergoes continued pretraining and instruction fine-tuning, enhancing its capabilities.</data>
  <data key="d3">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="DARE-TIES">
  <data key="d0">DARE-TIES</data>
  <data key="d1">category</data>
  <data key="d2">DARE-TIES is a model merging technique compared against other methods in the study.&lt;SEP&gt;DARE-TIES is a model that performed relatively better in the experiments, showing slight improvements compared to source models.&lt;SEP&gt;DARE-TIES is another technique for model merging that focuses on creating new models from existing high-performing models.&lt;SEP&gt;DARE-TIES refers to the parameters associated with evolutionary merging in model optimization.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a&lt;SEP&gt;chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b&lt;SEP&gt;chunk-d8ed04169bf3a7ed0de2249060ee26fb&lt;SEP&gt;chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="MGSM-JA">
  <data key="d0">MGSM-JA</data>
  <data key="d1">category</data>
  <data key="d2">MGSM-JA is a scoring metric used to evaluate the performance of models in the experiments.&lt;SEP&gt;MGSM-JA is a scoring metric used to evaluate the performance of the models in the study.&lt;SEP&gt;MGSM-JA is a task or benchmark used to evaluate model performance, particularly in accuracy metrics.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a&lt;SEP&gt;chunk-d8ed04169bf3a7ed0de2249060ee26fb&lt;SEP&gt;chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="JP-LMEH">
  <data key="d0">JP-LMEH</data>
  <data key="d1">category</data>
  <data key="d2">JP-LMEH is another benchmark that measures average performance metrics for model evaluations.&lt;SEP&gt;JP-LMEH is another scoring metric that assesses the performance of models specifically in Japanese language tasks.&lt;SEP&gt;JP-LMEH is another scoring metric used to assess model performance, which was impacted by fine-tuning.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a&lt;SEP&gt;chunk-d8ed04169bf3a7ed0de2249060ee26fb&lt;SEP&gt;chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="TIES-Merge">
  <data key="d0">TIES-Merge</data>
  <data key="d1">organization</data>
  <data key="d2">TIES-Merge is a method used for model merging that was evaluated in the context of performance comparison.&lt;SEP&gt;TIES-Merge is a model merging approach that was tested for performance against other merging methods in the context of machine learning.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a&lt;SEP&gt;chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Shisa Gamma 7B v1">
  <data key="d0">Shisa Gamma 7B v1</data>
  <data key="d1">organization</data>
  <data key="d2">Shisa Gamma 7B v1 is a source model used in the experiments for comparison with the proposed methods.</data>
  <data key="d3">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="WizardMath 7B v1.1">
  <data key="d0">WizardMath 7B v1.1</data>
  <data key="d1">organization</data>
  <data key="d2">WizardMath 7B v1.1 is another source model utilized in the evaluation of model merging techniques.</data>
  <data key="d3">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Performance Comparison Table">
  <data key="d0">Performance Comparison Table</data>
  <data key="d1">event</data>
  <data key="d2">The Performance Comparison Table presents results of various model merging methods and their effectiveness.</data>
  <data key="d3">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="W_{ij}">
  <data key="d0">W_{ij}</data>
  <data key="d1">category</data>
  <data key="d2">W_{ij} represents scaling parameters that are crucial in the performance of evolved models, as indicated by the study's findings.</data>
  <data key="d3">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="A and B">
  <data key="d0">A and B</data>
  <data key="d1">organization</data>
  <data key="d2">Models A and B are the two distinct models that were merged and analyzed in the study for performance.</data>
  <data key="d3">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="PS">
  <data key="d0">PS</data>
  <data key="d1">category</data>
  <data key="d2">PS refers to a specific method or approach used in the context of model merging and evaluation.</data>
  <data key="d3">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="DFS">
  <data key="d0">DFS</data>
  <data key="d1">category</data>
  <data key="d2">DFS is another method utilized in model merging, evaluated alongside other techniques in the study.</data>
  <data key="d3">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="10B model">
  <data key="d0">10B model</data>
  <data key="d1">organization</data>
  <data key="d2">The 10B model (PS+DFS) is a specific model configuration that was subjected to performance analysis.</data>
  <data key="d3">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Unoptimized Model Merging">
  <data key="d0">Unoptimized Model Merging</data>
  <data key="d1">category</data>
  <data key="d2">Unoptimized Model Merging refers to the category of methods compared against the proposed methods in the study.</data>
  <data key="d3">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Table 3">
  <data key="d0">Table 3</data>
  <data key="d1">event</data>
  <data key="d2">Table 3 summarizes the performance metrics of various merging methods and serves as a key reference point in the analysis.</data>
  <data key="d3">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Ablation Studies">
  <data key="d0">Ablation Studies</data>
  <data key="d1">event</data>
  <data key="d2">Ablation Studies are experiments conducted to understand the impact of different model configurations on performance.&lt;SEP&gt;Ablation Studies refer to experiments conducted to analyze the effect of removing certain parameters on model performance.</data>
  <data key="d3">chunk-d8ed04169bf3a7ed0de2249060ee26fb&lt;SEP&gt;chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="PS model">
  <data key="d0">PS model</data>
  <data key="d1">category</data>
  <data key="d2">The PS model is a merging method that significantly outperformed other baselines in the experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="shisa-gamma-7b-v1">
  <data key="d0">shisa-gamma-7b-v1</data>
  <data key="d1">organization</data>
  <data key="d2">Shisa-gamma-7b-v1 is a source model used in the merging experiments, fine-tuned for Japanese.&lt;SEP&gt;shisa-gamma-7b-v1 is a model categorized as a 7B source model, noted for its high performance metrics.</data>
  <data key="d3">chunk-08f49a80ddeee72c7972766e2571c12b&lt;SEP&gt;chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="WizardMath-7B-V1.1">
  <data key="d0">WizardMath-7B-V1.1</data>
  <data key="d1">organization</data>
  <data key="d2">WizardMath-7B-V1.1 is a 7B source model that demonstrates specific performance metrics in various categories.&lt;SEP&gt;WizardMath-7B-V1.1 is a math-specialized model that scored high on the GSM8k benchmark, indicating strong performance in math tasks.&lt;SEP&gt;WizardMath-7B-V1.1 is a mathematical language model released under a Non-Commercial, Research-only Microsoft License.&lt;SEP&gt;WizardMath-7B-V1.1 is a source model included in the experiments, but its training data is not publicly available.</data>
  <data key="d3">chunk-08f49a80ddeee72c7972766e2571c12b&lt;SEP&gt;chunk-ef78ab420a549ea9f241c278218c7a0a&lt;SEP&gt;chunk-96aa56aec4e4f18a590f28efaff42c99&lt;SEP&gt;chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Abel-7B-002">
  <data key="d0">Abel-7B-002</data>
  <data key="d1">organization</data>
  <data key="d2">Abel-7B-002 is a 7B source model with performance metrics that highlight its capabilities.&lt;SEP&gt;Abel-7B-002 is a source model utilized in the study, with proprietary training data.</data>
  <data key="d3">chunk-08f49a80ddeee72c7972766e2571c12b&lt;SEP&gt;chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="HuggingFace">
  <data key="d0">HuggingFace</data>
  <data key="d1">organization</data>
  <data key="d2">HuggingFace is a platform that provides models and tools for machine learning, including language models used in the evaluation.&lt;SEP&gt;HuggingFace is a platform where models like shisa-gamma-7b-v1, WizardMath-7B-V1.1, and Abel-7B-002 are published.&lt;SEP&gt;HuggingFace is an organization known for its contributions to natural language processing and machine learning, including the Open LLM Leaderboard.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a&lt;SEP&gt;chunk-96aa56aec4e4f18a590f28efaff42c99&lt;SEP&gt;chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Japanese-translated GSM8k">
  <data key="d0">Japanese-translated GSM8k</data>
  <data key="d1">event</data>
  <data key="d2">Japanese-translated GSM8k is the dataset used for fine-tuning the models in the experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="MGSM-JA Score">
  <data key="d0">MGSM-JA Score</data>
  <data key="d1">category</data>
  <data key="d2">The MGSM-JA Score is a performance metric used to evaluate the effectiveness of models in the context of machine learning experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="PS+DFS">
  <data key="d0">PS+DFS</data>
  <data key="d1">method</data>
  <data key="d2">PS+DFS is a hybrid merging method that combines PS and DFS approaches to achieve the best test performance on a target task.&lt;SEP&gt;PS+DFS is a method combining different model architectures to achieve better results in machine learning tasks.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a&lt;SEP&gt;chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="LoRA">
  <data key="d0">LoRA</data>
  <data key="d1">category</data>
  <data key="d2">LoRA (Low-Rank Adaptation) is a method used to fine-tune models efficiently, employed in the experiments for model optimization.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Epochs">
  <data key="d0">Epochs</data>
  <data key="d1">category</data>
  <data key="d2">Epochs refer to the number of complete passes through the training dataset during the model training process, set to 3 in the experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Learning Rates">
  <data key="d0">Learning Rates</data>
  <data key="d1">category</data>
  <data key="d2">Learning rates are hyperparameters that control the step size during optimization, tested at various values (1e-5, 5e-5, 1e-4) in the experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Distraction Effect">
  <data key="d0">Distraction Effect</data>
  <data key="d1">category</data>
  <data key="d2">The distraction effect refers to the impact of including irrelevant models in the selection process, affecting the performance outcomes in the experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Irrelevant Models">
  <data key="d0">Irrelevant Models</data>
  <data key="d1">category</data>
  <data key="d2">Irrelevant models are those that lack relevance to the specific tasks of Japanese or mathematics, included in the distraction experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="German Model">
  <data key="d0">German Model</data>
  <data key="d1">organization</data>
  <data key="d2">The German model, referred to as leo-mistral-hessianai-7b, is an irrelevant model included in the distraction experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Spanish Model">
  <data key="d0">Spanish Model</data>
  <data key="d1">organization</data>
  <data key="d2">The Spanish model, referred to as lince-mistral-7b-it-es, is another irrelevant model included in the distraction experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Chinese Model">
  <data key="d0">Chinese Model</data>
  <data key="d1">organization</data>
  <data key="d2">The Chinese model, referred to as Mistral-7B-v0.3-Chinese-Chat, is included as an irrelevant model in the distraction experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Biomedical Model">
  <data key="d0">Biomedical Model</data>
  <data key="d1">organization</data>
  <data key="d2">The biomedical model, referred to as BioMistral-7B, is classified as an irrelevant model in the distraction experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="French Model">
  <data key="d0">French Model</data>
  <data key="d1">organization</data>
  <data key="d2">The French model, referred to as Claire-Mistral-7B-0.1, is included as an irrelevant model in the distraction experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Korean Model">
  <data key="d0">Korean Model</data>
  <data key="d1">organization</data>
  <data key="d2">The Korean model, referred to as komt-mistral-7b-v1, is classified as an irrelevant model in the distraction experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Arabic Model">
  <data key="d0">Arabic Model</data>
  <data key="d1">organization</data>
  <data key="d2">The Arabic model, referred to as Mistral-7B-v0.1-arabic, is included as an irrelevant model in the distraction experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Italian Model">
  <data key="d0">Italian Model</data>
  <data key="d1">organization</data>
  <data key="d2">The Italian model, referred to as Loquace-7B-Mistral, is classified as an irrelevant model in the distraction experiments.</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Llama-2-13b">
  <data key="d0">Llama-2-13b</data>
  <data key="d1">organization</data>
  <data key="d2">Llama-2-13b is a large language model that serves as the backbone for various tasks but has training data only up to July 2023.&lt;SEP&gt;Llama-2-13b is an English general model used for various machine learning tasks, specifically in the context of the study.</data>
  <data key="d3">chunk-b650b88d4bebbc45d20b8908b045eff8&lt;SEP&gt;chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="ELYZA-japanese-Llama-2-13b-instruct">
  <data key="d0">ELYZA-japanese-Llama-2-13b-instruct</data>
  <data key="d1">model</data>
  <data key="d2">ELYZA-japanese-Llama-2-13b-instruct is a 13B source model that exhibits specific performance metrics.&lt;SEP&gt;ELYZA-japanese-Llama-2-13b-instruct is a Japanese general model fine-tuned for specific tasks in the Japanese language.</data>
  <data key="d3">chunk-08f49a80ddeee72c7972766e2571c12b&lt;SEP&gt;chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="MetaMath-13B-V1.0">
  <data key="d0">MetaMath-13B-V1.0</data>
  <data key="d1">organization</data>
  <data key="d2">MetaMath-13B-V1.0 is a 13B source model that showcases performance metrics in various categories.&lt;SEP&gt;MetaMath-13B-V1.0 is a specific model referenced in the context of the DFS-Merged Model, notable for its layer configurations and performance metrics.&lt;SEP&gt;MetaMath-13B-V1.0 is an English math model designed to perform mathematical tasks, utilized in the study for performance comparisons.</data>
  <data key="d3">chunk-08f49a80ddeee72c7972766e2571c12b&lt;SEP&gt;chunk-10ec50dbb94517b979f409f78b996890&lt;SEP&gt;chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Mistral-7B-v0.1">
  <data key="d0">Mistral-7B-v0.1</data>
  <data key="d1">model</data>
  <data key="d2">Mistral-7B-v0.1 is a model that falls under the category of 7B source models, showcasing various performance metrics.&lt;SEP&gt;Mistral-7B-v0.1 is a smaller model known for outperforming Llama-2-13b in basic mathematical abilities.</data>
  <data key="d3">chunk-08f49a80ddeee72c7972766e2571c12b&lt;SEP&gt;chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="GPU Memory Requirement">
  <data key="d0">GPU Memory Requirement</data>
  <data key="d1">category</data>
  <data key="d2">GPU Memory Requirement refers to the computational resources needed for model inference, which affects the feasibility of certain experiments.</data>
  <data key="d3">chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Performance Comparison">
  <data key="d0">Performance Comparison</data>
  <data key="d1">category</data>
  <data key="d2">Performance Comparison involves evaluating the effectiveness of different models based on specific metrics such as MGSM-JA and JP-LMEH.</data>
  <data key="d3">chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="DFS-Merged Model">
  <data key="d0">DFS-Merged Model</data>
  <data key="d1">organization</data>
  <data key="d2">The DFS-Merged Model is a computational model that integrates layers from different models to improve performance in specific tasks, particularly in reasoning and language processing.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model A">
  <data key="d0">Model A</data>
  <data key="d1">organization</data>
  <data key="d2">Model A is a source model used in the DFS-Merged Model, known for its initial performance in the DFS process.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model B">
  <data key="d0">Model B</data>
  <data key="d1">organization</data>
  <data key="d2">Model B is another source model incorporated into the DFS-Merged Model, which may have undergone extended fine-tuning affecting its layer outputs.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Layer #30">
  <data key="d0">Layer #30</data>
  <data key="d1">category</data>
  <data key="d2">Layer #30 is a specific layer from the MetaMath-13B-V1.0 model that was identified as redundant and skipped in the DFS-Merged Model to enhance performance.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Japanese General Model">
  <data key="d0">Japanese General Model</data>
  <data key="d1">organization</data>
  <data key="d2">The Japanese General Model is a source model that contributes layers to the DFS-Merged Model, helping to improve its performance in Japanese language tasks.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Task Scenarios">
  <data key="d0">Task Scenarios</data>
  <data key="d1">event</data>
  <data key="d2">Task Scenarios refer to specific instances where the DFS-Merged Model demonstrated improved reasoning capabilities compared to its source models.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model #5">
  <data key="d0">Model #5</data>
  <data key="d1">organization</data>
  <data key="d2">Model #5 refers to the merged model that utilizes a combination of layers from different models to enhance performance in tasks, particularly in the context of depth-first search (DFS) techniques.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model #6">
  <data key="d0">Model #6</data>
  <data key="d1">organization</data>
  <data key="d2">Model #6 is a variant of the merged model that was analyzed for its performance, particularly in relation to the scaling matrix's impact on results.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model #7">
  <data key="d0">Model #7</data>
  <data key="d1">organization</data>
  <data key="d2">Model #7 is another variant of the merged model used in the study to compare performance outcomes with other configurations.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Scaling Matrix W">
  <data key="d0">Scaling Matrix W</data>
  <data key="d1">category</data>
  <data key="d2">The Scaling Matrix W is a matrix utilized in the merged models to adjust layer outputs, crucial for performance optimization.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Layer Stacking">
  <data key="d0">Layer Stacking</data>
  <data key="d1">category</data>
  <data key="d2">Layer Stacking refers to the process of organizing and combining layers from different models to improve computational efficiency and output quality.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Performance Analysis">
  <data key="d0">Performance Analysis</data>
  <data key="d1">event</data>
  <data key="d2">Performance Analysis involves evaluating the effectiveness of the merged models in various scenarios to identify strengths and weaknesses.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Task Performance">
  <data key="d0">Task Performance</data>
  <data key="d1">event</data>
  <data key="d2">Task Performance measures how well the models perform specific tasks, particularly in reasoning and language processing.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Layer Removal">
  <data key="d0">Layer Removal</data>
  <data key="d1">event</data>
  <data key="d2">Layer Removal refers to the process of excluding certain layers from a model to enhance its overall performance based on analytical findings.</data>
  <data key="d3">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="">
  <data key="d0"></data>
  <data key="d1">person</data>
  <data key="d2">DVD8DVD&lt;SEP&gt;DVDDVD</data>
  <data key="d3">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="DVD">
  <data key="d0">DVD</data>
  <data key="d1">category</data>
  <data key="d2">DVD</data>
  <data key="d3">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="">
  <data key="d0"></data>
  <data key="d1">event</data>
  <data key="d2">8DVD&lt;SEP&gt;8DVD</data>
  <data key="d3">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="">
  <data key="d0"></data>
  <data key="d1">person</data>
  <data key="d2">DVD8</data>
  <data key="d3">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="">
  <data key="d0"></data>
  <data key="d1">category</data>
  <data key="d2"></data>
  <data key="d3">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="">
  <data key="d0"></data>
  <data key="d1">event</data>
  <data key="d2"></data>
  <data key="d3">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="">
  <data key="d0"></data>
  <data key="d1">person</data>
  <data key="d2"></data>
  <data key="d3">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="">
  <data key="d0"></data>
  <data key="d1">category</data>
  <data key="d2"></data>
  <data key="d3">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="">
  <data key="d0"></data>
  <data key="d1">person</data>
  <data key="d2"></data>
  <data key="d3">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Grandma Jones">
  <data key="d0">Grandma Jones</data>
  <data key="d1">person</data>
  <data key="d2">Grandma Jones is a character who is involved in cutting pies into pieces for guests.</data>
  <data key="d3">chunk-3ca398087b4d5dd48a0c1fe169e9b7fd</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Pies">
  <data key="d0">Pies</data>
  <data key="d1">category</data>
  <data key="d2">Pies are baked goods that were cut into pieces for serving to guests.</data>
  <data key="d3">chunk-3ca398087b4d5dd48a0c1fe169e9b7fd</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Guests">
  <data key="d0">Guests</data>
  <data key="d1">person</data>
  <data key="d2">Guests are individuals who partake in eating the pies that were prepared.</data>
  <data key="d3">chunk-3ca398087b4d5dd48a0c1fe169e9b7fd</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Pieces of Pie">
  <data key="d0">Pieces of Pie</data>
  <data key="d1">category</data>
  <data key="d2">Pieces of pie refer to the individual servings cut from the original pies for guests.</data>
  <data key="d3">chunk-3ca398087b4d5dd48a0c1fe169e9b7fd</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Total Pieces">
  <data key="d0">Total Pieces</data>
  <data key="d1">category</data>
  <data key="d2">Total pieces refer to the calculated number of pie pieces available for guests, which is 40.</data>
  <data key="d3">chunk-3ca398087b4d5dd48a0c1fe169e9b7fd</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Remaining Pieces">
  <data key="d0">Remaining Pieces</data>
  <data key="d1">category</data>
  <data key="d2">Remaining pieces refer to the number of pie pieces left after guests have eaten, which is 14.</data>
  <data key="d3">chunk-3ca398087b4d5dd48a0c1fe169e9b7fd</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Eaten Pieces">
  <data key="d0">Eaten Pieces</data>
  <data key="d1">category</data>
  <data key="d2">Eaten pieces refer to the total number of pie pieces consumed by guests, which is calculated as 26.</data>
  <data key="d3">chunk-3ca398087b4d5dd48a0c1fe169e9b7fd</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Event of Pie Serving">
  <data key="d0">Event of Pie Serving</data>
  <data key="d1">event</data>
  <data key="d2">The event involves Grandma Jones serving pies to guests, where they eat and enjoy the food prepared.</data>
  <data key="d3">chunk-3ca398087b4d5dd48a0c1fe169e9b7fd</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="GPT-4-V">
  <data key="d0">GPT-4-V</data>
  <data key="d1">organization</data>
  <data key="d2">GPT-4-V is an advanced AI model used for generating captions and processing images in a culturally sensitive manner.</data>
  <data key="d3">chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="LLaVA-1.6-Mistral-7B">
  <data key="d0">LLaVA-1.6-Mistral-7B</data>
  <data key="d1">organization</data>
  <data key="d2">LLaVA-1.6-Mistral-7B is a language model that was tested for its ability to understand and respond to cultural and mathematical queries in Japanese.&lt;SEP&gt;LLaVA-1.6-Mistral-7B is a source model used in experiments for generating visual language models.</data>
  <data key="d3">chunk-57491ddb5284500415bc46fc834a6343&lt;SEP&gt;chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Japanese Stable VLM">
  <data key="d0">Japanese Stable VLM</data>
  <data key="d1">organization</data>
  <data key="d2">Japanese Stable VLM is an initiative focused on developing a stable vision-language model for Japanese.&lt;SEP&gt;Japanese Stable VLM is an open-sourced visual language model trained on Japanese datasets.</data>
  <data key="d3">chunk-77432ea4d82a2648008d6d26c50ccdc9&lt;SEP&gt;chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="JA-VG-VQA-500">
  <data key="d0">JA-VG-VQA-500</data>
  <data key="d1">event</data>
  <data key="d2">JA-VG-VQA-500 is a benchmark dataset used to evaluate visual question answering abilities in Japanese.</data>
  <data key="d3">chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="JA-VLM-Bench-In-the-Wild">
  <data key="d0">JA-VLM-Bench-In-the-Wild</data>
  <data key="d1">event</data>
  <data key="d2">JA-VLM-Bench-In-the-Wild is a benchmark that assesses the performance of models on complex visual question answering tasks within Japanese cultural contexts.</data>
  <data key="d3">chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Fasttext">
  <data key="d0">Fasttext</data>
  <data key="d1">category</data>
  <data key="d2">Fasttext is a library used for language detection tasks in the experiments.</data>
  <data key="d3">chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Evolutionary Techniques">
  <data key="d0">Evolutionary Techniques</data>
  <data key="d1">category</data>
  <data key="d2">Evolutionary techniques are methods used to discover optimal ways to combine different models with diverse capabilities.</data>
  <data key="d3">chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Culturally-Specific Content">
  <data key="d0">Culturally-Specific Content</data>
  <data key="d1">category</data>
  <data key="d2">Culturally-specific content refers to material that is tailored to the cultural nuances and context of a particular group, in this case, Japanese.</data>
  <data key="d3">chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model Performance Comparison">
  <data key="d0">Model Performance Comparison</data>
  <data key="d1">event</data>
  <data key="d2">Model Performance Comparison refers to the evaluation of various models based on their performance metrics across different benchmarks.</data>
  <data key="d3">chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Evolutionary Model Merging">
  <data key="d0">Evolutionary Model Merging</data>
  <data key="d1">category</data>
  <data key="d2">Evolutionary Model Merging is a method that integrates various models from different domains to enhance capabilities and performance in AI applications.&lt;SEP&gt;Evolutionary model merging is a method that combines various AI models to create efficient and capable models, aiming to democratize AI access.</data>
  <data key="d3">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b&lt;SEP&gt;chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="EvoSDXL">
  <data key="d0">EvoSDXL</data>
  <data key="d1">organization</data>
  <data key="d2">EvoSDXL is an application of evolutionary model merging to diffusion image generation models, demonstrating the versatility of the method.</data>
  <data key="d3">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="SDXL-Lightning">
  <data key="d0">SDXL-Lightning</data>
  <data key="d1">organization</data>
  <data key="d2">SDXL-Lightning is a specialized variant of SDXL that uses adversarial loss for rapid image generation, enhancing the efficiency of diffusion models.</data>
  <data key="d3">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="EvoVLM-JP-v2">
  <data key="d0">EvoVLM-JP-v2</data>
  <data key="d1">organization</data>
  <data key="d2">EvoVLM-JP-v2 is a model developed using evolutionary model merging techniques, showcasing adaptability in AI development.</data>
  <data key="d3">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="EvoUkiyoe">
  <data key="d0">EvoUkiyoe</data>
  <data key="d1">organization</data>
  <data key="d2">EvoUkiyoe is another model produced through evolutionary model merging, highlighting the method's potential in various domains.</data>
  <data key="d3">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="MergeKit">
  <data key="d0">MergeKit</data>
  <data key="d1">organization</data>
  <data key="d2">MergeKit is an open-source software package that implements evolutionary model merging, making the technique accessible to a wider audience.</data>
  <data key="d3">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Optuna Hub">
  <data key="d0">Optuna Hub</data>
  <data key="d1">organization</data>
  <data key="d2">Optuna Hub is an open-source software package that incorporates evolutionary model merging, facilitating practical applications in AI.</data>
  <data key="d3">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="SLERP">
  <data key="d0">SLERP</data>
  <data key="d1">category</data>
  <data key="d2">SLERP is a technique used in model merging to interpolate between two different models, enhancing their combined performance.</data>
  <data key="d3">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Foundation Models">
  <data key="d0">Foundation Models</data>
  <data key="d1">category</data>
  <data key="d2">Foundation Models are large-scale models that serve as the basis for various AI applications and can be enhanced through evolutionary techniques.</data>
  <data key="d3">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Image Diffusion Models">
  <data key="d0">Image Diffusion Models</data>
  <data key="d1">category</data>
  <data key="d2">Image Diffusion Models are a class of generative models used to create images through iterative refinement.</data>
  <data key="d3">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Benchmark Tasks">
  <data key="d0">Benchmark Tasks</data>
  <data key="d1">category</data>
  <data key="d2">Benchmark Tasks are standardized tests used to evaluate the performance of models in various AI domains.</data>
  <data key="d3">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="LLM Experiments">
  <data key="d0">LLM Experiments</data>
  <data key="d1">event</data>
  <data key="d2">LLM Experiments refer to the various experiments conducted to optimize language models using specific datasets.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="VLM Experiments">
  <data key="d0">VLM Experiments</data>
  <data key="d1">event</data>
  <data key="d2">VLM Experiments involve the use of visual-language models and benchmark datasets for optimization and evaluation.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Hugging Face">
  <data key="d0">Hugging Face</data>
  <data key="d1">organization</data>
  <data key="d2">Hugging Face is a platform that hosts the Open LLM leaderboard and is involved in various AI and machine learning projects.&lt;SEP&gt;Hugging Face is a platform that provides datasets and tools for machine learning and AI research.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52&lt;SEP&gt;chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="GitHub">
  <data key="d0">GitHub</data>
  <data key="d1">organization</data>
  <data key="d2">GitHub is a code hosting platform that provides access to various open-source projects and datasets.&lt;SEP&gt;GitHub is a platform where the EvoLLM-JP-A model will be released for public access.</data>
  <data key="d3">chunk-96aa56aec4e4f18a590f28efaff42c99&lt;SEP&gt;chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Japanese Stable LM Beta">
  <data key="d0">Japanese Stable LM Beta</data>
  <data key="d1">event</data>
  <data key="d2">Japanese Stable LM Beta is a language model developed by Stability AI, aimed at enhancing Japanese language processing.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Evolutionary Optimization of Model Merging Recipes">
  <data key="d0">Evolutionary Optimization of Model Merging Recipes</data>
  <data key="d1">category</data>
  <data key="d2">Evolutionary Optimization of Model Merging Recipes is a project aimed at improving the integration of various AI models to enhance their capabilities and efficiency.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Parameter Space Model Merging">
  <data key="d0">Parameter Space Model Merging</data>
  <data key="d1">category</data>
  <data key="d2">Parameter Space Model Merging refers to a methodology developed within the project to combine different models by optimizing their parameters.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Vision-Language Models">
  <data key="d0">Vision-Language Models</data>
  <data key="d1">category</data>
  <data key="d2">Vision-Language Models are AI models designed to process and understand both visual and textual information, enhancing multimodal AI capabilities.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Diffusion Models">
  <data key="d0">Diffusion Models</data>
  <data key="d1">category</data>
  <data key="d2">Diffusion Models are a type of generative model used in machine learning, particularly in image synthesis and transformation tasks.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Neural Architecture Search">
  <data key="d0">Neural Architecture Search</data>
  <data key="d1">category</data>
  <data key="d2">Neural Architecture Search is a technique in AI that automates the design of neural networks to optimize performance.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Morphology Evolution Literature">
  <data key="d0">Morphology Evolution Literature</data>
  <data key="d1">category</data>
  <data key="d2">Morphology Evolution Literature refers to research and studies focused on the evolution of structures and forms in biological and computational contexts.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Environmental Footprint">
  <data key="d0">Environmental Footprint</data>
  <data key="d1">category</data>
  <data key="d2">Environmental Footprint measures the impact of AI development and deployment on the environment, emphasizing sustainability.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Ethical and Societal Impact">
  <data key="d0">Ethical and Societal Impact</data>
  <data key="d1">category</data>
  <data key="d2">Ethical and Societal Impact examines the implications of AI technologies on society, highlighting the need for responsible AI deployment.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model Evaluation">
  <data key="d0">Model Evaluation</data>
  <data key="d1">category</data>
  <data key="d2">Model Evaluation is the process of assessing the performance and reliability of AI models to ensure their effectiveness in applications.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Datasets">
  <data key="d0">Datasets</data>
  <data key="d1">category</data>
  <data key="d2">Datasets are collections of data used for training and evaluating AI models, critical for their development and performance assessment.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="OpenAI">
  <data key="d0">OpenAI</data>
  <data key="d1">organization</data>
  <data key="d2">OpenAI is an artificial intelligence research organization known for developing advanced AI models and technologies.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Stability AI">
  <data key="d0">Stability AI</data>
  <data key="d1">organization</data>
  <data key="d2">Stability AI is a company focused on developing artificial intelligence technologies, including language models and evaluation tools.&lt;SEP&gt;Stability AI is a company focused on developing artificial intelligence technologies, including language models.&lt;SEP&gt;Stability AI is an organization that focuses on creating stable and reliable AI models for various applications.</data>
  <data key="d3">chunk-3d9244715091995dd6afa5bd49e8dc52&lt;SEP&gt;chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Emanuele Aiello">
  <data key="d0">Emanuele Aiello</data>
  <data key="d1">person</data>
  <data key="d2">Emanuele Aiello is a researcher involved in the development of large autoregressive multimodal models and has published works in the field of AI.&lt;SEP&gt;Emanuele Aiello is a researcher involved in the development of large autoregressive multimodal models.</data>
  <data key="d3">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="shisa-gamma-7b">
  <data key="d0">shisa-gamma-7b</data>
  <data key="d1">category</data>
  <data key="d2">Shisa-gamma-7b is a generative AI model available on HuggingFace, designed for various applications in natural language processing.&lt;SEP&gt;Shisa-gamma-7b is a generative AI model available on HuggingFace, developed for various applications.</data>
  <data key="d3">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Stable Diffusion WebUI">
  <data key="d0">Stable Diffusion WebUI</data>
  <data key="d1">category</data>
  <data key="d2">Stable Diffusion WebUI is a user interface for interacting with the Stable Diffusion model, facilitating its use in generating images.&lt;SEP&gt;Stable Diffusion WebUI is an interface that allows users to interact with the Stable Diffusion model for image generation.</data>
  <data key="d3">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Qwen-VL">
  <data key="d0">Qwen-VL</data>
  <data key="d1">category</data>
  <data key="d2">Qwen-VL is a versatile vision-language model designed for understanding and processing visual and textual information.&lt;SEP&gt;Qwen-VL is a versatile vision-language model that integrates visual and textual understanding for various applications.</data>
  <data key="d3">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Generative AI for Math: Abel">
  <data key="d0">Generative AI for Math: Abel</data>
  <data key="d1">category</data>
  <data key="d2">Generative AI for Math: Abel is a project aimed at applying generative AI technologies to solve mathematical problems.&lt;SEP&gt;Generative AI for Math: Abel is a project that applies generative AI techniques to solve mathematical problems.</data>
  <data key="d3">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model Merging by Uncertainty-Based Gradient Matching">
  <data key="d0">Model Merging by Uncertainty-Based Gradient Matching</data>
  <data key="d1">category</data>
  <data key="d2">Model Merging by Uncertainty-Based Gradient Matching is a research paper focusing on techniques for combining machine learning models effectively.&lt;SEP&gt;Model Merging by Uncertainty-Based Gradient Matching is a research paper focusing on techniques for merging machine learning models.</data>
  <data key="d3">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="InstructBLIP">
  <data key="d0">InstructBLIP</data>
  <data key="d1">category</data>
  <data key="d2">InstructBLIP is a project aimed at creating general-purpose vision-language models with instruction tuning capabilities.&lt;SEP&gt;InstructBLIP is a project aimed at developing general-purpose vision-language models with capabilities for instruction tuning and various applications.</data>
  <data key="d3">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Lili Yu">
  <data key="d0">Lili Yu</data>
  <data key="d1">person</data>
  <data key="d2">Lili Yu is a researcher contributing to the field of artificial intelligence, particularly in multimodal models.</data>
  <data key="d3">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Yixin Nie">
  <data key="d0">Yixin Nie</data>
  <data key="d1">person</data>
  <data key="d2">Yixin Nie is a researcher who has worked on large-scale AI models and their applications.</data>
  <data key="d3">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Armen Aghajanyan">
  <data key="d0">Armen Aghajanyan</data>
  <data key="d1">person</data>
  <data key="d2">Armen Aghajanyan is a researcher known for contributions to AI model training and evaluation.</data>
  <data key="d3">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Barlas Oguz">
  <data key="d0">Barlas Oguz</data>
  <data key="d1">person</data>
  <data key="d2">Barlas Oguz is a researcher involved in developing AI technologies and models.</data>
  <data key="d3">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="JP Language Model Evaluation Harness">
  <data key="d0">JP Language Model Evaluation Harness</data>
  <data key="d1">category</data>
  <data key="d2">The JP Language Model Evaluation Harness is a tool developed by Stability AI for evaluating Japanese language models.</data>
  <data key="d3">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Daniel M Roy">
  <data key="d0">Daniel M Roy</data>
  <data key="d1">person</data>
  <data key="d2">Daniel M Roy is an author who contributed to the field of deep learning, focusing on generalization bounds for neural networks.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Adam Gaier">
  <data key="d0">Adam Gaier</data>
  <data key="d1">person</data>
  <data key="d2">Adam Gaier is an author known for his work on weight agnostic neural networks, contributing to advances in neural information processing systems.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Leo Gao">
  <data key="d0">Leo Gao</data>
  <data key="d1">person</data>
  <data key="d2">Leo Gao is an author involved in developing frameworks for few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Mistral 7B">
  <data key="d0">Mistral 7B</data>
  <data key="d1">event</data>
  <data key="d2">Mistral 7B refers to a specific model or event related to advancements in language models, as referenced in the research.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="arXiv preprint">
  <data key="d0">arXiv preprint</data>
  <data key="d1">category</data>
  <data key="d2">arXiv preprint is a category of preprints that includes early research outputs in various fields, particularly in computer science and artificial intelligence.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Baber Abbasi">
  <data key="d0">Baber Abbasi</data>
  <data key="d1">person</data>
  <data key="d2">Baber Abbasi is an author who contributed to the research on few-shot language model evaluation, focusing on advancements in AI.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Stella Biderman">
  <data key="d0">Stella Biderman</data>
  <data key="d1">person</data>
  <data key="d2">Stella Biderman is an author involved in the development of frameworks for few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Sid Black">
  <data key="d0">Sid Black</data>
  <data key="d1">person</data>
  <data key="d2">Sid Black is an author who contributed to the framework for evaluating language models in the context of AI research.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Anthony DiPofi">
  <data key="d0">Anthony DiPofi</data>
  <data key="d1">person</data>
  <data key="d2">Anthony DiPofi is an author associated with the research on few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Charles Foster">
  <data key="d0">Charles Foster</data>
  <data key="d1">person</data>
  <data key="d2">Charles Foster is an author contributing to the field of AI, particularly in few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Laurence Golding">
  <data key="d0">Laurence Golding</data>
  <data key="d1">person</data>
  <data key="d2">Laurence Golding is an author involved in the development of frameworks for few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Jeffrey Hsu">
  <data key="d0">Jeffrey Hsu</data>
  <data key="d1">person</data>
  <data key="d2">Jeffrey Hsu is an author who contributed to the research on few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Alain Le Noac'h">
  <data key="d0">Alain Le Noac'h</data>
  <data key="d1">person</data>
  <data key="d2">Alain Le Noac'h is an author involved in the few-shot language model evaluation research.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Haonan Li">
  <data key="d0">Haonan Li</data>
  <data key="d1">person</data>
  <data key="d2">Haonan Li is an author contributing to advancements in few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Kyle McDonell">
  <data key="d0">Kyle McDonell</data>
  <data key="d1">person</data>
  <data key="d2">Kyle McDonell is an author associated with the few-shot language model evaluation research.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Niklas Muennighoff">
  <data key="d0">Niklas Muennighoff</data>
  <data key="d1">person</data>
  <data key="d2">Niklas Muennighoff is an author who contributed to the framework for few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Chris Ociepa">
  <data key="d0">Chris Ociepa</data>
  <data key="d1">person</data>
  <data key="d2">Chris Ociepa is an author involved in the research on few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Jason Phang">
  <data key="d0">Jason Phang</data>
  <data key="d1">person</data>
  <data key="d2">Jason Phang is an author contributing to advancements in AI through few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Laria Reynolds">
  <data key="d0">Laria Reynolds</data>
  <data key="d1">person</data>
  <data key="d2">Laria Reynolds is an author involved in the few-shot language model evaluation research.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Hailey Schoelkopf">
  <data key="d0">Hailey Schoelkopf</data>
  <data key="d1">person</data>
  <data key="d2">Hailey Schoelkopf is an author contributing to the research on few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Aviya Skowron">
  <data key="d0">Aviya Skowron</data>
  <data key="d1">person</data>
  <data key="d2">Aviya Skowron is an author associated with the few-shot language model evaluation framework.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Lintang Sutawika">
  <data key="d0">Lintang Sutawika</data>
  <data key="d1">person</data>
  <data key="d2">Lintang Sutawika is an author contributing to advancements in few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Eric Tang">
  <data key="d0">Eric Tang</data>
  <data key="d1">person</data>
  <data key="d2">Eric Tang is a researcher involved in machine learning and its evaluation, particularly in mathematical problem-solving.&lt;SEP&gt;Eric Tang is an author involved in the few-shot language model evaluation research.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Anish Thite">
  <data key="d0">Anish Thite</data>
  <data key="d1">person</data>
  <data key="d2">Anish Thite is an author who contributed to the framework for few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Ben Wang">
  <data key="d0">Ben Wang</data>
  <data key="d1">person</data>
  <data key="d2">Ben Wang is an author associated with the few-shot language model evaluation research.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Kevin Wang">
  <data key="d0">Kevin Wang</data>
  <data key="d1">person</data>
  <data key="d2">Kevin Wang is an author contributing to the advancements in AI through few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Andy Zou">
  <data key="d0">Andy Zou</data>
  <data key="d1">person</data>
  <data key="d2">Andy Zou is an author involved in the research on few-shot language model evaluation.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Charles O. Goddard">
  <data key="d0">Charles O. Goddard</data>
  <data key="d1">person</data>
  <data key="d2">Charles O. Goddard is also associated with the mergekit project, which is relevant to AI model development.&lt;SEP&gt;Charles O. Goddard is an author who developed the mergekit tool, contributing to AI research.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Mor Geva">
  <data key="d0">Mor Geva</data>
  <data key="d1">person</data>
  <data key="d2">Mor Geva is an author contributing to the understanding of transformer feed-forward layers in AI.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Avi Caciularu">
  <data key="d0">Avi Caciularu</data>
  <data key="d1">person</data>
  <data key="d2">Avi Caciularu is an author involved in research on transformer models and their applications.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Kevin Ro Wang">
  <data key="d0">Kevin Ro Wang</data>
  <data key="d1">person</data>
  <data key="d2">Kevin Ro Wang is an author contributing to advancements in transformer models in AI.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Yoav Goldberg">
  <data key="d0">Yoav Goldberg</data>
  <data key="d1">person</data>
  <data key="d2">Yoav Goldberg is an author who has published research on transformer models and their implications in AI.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Nikolaus Hansen">
  <data key="d0">Nikolaus Hansen</data>
  <data key="d1">person</data>
  <data key="d2">Nikolaus Hansen is an author known for his work on evolutionary computation and related algorithms.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Sepp Hochreiter">
  <data key="d0">Sepp Hochreiter</data>
  <data key="d1">person</data>
  <data key="d2">Sepp Hochreiter is an author known for his contributions to neural networks and optimization techniques.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Jrgen Schmidhuber">
  <data key="d0">Jrgen Schmidhuber</data>
  <data key="d1">person</data>
  <data key="d2">Jrgen Schmidhuber is an author recognized for his work in deep learning and neural networks.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Gabriel Ilharco">
  <data key="d0">Gabriel Ilharco</data>
  <data key="d1">person</data>
  <data key="d2">Gabriel Ilharco is a co-author in the research discussing model soups in machine learning.&lt;SEP&gt;Gabriel Ilharco is a researcher working on editing models and task arithmetic in machine learning contexts.&lt;SEP&gt;Gabriel Ilharco is an author contributing to research on editing models in AI.&lt;SEP&gt;Gabriel Ilharco is an author contributing to research on model soups in AI.&lt;SEP&gt;Gabriel Ilharco is involved in research on AI and natural language processing.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2&lt;SEP&gt;chunk-6dacb14670f826534e89e51fc52ea26f&lt;SEP&gt;chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36&lt;SEP&gt;chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Marco Tulio Ribeiro">
  <data key="d0">Marco Tulio Ribeiro</data>
  <data key="d1">person</data>
  <data key="d2">Marco Tulio Ribeiro is an author involved in research on model interpretability and editing in AI.&lt;SEP&gt;Marco Tulio Ribeiro is known for his work in interpretability and adversarial machine learning, contributing to model editing techniques.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Mitchell Wortsman">
  <data key="d0">Mitchell Wortsman</data>
  <data key="d1">person</data>
  <data key="d2">Mitchell Wortsman is a researcher contributing to AI and machine learning methodologies.&lt;SEP&gt;Mitchell Wortsman is a researcher focusing on adversarial techniques and model editing in machine learning.&lt;SEP&gt;Mitchell Wortsman is an author contributing to advancements in model editing techniques.&lt;SEP&gt;Mitchell Wortsman is an author who studied model averaging techniques in AI.&lt;SEP&gt;Mitchell Wortsman is an author of research related to model soups and fine-tuning techniques in machine learning.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2&lt;SEP&gt;chunk-6dacb14670f826534e89e51fc52ea26f&lt;SEP&gt;chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36&lt;SEP&gt;chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Suchin Gururangan">
  <data key="d0">Suchin Gururangan</data>
  <data key="d1">person</data>
  <data key="d2">Suchin Gururangan is a researcher involved in natural language processing and its applications in adversarial settings.&lt;SEP&gt;Suchin Gururangan is an author involved in research on AI model editing.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Ludwig Schmidt">
  <data key="d0">Ludwig Schmidt</data>
  <data key="d1">person</data>
  <data key="d2">Ludwig Schmidt is a researcher known for his work on machine learning interpretability and adversarial robustness.&lt;SEP&gt;Ludwig Schmidt is an author contributing to the research on model editing in AI.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Hannaneh Hajishirzi">
  <data key="d0">Hannaneh Hajishirzi</data>
  <data key="d1">person</data>
  <data key="d2">Hannaneh Hajishirzi is a prominent researcher in natural language processing, focusing on adversarial techniques and model evaluation.&lt;SEP&gt;Hannaneh Hajishirzi is an author known for her contributions to AI research and model interpretability.</data>
  <data key="d3">chunk-6dacb14670f826534e89e51fc52ea26f&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Ali Farhadi">
  <data key="d0">Ali Farhadi</data>
  <data key="d1">person</data>
  <data key="d2">Ali Farhadi is a prominent researcher known for his work in computer vision and AI.&lt;SEP&gt;Ali Farhadi is a researcher contributing to machine learning advancements, specifically in model training.&lt;SEP&gt;Ali Farhadi is a researcher known for contributions to computer vision and machine learning, particularly in the context of adversarial models.&lt;SEP&gt;Ali Farhadi is an author involved in research on AI models and their applications.&lt;SEP&gt;Ali Farhadi is an author known for contributions to AI and machine learning.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2&lt;SEP&gt;chunk-6dacb14670f826534e89e51fc52ea26f&lt;SEP&gt;chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36&lt;SEP&gt;chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Neural Information Processing Systems">
  <data key="d0">Neural Information Processing Systems</data>
  <data key="d1">event</data>
  <data key="d2">Neural Information Processing Systems is a prominent conference series focusing on advancements in machine learning and artificial intelligence.</data>
  <data key="d3">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Nitish Shirish Keskar">
  <data key="d0">Nitish Shirish Keskar</data>
  <data key="d1">person</data>
  <data key="d2">Nitish Shirish Keskar is a researcher known for his work in deep learning and has contributed to significant publications in the field.</data>
  <data key="d3">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Dheevatsa Mudigere">
  <data key="d0">Dheevatsa Mudigere</data>
  <data key="d1">person</data>
  <data key="d2">Dheevatsa Mudigere is a researcher who co-authored works on large-batch training in deep learning.</data>
  <data key="d3">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Jorge Nocedal">
  <data key="d0">Jorge Nocedal</data>
  <data key="d1">person</data>
  <data key="d2">Jorge Nocedal is a prominent figure in optimization and machine learning, contributing to various research papers.</data>
  <data key="d3">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Mikhail Smelyanskiy">
  <data key="d0">Mikhail Smelyanskiy</data>
  <data key="d1">person</data>
  <data key="d2">Mikhail Smelyanskiy is a researcher in AI, recognized for his contributions to advancements in deep learning.</data>
  <data key="d3">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Ping Tak Peter Tang">
  <data key="d0">Ping Tak Peter Tang</data>
  <data key="d1">person</data>
  <data key="d2">Ping Tak Peter Tang is a researcher who has contributed to the field of deep learning and optimization.</data>
  <data key="d3">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Maxime Labonne">
  <data key="d0">Maxime Labonne</data>
  <data key="d1">person</data>
  <data key="d2">Maxime Labonne is a researcher known for his work on merging models and has published several papers in the AI field.</data>
  <data key="d3">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="International Conference on Learning Representations">
  <data key="d0">International Conference on Learning Representations</data>
  <data key="d1">event</data>
  <data key="d2">The International Conference on Learning Representations is a major conference that focuses on the latest research in representation learning and deep learning.</data>
  <data key="d3">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Hugging Face Blog">
  <data key="d0">Hugging Face Blog</data>
  <data key="d1">organization</data>
  <data key="d2">Hugging Face Blog is a platform that shares insights and developments in natural language processing and machine learning.</data>
  <data key="d3">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="arXiv">
  <data key="d0">arXiv</data>
  <data key="d1">organization</data>
  <data key="d2">arXiv is a repository for preprints in various fields including computer science, where researchers share their findings before formal publication.&lt;SEP&gt;arXiv is a repository for research papers in various scientific fields, including AI and machine learning.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f&lt;SEP&gt;chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="CoRR">
  <data key="d0">CoRR</data>
  <data key="d1">organization</data>
  <data key="d2">CoRR (Computing Research Repository) is an online repository for computer science research papers.</data>
  <data key="d3">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="WizardMath">
  <data key="d0">WizardMath</data>
  <data key="d1">category</data>
  <data key="d2">WizardMath refers to a category of research focused on enhancing mathematical reasoning capabilities in large language models.</data>
  <data key="d3">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Large-Batch Training">
  <data key="d0">Large-Batch Training</data>
  <data key="d1">category</data>
  <data key="d2">Large-batch training is a technique in deep learning that focuses on optimizing the training process by using larger batches of data.</data>
  <data key="d3">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Henning Petzka">
  <data key="d0">Henning Petzka</data>
  <data key="d1">person</data>
  <data key="d2">Henning Petzka is one of the authors contributing to the research on relative flatness and generalization in neural networks.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Michael Kamp">
  <data key="d0">Michael Kamp</data>
  <data key="d1">person</data>
  <data key="d2">Michael Kamp is a co-author of the paper discussing relative flatness and generalization in neural networks.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Linara Adilova">
  <data key="d0">Linara Adilova</data>
  <data key="d1">person</data>
  <data key="d2">Linara Adilova is a researcher involved in the study of relative flatness and generalization in neural networks.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Cristian Sminchisescu">
  <data key="d0">Cristian Sminchisescu</data>
  <data key="d1">person</data>
  <data key="d2">Cristian Sminchisescu is a co-author of the paper on relative flatness and generalization in neural networks.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Mario Boley">
  <data key="d0">Mario Boley</data>
  <data key="d1">person</data>
  <data key="d2">Mario Boley is a researcher who contributed to the study on relative flatness and generalization in neural networks.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Advances in Neural Information Processing Systems">
  <data key="d0">Advances in Neural Information Processing Systems</data>
  <data key="d1">organization</data>
  <data key="d2">Advances in Neural Information Processing Systems is a conference where significant research in neural networks is presented.&lt;SEP&gt;Advances in Neural Information Processing Systems is a conference where significant research papers, including those on language models and reinforcement learning, are presented.&lt;SEP&gt;This is a leading conference where significant advancements in neural information processing are presented, including research on model merging techniques.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36&lt;SEP&gt;chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Colin Raffel">
  <data key="d0">Colin Raffel</data>
  <data key="d1">person</data>
  <data key="d2">Colin Raffel is an author advocating for building machine learning models similarly to open-source software.&lt;SEP&gt;Colin Raffel is an author contributing to research on AI model updates.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f&lt;SEP&gt;chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Eric Raymond">
  <data key="d0">Eric Raymond</data>
  <data key="d1">person</data>
  <data key="d2">Eric Raymond is known for his influential work 'The Cathedral and the Bazaar' discussing software development methodologies.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="LM Benchmark">
  <data key="d0">LM Benchmark</data>
  <data key="d1">organization</data>
  <data key="d2">LM Benchmark is a research initiative focused on evaluating language models.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Kigali">
  <data key="d0">Kigali</data>
  <data key="d1">geo</data>
  <data key="d2">Kigali is the capital city of Rwanda and the host location for the ICLR 2023 conference.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="The Eleventh International Conference on Learning Representations">
  <data key="d0">The Eleventh International Conference on Learning Representations</data>
  <data key="d1">event</data>
  <data key="d2">The Eleventh International Conference on Learning Representations is a significant event in the field of machine learning held in Kigali, Rwanda.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Freda Shi">
  <data key="d0">Freda Shi</data>
  <data key="d1">person</data>
  <data key="d2">Freda Shi is a researcher who contributed to the paper discussing multilingual chain-of-thought reasoning in language models.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Mirac Suzgun">
  <data key="d0">Mirac Suzgun</data>
  <data key="d1">person</data>
  <data key="d2">Mirac Suzgun is a co-author of the paper on multilingual chain-of-thought reasoning in language models.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Markus Freitag">
  <data key="d0">Markus Freitag</data>
  <data key="d1">person</data>
  <data key="d2">Markus Freitag is one of the authors who worked on language models as multilingual chain-of-thought reasoners.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Xuezhi Wang">
  <data key="d0">Xuezhi Wang</data>
  <data key="d1">person</data>
  <data key="d2">Xuezhi Wang is a researcher involved in the study of multilingual reasoning in language models.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Suraj Srivats">
  <data key="d0">Suraj Srivats</data>
  <data key="d1">person</data>
  <data key="d2">Suraj Srivats is a contributor to the research on multilingual chain-of-thought reasoning in language models.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Soroush Vosoughi">
  <data key="d0">Soroush Vosoughi</data>
  <data key="d1">person</data>
  <data key="d2">Soroush Vosoughi is a co-author of the paper discussing multilingual reasoning in language models.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Hyung Won Chung">
  <data key="d0">Hyung Won Chung</data>
  <data key="d1">person</data>
  <data key="d2">Hyung Won Chung is a researcher who contributed to the multilingual chain-of-thought reasoning study.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Yi Tay">
  <data key="d0">Yi Tay</data>
  <data key="d1">person</data>
  <data key="d2">Yi Tay is one of the authors involved in the research on multilingual reasoning in language models.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Sebastian Ruder">
  <data key="d0">Sebastian Ruder</data>
  <data key="d1">person</data>
  <data key="d2">Sebastian Ruder is a researcher who contributed to the study on multilingual chain-of-thought reasoning in language models.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Denny Zhou">
  <data key="d0">Denny Zhou</data>
  <data key="d1">person</data>
  <data key="d2">Denny Zhou is a co-author of the research paper on multilingual reasoning in language models.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Dipanjan Das">
  <data key="d0">Dipanjan Das</data>
  <data key="d1">person</data>
  <data key="d2">Dipanjan Das is a contributor to the research on multilingual chain-of-thought reasoning in language models.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Jason Wei">
  <data key="d0">Jason Wei</data>
  <data key="d1">person</data>
  <data key="d2">Jason Wei is a researcher involved in the study of multilingual reasoning in language models.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Proceedings of the AAAI Conference on Artificial Intelligence">
  <data key="d0">Proceedings of the AAAI Conference on Artificial Intelligence</data>
  <data key="d1">event</data>
  <data key="d2">Proceedings of the AAAI Conference on Artificial Intelligence is a collection of research presented at the AAAI conference.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition">
  <data key="d0">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</data>
  <data key="d1">event</data>
  <data key="d2">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition is a compilation of research papers presented at the conference.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="International Conference on Computational Linguistics">
  <data key="d0">International Conference on Computational Linguistics</data>
  <data key="d1">event</data>
  <data key="d2">International Conference on Computational Linguistics is a conference where significant research in computational linguistics is presented.</data>
  <data key="d3">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Kenneth O Stanley">
  <data key="d0">Kenneth O Stanley</data>
  <data key="d1">person</data>
  <data key="d2">Kenneth O Stanley is an author known for his work in evolutionary computation and neural networks.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Risto Miikkulainen">
  <data key="d0">Risto Miikkulainen</data>
  <data key="d1">person</data>
  <data key="d2">Risto Miikkulainen is an author recognized for contributions in the field of neural networks and evolutionary algorithms.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Marc Pickett">
  <data key="d0">Marc Pickett</data>
  <data key="d1">person</data>
  <data key="d2">Marc Pickett is an author involved in research related to transformer models.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Aakash Kumar Nain">
  <data key="d0">Aakash Kumar Nain</data>
  <data key="d1">person</data>
  <data key="d2">Aakash Kumar Nain is an author contributing to the study of transformer layers in AI.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Llion Jones">
  <data key="d0">Llion Jones</data>
  <data key="d1">person</data>
  <data key="d2">Llion Jones is an author who has worked on topics related to transformer models in AI.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Yi-Lin Sung">
  <data key="d0">Yi-Lin Sung</data>
  <data key="d1">person</data>
  <data key="d2">Yi-Lin Sung is an author focusing on multimodal model merging in AI research.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Linjie Li">
  <data key="d0">Linjie Li</data>
  <data key="d1">person</data>
  <data key="d2">Linjie Li is an author contributing to research in multimodal models.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Kevin Lin">
  <data key="d0">Kevin Lin</data>
  <data key="d1">person</data>
  <data key="d2">Kevin Lin is an author involved in the empirical study of model merging in AI.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Zhe Gan">
  <data key="d0">Zhe Gan</data>
  <data key="d1">person</data>
  <data key="d2">Zhe Gan is an author who has researched multimodal model merging.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Mohit Bansal">
  <data key="d0">Mohit Bansal</data>
  <data key="d1">person</data>
  <data key="d2">Mohit Bansal is an author known for contributions in AI and multimodal models.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Lijuan Wang">
  <data key="d0">Lijuan Wang</data>
  <data key="d1">person</data>
  <data key="d2">Lijuan Wang is an author who has worked on multimodal model research.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Yingtao Tian">
  <data key="d0">Yingtao Tian</data>
  <data key="d1">person</data>
  <data key="d2">Yingtao Tian is an author contributing to neuroevolution research.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Tom White">
  <data key="d0">Tom White</data>
  <data key="d1">person</data>
  <data key="d2">Tom White is an author who has researched generative networks and sampling methods.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Samir Ya Gadre">
  <data key="d0">Samir Ya Gadre</data>
  <data key="d1">person</data>
  <data key="d2">Samir Ya Gadre is a contributor to the research on model soups and their impact on accuracy in machine learning.&lt;SEP&gt;Samir Ya Gadre is a researcher working on advancements in machine learning and AI.&lt;SEP&gt;Samir Ya Gadre is an author involved in advancing AI model accuracy.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-144f32db3d945848c593404dfe014f4f&lt;SEP&gt;chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Rebecca Roelofs">
  <data key="d0">Rebecca Roelofs</data>
  <data key="d1">person</data>
  <data key="d2">Rebecca Roelofs is a researcher contributing to AI methodologies and applications.&lt;SEP&gt;Rebecca Roelofs is an author who has worked on model optimization techniques.&lt;SEP&gt;Rebecca Roelofs is involved in the research on averaging weights of multiple fine-tuned models.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-144f32db3d945848c593404dfe014f4f&lt;SEP&gt;chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Raphael Gontijo-Lopes">
  <data key="d0">Raphael Gontijo-Lopes</data>
  <data key="d1">person</data>
  <data key="d2">Raphael Gontijo-Lopes is a researcher who co-authored the paper on model soups in machine learning.&lt;SEP&gt;Raphael Gontijo-Lopes is an author focusing on AI model performance.&lt;SEP&gt;Raphael Gontijo-Lopes is involved in research related to AI and machine learning.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-144f32db3d945848c593404dfe014f4f&lt;SEP&gt;chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Ari S Morcos">
  <data key="d0">Ari S Morcos</data>
  <data key="d1">person</data>
  <data key="d2">Ari S Morcos is a contributor to the research on improving model accuracy through model soups.&lt;SEP&gt;Ari S Morcos is a researcher focusing on AI systems and methodologies.&lt;SEP&gt;Ari S Morcos is an author who has researched AI model efficiency.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-144f32db3d945848c593404dfe014f4f&lt;SEP&gt;chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Hongseok Namkoong">
  <data key="d0">Hongseok Namkoong</data>
  <data key="d1">person</data>
  <data key="d2">Hongseok Namkoong is a co-author of research discussing advancements in machine learning models.&lt;SEP&gt;Hongseok Namkoong is a researcher contributing to advancements in AI technologies.&lt;SEP&gt;Hongseok Namkoong is an author contributing to AI model studies.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-144f32db3d945848c593404dfe014f4f&lt;SEP&gt;chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Yair Carmon">
  <data key="d0">Yair Carmon</data>
  <data key="d1">person</data>
  <data key="d2">Yair Carmon is a co-author of the research paper on model soups and machine learning.&lt;SEP&gt;Yair Carmon is an author involved in AI research and model evaluation.&lt;SEP&gt;Yair Carmon is involved in AI research, particularly in model evaluation and training.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-144f32db3d945848c593404dfe014f4f&lt;SEP&gt;chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Simon Kornblith">
  <data key="d0">Simon Kornblith</data>
  <data key="d1">person</data>
  <data key="d2">Simon Kornblith is a researcher focused on AI and machine learning methodologies.&lt;SEP&gt;Simon Kornblith is an author contributing to AI model performance research.&lt;SEP&gt;Simon Kornblith is involved in research on improving machine learning model accuracy.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-144f32db3d945848c593404dfe014f4f&lt;SEP&gt;chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Prateek Yadav">
  <data key="d0">Prateek Yadav</data>
  <data key="d1">person</data>
  <data key="d2">Prateek Yadav is an author focused on compression techniques for AI models.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Leshem Choshen">
  <data key="d0">Leshem Choshen</data>
  <data key="d1">person</data>
  <data key="d2">Leshem Choshen is an author involved in parameter-efficient updates in AI.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Derek Tam">
  <data key="d0">Derek Tam</data>
  <data key="d1">person</data>
  <data key="d2">Derek Tam is an author who has studied model merging and interference.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Chao Li">
  <data key="d0">Chao Li</data>
  <data key="d1">person</data>
  <data key="d2">Chao Li is an author contributing to AI model research and development.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Chengen Huang">
  <data key="d0">Chengen Huang</data>
  <data key="d1">person</data>
  <data key="d2">Chengen Huang is an author involved in AI research.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Ge Zhang">
  <data key="d0">Ge Zhang</data>
  <data key="d1">person</data>
  <data key="d2">Ge Zhang is an author contributing to advancements in AI models.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Guanwei Zhang">
  <data key="d0">Guanwei Zhang</data>
  <data key="d1">person</data>
  <data key="d2">Guanwei Zhang is an author known for work in AI model efficiency.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Heng Li">
  <data key="d0">Heng Li</data>
  <data key="d1">person</data>
  <data key="d2">Heng Li is an author involved in AI model studies and advancements.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Jiangcheng Zhu">
  <data key="d0">Jiangcheng Zhu</data>
  <data key="d1">person</data>
  <data key="d2">Jiangcheng Zhu is an author contributing to AI research.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Jianqun Chen">
  <data key="d0">Jianqun Chen</data>
  <data key="d1">person</data>
  <data key="d2">Jianqun Chen is an author focused on AI model development.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Jing Chang">
  <data key="d0">Jing Chang</data>
  <data key="d1">person</data>
  <data key="d2">Jing Chang is an author contributing to AI model research.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="International Conference on Machine Learning">
  <data key="d0">International Conference on Machine Learning</data>
  <data key="d1">event</data>
  <data key="d2">The International Conference on Machine Learning is a prominent event where research on machine learning is presented and discussed.&lt;SEP&gt;The International Conference on Machine Learning is a prominent event where researchers present advancements in machine learning and AI.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</node>
<node id="Advances in Neural Information Processing Systems 36">
  <data key="d0">Advances in Neural Information Processing Systems 36</data>
  <data key="d1">event</data>
  <data key="d2">Advances in Neural Information Processing Systems 36 is a conference focusing on neural information processing and AI advancements.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Evolving Neural Networks Through Augmenting Topologies">
  <data key="d0">Evolving Neural Networks Through Augmenting Topologies</data>
  <data key="d1">event</data>
  <data key="d2">Evolving Neural Networks Through Augmenting Topologies is a significant paper discussing advancements in neural networks and evolutionary computation.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Transformer Layers as Painters">
  <data key="d0">Transformer Layers as Painters</data>
  <data key="d1">event</data>
  <data key="d2">Transformer Layers as Painters is a research paper analyzing the application of transformer layers in AI models.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="An Empirical Study of Multimodal Model Merging">
  <data key="d0">An Empirical Study of Multimodal Model Merging</data>
  <data key="d1">event</data>
  <data key="d2">An Empirical Study of Multimodal Model Merging is a research paper that explores techniques for merging different AI models effectively.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="EvoJAX: Hardware-Accelerated Neuroevolution">
  <data key="d0">EvoJAX: Hardware-Accelerated Neuroevolution</data>
  <data key="d1">event</data>
  <data key="d2">EvoJAX: Hardware-Accelerated Neuroevolution is a paper focusing on enhancing neuroevolution techniques using hardware acceleration.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Sampling Generative Networks">
  <data key="d0">Sampling Generative Networks</data>
  <data key="d1">event</data>
  <data key="d2">Sampling Generative Networks is a research paper that discusses methods for improving generative networks in AI.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Model Soups">
  <data key="d0">Model Soups</data>
  <data key="d1">event</data>
  <data key="d2">Model Soups is a paper that examines the concept of averaging weights from multiple fine-tuned AI models to enhance accuracy.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Compeft">
  <data key="d0">Compeft</data>
  <data key="d1">event</data>
  <data key="d2">Compeft is a research paper that presents techniques for communicating parameter-efficient updates in AI models.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Yi: Open Foundation Models by 01.ai">
  <data key="d0">Yi: Open Foundation Models by 01.ai</data>
  <data key="d1">event</data>
  <data key="d2">Yi: Open Foundation Models by 01.ai is a research paper that discusses the development of open foundation models in AI.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Language Models are Super Mario">
  <data key="d0">Language Models are Super Mario</data>
  <data key="d1">event</data>
  <data key="d2">Language Models are Super Mario is a paper exploring how language models can absorb capabilities from similar models.</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Stability AI Japan">
  <data key="d0">Stability AI Japan</data>
  <data key="d1">organization</data>
  <data key="d2">Stability AI Japan is an organization that has developed a fork of the lm-eval-harness for evaluating language models.</data>
  <data key="d3">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="EvoLLM-JP">
  <data key="d0">EvoLLM-JP</data>
  <data key="d1">category</data>
  <data key="d2">EvoLLM-JP is a language model developed by merging various models to improve Japanese language understanding and performance.</data>
  <data key="d3">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="EvoLLM-JP-A">
  <data key="d0">EvoLLM-JP-A</data>
  <data key="d1">category</data>
  <data key="d2">EvoLLM-JP-A is a model derived from merging open-source models, released under the Apache 2.0 license.</data>
  <data key="d3">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Setsubun">
  <data key="d0">Setsubun</data>
  <data key="d1">event</data>
  <data key="d2">Setsubun is a Japanese cultural event that marks the day before the beginning of spring (Risshun).&lt;SEP&gt;Setsubun is a traditional Japanese event marking the beginning of spring, where people throw beans to drive away evil spirits and invite good fortune.&lt;SEP&gt;Setsubun is a traditional Japanese event that marks the eve of the first day of spring, celebrated with rituals to drive away evil spirits.&lt;SEP&gt;Setsubun is a traditional Japanese festival that marks the day before the beginning of spring, celebrated with rituals to drive away evil spirits.</data>
  <data key="d3">chunk-19ca32b4278ab8247ef043d83b7b65ef&lt;SEP&gt;chunk-96aa56aec4e4f18a590f28efaff42c99&lt;SEP&gt;chunk-57491ddb5284500415bc46fc834a6343&lt;SEP&gt;chunk-62a104ca37857a0566f04bc4cd794e8c</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="lm-eval-harness">
  <data key="d0">lm-eval-harness</data>
  <data key="d1">category</data>
  <data key="d2">lm-eval-harness is a framework used for evaluating the performance of language models, particularly in the context of Japanese language models.</data>
  <data key="d3">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Rinna">
  <data key="d0">Rinna</data>
  <data key="d1">organization</data>
  <data key="d2">Rinna is an organization that maintains leaderboards for language models, allowing for performance comparisons.</data>
  <data key="d3">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Japanese Language Model Evaluation Harness">
  <data key="d0">Japanese Language Model Evaluation Harness</data>
  <data key="d1">category</data>
  <data key="d2">Japanese Language Model Evaluation Harness is a benchmark suite consisting of tasks to evaluate Japanese language proficiency.</data>
  <data key="d3">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Llama-2-13b-hf">
  <data key="d0">Llama-2-13b-hf</data>
  <data key="d1">organization</data>
  <data key="d2">Llama-2-13b-hf is a 13B source model recognized for its performance metrics.</data>
  <data key="d3">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Swallow-70b-instruct-hf">
  <data key="d0">Swallow-70b-instruct-hf</data>
  <data key="d1">organization</data>
  <data key="d2">Swallow-70b-instruct-hf is a 70B model that demonstrates high performance metrics across various categories.&lt;SEP&gt;Swallow-70b-instruct-hf is a model designed for instruction tasks, showcasing high performance across various metrics.&lt;SEP&gt;Swallow-70b-instruct-hf is a model that demonstrates high performance metrics in various tasks.</data>
  <data key="d3">chunk-9644ad737be96b755c1c9bdea916b724&lt;SEP&gt;chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="7B source models">
  <data key="d0">7B source models</data>
  <data key="d1">category</data>
  <data key="d2">7B source models refer to a category of AI models characterized by their performance metrics and capabilities.</data>
  <data key="d3">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="7B merged models">
  <data key="d0">7B merged models</data>
  <data key="d1">category</data>
  <data key="d2">7B merged models encompass models that combine various 7B source models to enhance performance.</data>
  <data key="d3">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="13B source models">
  <data key="d0">13B source models</data>
  <data key="d1">category</data>
  <data key="d2">13B source models are a category of AI models distinguished by their larger size and performance metrics.</data>
  <data key="d3">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="13B merged models">
  <data key="d0">13B merged models</data>
  <data key="d1">category</data>
  <data key="d2">13B merged models consist of models that integrate multiple 13B source models for improved performance.</data>
  <data key="d3">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Swallow-70b-hf">
  <data key="d0">Swallow-70b-hf</data>
  <data key="d1">organization</data>
  <data key="d2">Swallow-70b-hf is a model version that showcases competitive performance scores.&lt;SEP&gt;Swallow-70b-hf is a variant model that demonstrates competitive performance in several evaluations.</data>
  <data key="d3">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="japanese-stablelm-base-beta-70b">
  <data key="d0">japanese-stablelm-base-beta-70b</data>
  <data key="d1">organization</data>
  <data key="d2">japanese-stablelm-base-beta-70b is a model that combines features from multiple configurations for effective performance.&lt;SEP&gt;japanese-stablelm-base-beta-70b is a model that combines features from multiple configurations to achieve effective performance.</data>
  <data key="d3">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="nekomata-14b-instruction">
  <data key="d0">nekomata-14b-instruction</data>
  <data key="d1">organization</data>
  <data key="d2">nekomata-14b-instruction is a model focused on instruction-based tasks, yielding impressive performance metrics.&lt;SEP&gt;nekomata-14b-instruction is a model with specific instruction capabilities, yielding impressive performance metrics.</data>
  <data key="d3">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="japanese-stablelm-instruct-beta-70b">
  <data key="d0">japanese-stablelm-instruct-beta-70b</data>
  <data key="d1">organization</data>
  <data key="d2">japanese-stablelm-instruct-beta-70b is an advanced model designed for instruction-based tasks.&lt;SEP&gt;japanese-stablelm-instruct-beta-70b is an advanced model tailored for instruction tasks, demonstrating high effectiveness.</data>
  <data key="d3">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="nekomata-14b">
  <data key="d0">nekomata-14b</data>
  <data key="d1">organization</data>
  <data key="d2">nekomata-14b is a model that provides strong performance across various benchmarks and tasks.&lt;SEP&gt;nekomata-14b is a model that provides strong performance across various benchmarks.</data>
  <data key="d3">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="youri-7b-chat">
  <data key="d0">youri-7b-chat</data>
  <data key="d1">organization</data>
  <data key="d2">youri-7b-chat is a conversational model optimized for chat interactions, exhibiting high engagement.&lt;SEP&gt;youri-7b-chat is a conversational model optimized for chat interactions.</data>
  <data key="d3">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Llama-2-70b-hf">
  <data key="d0">Llama-2-70b-hf</data>
  <data key="d1">organization</data>
  <data key="d2">Llama-2-70b-hf is a model that integrates various capabilities for enhanced performance across tasks.&lt;SEP&gt;Llama-2-70b-hf is a model that integrates various capabilities for high performance.</data>
  <data key="d3">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="youri-7b-instruction">
  <data key="d0">youri-7b-instruction</data>
  <data key="d1">organization</data>
  <data key="d2">youri-7b-instruction is a model specifically designed for instruction-based tasks, achieving notable performance scores.&lt;SEP&gt;youri-7b-instruction is a model tailored for instruction-based tasks with notable performance scores.</data>
  <data key="d3">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Qwen-14B">
  <data key="d0">Qwen-14B</data>
  <data key="d1">organization</data>
  <data key="d2">Qwen-14B is a model that demonstrates competitive performance metrics in various tasks.&lt;SEP&gt;Qwen-14B is a model that showcases competitive performance metrics across different evaluations and tasks.</data>
  <data key="d3">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Swallow-MX-8x7b-NVE-v0.1">
  <data key="d0">Swallow-MX-8x7b-NVE-v0.1</data>
  <data key="d1">organization</data>
  <data key="d2">Swallow-MX-8x7b-NVE-v0.1 is a model that highlights high performance in specific evaluations and tasks.&lt;SEP&gt;Swallow-MX-8x7b-NVE-v0.1 is a model that showcases high performance in specific evaluations.</data>
  <data key="d3">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="youri-7b-chat-gptq">
  <data key="d0">youri-7b-chat-gptq</data>
  <data key="d1">organization</data>
  <data key="d2">youri-7b-chat-gptq is a variant of the youri model optimized for chat interactions.&lt;SEP&gt;youri-7b-chat-gptq is an enhanced variant of the youri model optimized for chat interactions with advanced capabilities.</data>
  <data key="d3">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="nekomata-14b-instruction-gguf">
  <data key="d0">nekomata-14b-instruction-gguf</data>
  <data key="d1">organization</data>
  <data key="d2">nekomata-14b-instruction-gguf is an enhanced model version focusing on instruction tasks.&lt;SEP&gt;nekomata-14b-instruction-gguf is an upgraded model focusing on instruction tasks with impressive performance metrics.</data>
  <data key="d3">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Llama-2-70b-chat-hf">
  <data key="d0">Llama-2-70b-chat-hf</data>
  <data key="d1">organization</data>
  <data key="d2">Llama-2-70b-chat-hf is a conversational model designed for effective chat interactions.&lt;SEP&gt;Llama-2-70b-chat-hf is a conversational model tailored for effective chat interactions, demonstrating high engagement.&lt;SEP&gt;Llama-2-70b-chat-hf is a large language model developed with 70 billion parameters, known for its performance in conversational AI and natural language processing tasks.&lt;SEP&gt;Llama-2-70b-chat-hf is a large language model with 70 billion parameters, known for its high performance in various natural language processing tasks.</data>
  <data key="d3">chunk-9644ad737be96b755c1c9bdea916b724&lt;SEP&gt;chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="youri-7b-instruction-gptq">
  <data key="d0">youri-7b-instruction-gptq</data>
  <data key="d1">organization</data>
  <data key="d2">youri-7b-instruction-gptq is a language model with 7 billion parameters, designed for instruction-based tasks with notable accuracy.&lt;SEP&gt;youri-7b-instruction-gptq is a language model with 7 billion parameters, optimized for instructional tasks, demonstrating high accuracy in following user prompts.</data>
  <data key="d3">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="japanese-stablelm-base-gamma-7b">
  <data key="d0">japanese-stablelm-base-gamma-7b</data>
  <data key="d1">organization</data>
  <data key="d2">japanese-stablelm-base-gamma-7b is a Japanese language model with 7 billion parameters, optimized for language understanding and generation tasks.&lt;SEP&gt;japanese-stablelm-base-gamma-7b is a language model with 7 billion parameters, specifically designed for understanding and generating Japanese text.</data>
  <data key="d3">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Swallow-13b-instruct-hf">
  <data key="d0">Swallow-13b-instruct-hf</data>
  <data key="d1">organization</data>
  <data key="d2">Swallow-13b-instruct-hf is a hybrid language model with 13 billion parameters, integrating instruction-following capabilities for enhanced usability in applications.&lt;SEP&gt;Swallow-13b-instruct-hf is a hybrid model combining 13 billion parameters and instruction-following capabilities, enhancing its usability in various applications.</data>
  <data key="d3">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="nekomata-14b-gguf">
  <data key="d0">nekomata-14b-gguf</data>
  <data key="d1">organization</data>
  <data key="d2">nekomata-14b-gguf is a language model with 14 billion parameters, recognized for its ability to generate coherent and contextually relevant text.&lt;SEP&gt;nekomata-14b-gguf is a language model with 14 billion parameters, recognized for its performance in generating coherent text.</data>
  <data key="d3">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Swallow-MS-7b-v0.1">
  <data key="d0">Swallow-MS-7b-v0.1</data>
  <data key="d1">organization</data>
  <data key="d2">Swallow-MS-7b-v0.1 is a model with 7 billion parameters, focused on multi-task learning and instruction following.&lt;SEP&gt;Swallow-MS-7b-v0.1 is a multi-task language model with 7 billion parameters, designed for instruction following and various natural language tasks.</data>
  <data key="d3">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Swallow-7b-instruct-hf">
  <data key="d0">Swallow-7b-instruct-hf</data>
  <data key="d1">organization</data>
  <data key="d2">Swallow-7b-instruct-hf is a language model with 7 billion parameters, designed for instruction-based interactions.&lt;SEP&gt;Swallow-7b-instruct-hf is a language model with 7 billion parameters, focused on instruction-based interactions to improve user experience.</data>
  <data key="d3">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="japanese-stablelm-instruct-beta-7b">
  <data key="d0">japanese-stablelm-instruct-beta-7b</data>
  <data key="d1">organization</data>
  <data key="d2">japanese-stablelm-instruct-beta-7b is a Japanese language model with 7 billion parameters, aimed at improving instruction-based tasks.&lt;SEP&gt;japanese-stablelm-instruct-beta-7b is a language model with 7 billion parameters, aimed at improving performance in instruction-based tasks in Japanese.</data>
  <data key="d3">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Qwen-7B">
  <data key="d0">Qwen-7B</data>
  <data key="d1">organization</data>
  <data key="d2">Qwen-7B is a language model with 7 billion parameters, known for its efficiency and performance in diverse language tasks.&lt;SEP&gt;Qwen-7B is a language model with 7 billion parameters, known for its efficiency in executing diverse language tasks and applications.</data>
  <data key="d3">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="youri-7b-gptq">
  <data key="d0">youri-7b-gptq</data>
  <data key="d1">organization</data>
  <data key="d2">youri-7b-gptq is a 7 billion parameter model designed for generating text based on prompts.&lt;SEP&gt;youri-7b-gptq is a 7 billion parameter model that generates text based on user prompts, showcasing capabilities in conversational AI.&lt;SEP&gt;youri-7b-gptq is a model characterized by its performance metrics, including various scores across different categories.</data>
  <data key="d3">chunk-de4fd08ec74cc6d0a33238a38b1bdb52&lt;SEP&gt;chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="youri-7b">
  <data key="d0">youri-7b</data>
  <data key="d1">organization</data>
  <data key="d2">youri-7b is a language model with 7 billion parameters, utilized for various natural language processing applications and tasks.&lt;SEP&gt;youri-7b is a language model with 7 billion parameters, utilized for various natural language processing applications.&lt;SEP&gt;youri-7b is a model with performance metrics that indicate its capabilities in various tasks.</data>
  <data key="d3">chunk-de4fd08ec74cc6d0a33238a38b1bdb52&lt;SEP&gt;chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0">
  <data key="d0">llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0</data>
  <data key="d1">organization</data>
  <data key="d2">llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0 is a 13 billion parameter model designed for instruction-following tasks in Japanese language processing.</data>
  <data key="d3">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="ELYZA-japanese-Llama-2-7b-instruct">
  <data key="d0">ELYZA-japanese-Llama-2-7b-instruct</data>
  <data key="d1">organization</data>
  <data key="d2">ELYZA-japanese-Llama-2-7b-instruct is a model known for its instruction-based performance across multiple metrics.</data>
  <data key="d3">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="ELYZA-japanese-Llama-2-7b">
  <data key="d0">ELYZA-japanese-Llama-2-7b</data>
  <data key="d1">organization</data>
  <data key="d2">ELYZA-japanese-Llama-2-7b is a model with specific performance scores that highlight its capabilities.</data>
  <data key="d3">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="nekomata-7b-gguf">
  <data key="d0">nekomata-7b-gguf</data>
  <data key="d1">organization</data>
  <data key="d2">nekomata-7b-gguf is a model with performance metrics that reflect its efficiency and effectiveness.</data>
  <data key="d3">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="japanese-stablelm-instruct-ja_vocab-beta-7b">
  <data key="d0">japanese-stablelm-instruct-ja_vocab-beta-7b</data>
  <data key="d1">organization</data>
  <data key="d2">japanese-stablelm-instruct-ja_vocab-beta-7b is a model designed for instruction tasks with notable performance scores.</data>
  <data key="d3">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="japanese-stablelm-base-ja_vocab-beta-7b">
  <data key="d0">japanese-stablelm-base-ja_vocab-beta-7b</data>
  <data key="d1">organization</data>
  <data key="d2">japanese-stablelm-base-ja_vocab-beta-7b is a model that exhibits performance across various parameters.</data>
  <data key="d3">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="stockmark-13b">
  <data key="d0">stockmark-13b</data>
  <data key="d1">organization</data>
  <data key="d2">stockmark-13b is a model characterized by its performance metrics in various categories.</data>
  <data key="d3">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Llama-2-7b-hf">
  <data key="d0">Llama-2-7b-hf</data>
  <data key="d1">organization</data>
  <data key="d2">Llama-2-7b-hf is a model noted for its performance scores in different tasks.</data>
  <data key="d3">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="calm2-7b">
  <data key="d0">calm2-7b</data>
  <data key="d1">organization</data>
  <data key="d2">calm2-7b is a model with performance metrics that indicate its capabilities in various areas.</data>
  <data key="d3">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="plamo-13b">
  <data key="d0">plamo-13b</data>
  <data key="d1">organization</data>
  <data key="d2">plamo-13b is a model with performance scores across several categories, indicating its capabilities.</data>
  <data key="d3">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="llm-jp-13b-v1.0">
  <data key="d0">llm-jp-13b-v1.0</data>
  <data key="d1">organization</data>
  <data key="d2">llm-jp-13b-v1.0 is a model characterized by its performance metrics.</data>
  <data key="d3">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="plamo-13b-instruct-nc">
  <data key="d0">plamo-13b-instruct-nc</data>
  <data key="d1">organization</data>
  <data key="d2">plamo-13b-instruct-nc is a model designed for instruction tasks with specific performance metrics.</data>
  <data key="d3">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="plamo-13b-instruct">
  <data key="d0">plamo-13b-instruct</data>
  <data key="d1">organization</data>
  <data key="d2">plamo-13b-instruct is a model with performance metrics that reflect its capabilities in instruction tasks.</data>
  <data key="d3">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="EvoLLM-JP-v1-7B">
  <data key="d0">EvoLLM-JP-v1-7B</data>
  <data key="d1">organization</data>
  <data key="d2">EvoLLM-JP-v1-7B is a language model that was tested for its ability to solve mathematical problems and understand cultural nuances in Japanese.</data>
  <data key="d3">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Koi-nobori">
  <data key="d0">Koi-nobori</data>
  <data key="d1">category</data>
  <data key="d2">Koi-nobori is a cultural tradition in Japan involving carp streamers that signify the celebration of Children's Day.</data>
  <data key="d3">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Japanese Culture">
  <data key="d0">Japanese Culture</data>
  <data key="d1">category</data>
  <data key="d2">Japanese Culture encompasses various traditions, customs, and practices unique to Japan, influencing language and understanding.</data>
  <data key="d3">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Traffic Lights">
  <data key="d0">Traffic Lights</data>
  <data key="d1">category</data>
  <data key="d2">Traffic Lights are signaling devices positioned at road intersections to control vehicle and pedestrian traffic.</data>
  <data key="d3">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Risshun">
  <data key="d0">Risshun</data>
  <data key="d1">event</data>
  <data key="d2">Risshun is the beginning of spring according to the lunisolar Japanese calendar, which coincides with Setsubun.&lt;SEP&gt;Risshun refers to the first day of spring in the Japanese lunar calendar, which varies each year.</data>
  <data key="d3">chunk-19ca32b4278ab8247ef043d83b7b65ef&lt;SEP&gt;chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Japanese-Stable-VLM">
  <data key="d0">Japanese-Stable-VLM</data>
  <data key="d1">organization</data>
  <data key="d2">Japanese-Stable-VLM is a language model that demonstrates knowledge of Japanese culture and language nuances.</data>
  <data key="d3">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="EvoVLM-JP">
  <data key="d0">EvoVLM-JP</data>
  <data key="d1">organization</data>
  <data key="d2">EvoVLM-JP is a language model that shows fluency in Japanese expression and cultural understanding.</data>
  <data key="d3">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="MGSM Test Set">
  <data key="d0">MGSM Test Set</data>
  <data key="d1">category</data>
  <data key="d2">MGSM Test Set is a collection of questions used to evaluate the performance of language models in solving mathematical problems.</data>
  <data key="d3">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="February">
  <data key="d0">February</data>
  <data key="d1">geo</data>
  <data key="d2">February is the second month of the year, during which Setsubun is celebrated.</data>
  <data key="d3">chunk-19ca32b4278ab8247ef043d83b7b65ef</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="2021">
  <data key="d0">2021</data>
  <data key="d1">category</data>
  <data key="d2">2021 is the year when last year's Setsubun was celebrated on February 3.</data>
  <data key="d3">chunk-19ca32b4278ab8247ef043d83b7b65ef</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="2022">
  <data key="d0">2022</data>
  <data key="d1">category</data>
  <data key="d2">2022 is the year when this year's Setsubun is celebrated on February 4.</data>
  <data key="d3">chunk-19ca32b4278ab8247ef043d83b7b65ef</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="2023">
  <data key="d0">2023</data>
  <data key="d1">category</data>
  <data key="d2">2023 is the year when the combined date of last year's and this year's Setsubun is calculated to be February 3.</data>
  <data key="d3">chunk-19ca32b4278ab8247ef043d83b7b65ef</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Last Year's Setsubun">
  <data key="d0">Last Year's Setsubun</data>
  <data key="d1">event</data>
  <data key="d2">Last year's Setsubun was celebrated on February 2, 2021, marking the end of winter and the beginning of spring.</data>
  <data key="d3">chunk-19ca32b4278ab8247ef043d83b7b65ef</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="This Year's Setsubun">
  <data key="d0">This Year's Setsubun</data>
  <data key="d1">event</data>
  <data key="d2">This year's Setsubun is celebrated on February 3, 2022, as part of the same annual tradition.</data>
  <data key="d3">chunk-19ca32b4278ab8247ef043d83b7b65ef</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="John">
  <data key="d0">John</data>
  <data key="d1">person</data>
  <data key="d2">John is a character who possesses three boxes and calculates their total inner volume.&lt;SEP&gt;John is a character who possesses three boxes with specific dimensions and is involved in a mathematical problem regarding their volume.</data>
  <data key="d3">chunk-30dbd5c2c163db07de6f5e159458d713&lt;SEP&gt;chunk-62a104ca37857a0566f04bc4cd794e8c</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Boxes">
  <data key="d0">Boxes</data>
  <data key="d1">category</data>
  <data key="d2">The boxes are categorized as containers with dimensions of 5 inches by 6 inches by 4 inches, with walls that are 1 inch thick.&lt;SEP&gt;The boxes are three containers with specific dimensions that John is measuring for volume.</data>
  <data key="d3">chunk-30dbd5c2c163db07de6f5e159458d713&lt;SEP&gt;chunk-62a104ca37857a0566f04bc4cd794e8c</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Volume Calculation">
  <data key="d0">Volume Calculation</data>
  <data key="d1">event</data>
  <data key="d2">Volume Calculation refers to the mathematical process of determining the total inner volume of the boxes, considering their dimensions and wall thickness.</data>
  <data key="d3">chunk-62a104ca37857a0566f04bc4cd794e8c</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Volume">
  <data key="d0">Volume</data>
  <data key="d1">category</data>
  <data key="d2">Volume is a mathematical concept that refers to the amount of space that a substance or object occupies, measured in cubic units.</data>
  <data key="d3">chunk-62a104ca37857a0566f04bc4cd794e8c</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Cubic Inches">
  <data key="d0">Cubic Inches</data>
  <data key="d1">category</data>
  <data key="d2">Cubic inches is a unit of measurement for volume, commonly used in the United States to measure the capacity of containers.</data>
  <data key="d3">chunk-62a104ca37857a0566f04bc4cd794e8c</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Dimensions">
  <data key="d0">Dimensions</data>
  <data key="d1">category</data>
  <data key="d2">Dimensions refer to the measurements of an object, including length, width, and height, which are essential for calculating volume.&lt;SEP&gt;Dimensions refer to the measurements of length, width, and height that define the size of the box.</data>
  <data key="d3">chunk-30dbd5c2c163db07de6f5e159458d713&lt;SEP&gt;chunk-62a104ca37857a0566f04bc4cd794e8c</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Wall Thickness">
  <data key="d0">Wall Thickness</data>
  <data key="d1">category</data>
  <data key="d2">Wall Thickness refers to the measurement of the thickness of the walls of the boxes, which affects the internal volume calculation.</data>
  <data key="d3">chunk-62a104ca37857a0566f04bc4cd794e8c</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="72 cubic inches">
  <data key="d0">72 cubic inches</data>
  <data key="d1">event</data>
  <data key="d2">The total inner volume calculated by John for the three boxes is 72 cubic inches.</data>
  <data key="d3">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Box">
  <data key="d0">Box</data>
  <data key="d1">category</data>
  <data key="d2">A box is a three-dimensional container used for storage or transportation, in this case, having specific dimensions and wall thickness.</data>
  <data key="d3">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Inner Volume">
  <data key="d0">Inner Volume</data>
  <data key="d1">category</data>
  <data key="d2">Inner volume is the space inside the box calculated by multiplying its internal dimensions.</data>
  <data key="d3">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="1 inch">
  <data key="d0">1 inch</data>
  <data key="d1">category</data>
  <data key="d2">1 inch is a unit of measurement used to denote the thickness of the walls of the box.</data>
  <data key="d3">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="5 inches">
  <data key="d0">5 inches</data>
  <data key="d1">category</data>
  <data key="d2">5 inches is the original length measurement of the box before accounting for wall thickness.</data>
  <data key="d3">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="6 inches">
  <data key="d0">6 inches</data>
  <data key="d1">category</data>
  <data key="d2">6 inches is the original width measurement of the box before accounting for wall thickness.</data>
  <data key="d3">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="4 inches">
  <data key="d0">4 inches</data>
  <data key="d1">category</data>
  <data key="d2">4 inches is the original height measurement of the box before accounting for wall thickness.</data>
  <data key="d3">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="3 boxes">
  <data key="d0">3 boxes</data>
  <data key="d1">category</data>
  <data key="d2">Three boxes are referenced in the text, indicating the total quantity John is measuring for volume.</data>
  <data key="d3">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Peace Tower">
  <data key="d0">Peace Tower</data>
  <data key="d1">organization</data>
  <data key="d2">The Peace Tower is a building located in Shibuya Ward, Tokyo, Japan, symbolizing peace and visited by many for prayer.</data>
  <data key="d3">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="August 15, 1945">
  <data key="d0">August 15, 1945</data>
  <data key="d1">event</data>
  <data key="d2">August 15, 1945, marks the day when the Peace Tower was destroyed during World War II by the Japanese military.</data>
  <data key="d3">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="1964">
  <data key="d0">1964</data>
  <data key="d1">event</data>
  <data key="d2">The year 1964 signifies when the Peace Tower was reconstructed after its destruction during World War II.</data>
  <data key="d3">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="World War II">
  <data key="d0">World War II</data>
  <data key="d1">category</data>
  <data key="d2">World War II was a global conflict that involved many countries and resulted in significant historical events, including the atomic bombings of Hiroshima and Nagasaki.&lt;SEP&gt;World War II was a global conflict that lasted from 1939 to 1945, significantly impacting many nations, including Japan.</data>
  <data key="d3">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Hiroshima">
  <data key="d0">Hiroshima</data>
  <data key="d1">geo</data>
  <data key="d2">Hiroshima is a city in Japan that was the target of an atomic bomb during World War II and is known for its historical significance.</data>
  <data key="d3">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Atomic Bomb Dome">
  <data key="d0">Atomic Bomb Dome</data>
  <data key="d1">organization</data>
  <data key="d2">The Atomic Bomb Dome is a historical building in Hiroshima, Japan, that remains as a symbol of the destruction caused by the atomic bomb during World War II.</data>
  <data key="d3">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Hiroshima Prefectural Industrial Promotion Hall">
  <data key="d0">Hiroshima Prefectural Industrial Promotion Hall</data>
  <data key="d1">organization</data>
  <data key="d2">The Hiroshima Prefectural Industrial Promotion Hall is the original name of the building now known as the Atomic Bomb Dome, which was damaged during the atomic bombing.</data>
  <data key="d3">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="August 6, 1945">
  <data key="d0">August 6, 1945</data>
  <data key="d1">event</data>
  <data key="d2">August 6, 1945, is the date when an atomic bomb was dropped on Hiroshima, leading to widespread destruction and loss of life.</data>
  <data key="d3">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="UNESCO World Heritage Site">
  <data key="d0">UNESCO World Heritage Site</data>
  <data key="d1">category</data>
  <data key="d2">A UNESCO World Heritage Site is a landmark or area with legal protection by an international convention, recognized for its cultural or historical significance, such as the Atomic Bomb Dome.</data>
  <data key="d3">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="fine-tuning">
  <data key="d0">fine-tuning</data>
  <data key="d3">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d2">Fine-tuning often led to significant decreases in JP-LMEH scores, indicating its impact on model performance.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Alex Young">
  <data key="d0">Alex Young</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d2">Alex Young is an author of Yi: Open Foundation Models by 01.ai, indicating his contribution to the research.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Le Yu">
  <data key="d0">Le Yu</data>
  <data key="d3">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d2">Le Yu is an author of Language Models are Super Mario, indicating his involvement in the research.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">evolutionary_optimization_model_merging.txt</data>
</node>
<node id="Yue Zhou">
  <data key="d0">Yue Zhou</data>
  <data key="d1">person</data>
  <data key="d2">Yue Zhou is a researcher affiliated with the School of Artificial Intelligence at Jilin University, contributing to advancements in model merging techniques.</data>
  <data key="d3">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Yi Chang">
  <data key="d0">Yi Chang</data>
  <data key="d1">person</data>
  <data key="d2">Yi Chang is a contributor to the research on data selection at scale in machine learning.&lt;SEP&gt;Yi Chang is a researcher associated with the Engineering Research Center of Knowledge-Driven Human-Machine Intelligence, contributing to the study of model merging.&lt;SEP&gt;Yi Chang is a researcher working on machine learning and natural language processing, contributing to the development of evaluation datasets.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-3df998a85b2dff618feb2e2ce206b62c&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Yuan Wu">
  <data key="d0">Yuan Wu</data>
  <data key="d1">person</data>
  <data key="d2">Yuan Wu is a co-author of the paper discussing bias-alleviating low-rank adaptation.&lt;SEP&gt;Yuan Wu is a researcher engaged in the evaluation of large language models, particularly in health-related applications.&lt;SEP&gt;Yuan Wu is a researcher involved in the study of data selection methods in machine learning.&lt;SEP&gt;Yuan Wu is a researcher involved in the study of model merging and is affiliated with Jilin University.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-3df998a85b2dff618feb2e2ce206b62c&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36&lt;SEP&gt;chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jilin University">
  <data key="d0">Jilin University</data>
  <data key="d1">organization</data>
  <data key="d2">Jilin University is an educational institution in China, known for its research in artificial intelligence and related fields.</data>
  <data key="d3">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Engineering Research Center of Knowledge-Driven Human-Machine Intelligence">
  <data key="d0">Engineering Research Center of Knowledge-Driven Human-Machine Intelligence</data>
  <data key="d1">organization</data>
  <data key="d2">This center focuses on research integrating human and machine intelligence, contributing to advancements in AI technologies.</data>
  <data key="d3">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="International Center of Future Science">
  <data key="d0">International Center of Future Science</data>
  <data key="d1">organization</data>
  <data key="d2">The International Center of Future Science at Jilin University is dedicated to innovative research in various scientific domains.</data>
  <data key="d3">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Mixup Model Merge (M3)">
  <data key="d0">Mixup Model Merge (M3)</data>
  <data key="d1">category</data>
  <data key="d2">Mixup Model Merge (M3) is a novel method for merging the parameters of large language models, enhancing their performance and robustness.</data>
  <data key="d3">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Natural Language Processing (NLP)">
  <data key="d0">Natural Language Processing (NLP)</data>
  <data key="d1">category</data>
  <data key="d2">NLP is a field of artificial intelligence that focuses on the interaction between computers and human language, utilizing large language models for various tasks.</data>
  <data key="d3">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Large Language Models (LLMs)">
  <data key="d0">Large Language Models (LLMs)</data>
  <data key="d1">category</data>
  <data key="d2">LLMs are advanced AI models that have shown significant capabilities in understanding and generating human language.</data>
  <data key="d3">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Supervised Fine-Tuning (SFT)">
  <data key="d0">Supervised Fine-Tuning (SFT)</data>
  <data key="d1">category</data>
  <data key="d2">Supervised Fine-Tuning (SFT) is a technique used to adapt large language models to specific tasks through training on domain-specific data.</data>
  <data key="d3">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="M&lt;sup&gt;3&lt;/sup&gt;">
  <data key="d0">M&lt;sup&gt;3&lt;/sup&gt;</data>
  <data key="d1">category</data>
  <data key="d2">M&lt;sup&gt;3&lt;/sup&gt; is a method for merging language models by interpolating their parameters, enhancing their performance on various tasks.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; is a method that combines parameters from different models through linear interpolation to enhance performance across various tasks.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; is a model enhancement strategy that improves performance across various tasks and merging strategies.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; is a model merging technique integrated into various merging strategies for fine-tuned LLMs.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; is a novel approach for merging fine-tuned language models by introducing randomness into the parameter linear interpolation process.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; is a novel approach for merging fine-tuned language models, enhancing performance through dynamic parameter adjustment.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; is a proposed approach for improving the performance of merged models in machine learning, specifically addressing issues related to merging ratios and fine-tuning.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; is a proposed method aimed at improving the out-of-distribution (OOD) robustness and adversarial robustness of merged models in machine learning.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; refers to a model merging technique that integrates multiple models to enhance performance.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; stands for Mixup Model Merge, a novel technique that introduces randomness in model merging, enabling better exploration of parameter space and improving model performance.</data>
  <data key="d3">chunk-070652f4f36e86620289fcafaaa29599&lt;SEP&gt;chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff&lt;SEP&gt;chunk-c8ff785e8102dfde59dbc9309abb6b6a&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7&lt;SEP&gt;chunk-6c3c559e2fcc2c84ba368ddf5b7db117&lt;SEP&gt;chunk-47449b4eaab59a26168fa9d821973678&lt;SEP&gt;chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="WizardLM-13B">
  <data key="d0">WizardLM-13B</data>
  <data key="d1">organization</data>
  <data key="d2">WizardLM-13B is a fine-tuned LLM that specializes in instruction following and is part of the experiments conducted to validate M&lt;sup&gt;3&lt;/sup&gt;.&lt;SEP&gt;WizardLM-13B is a large language model designed for various task-specific applications in machine learning.&lt;SEP&gt;WizardLM-13B is a task-specific fine-tuned LLM designed for instruction-following tasks.&lt;SEP&gt;WizardLM-13B is an instruction-following model based on Llama-2-13b, aimed at improving performance in open-domain tasks.</data>
  <data key="d3">chunk-17c651f0ad506eb37b0f1742513e2650&lt;SEP&gt;chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="WizardMath-13B">
  <data key="d0">WizardMath-13B</data>
  <data key="d1">organization</data>
  <data key="d2">WizardMath-13B is a fine-tuned LLM focused on mathematical reasoning, utilized in the evaluation of the M&lt;sup&gt;3&lt;/sup&gt; method.&lt;SEP&gt;WizardMath-13B is a model optimized for mathematical reasoning, designed to enhance Chain-of-Thought capabilities.&lt;SEP&gt;WizardMath-13B is a model that is being merged with another model to enhance performance in code generation tasks.&lt;SEP&gt;WizardMath-13B is a specialized large language model focused on mathematical tasks and computations.&lt;SEP&gt;WizardMath-13B is a task-specific fine-tuned LLM designed for mathematical reasoning tasks.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7&lt;SEP&gt;chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="llama-2-13b-code-alpaca">
  <data key="d0">llama-2-13b-code-alpaca</data>
  <data key="d1">organization</data>
  <data key="d2">llama-2-13b-code-alpaca is a fine-tuned LLM that specializes in code generation, also tested to assess the effectiveness of M&lt;sup&gt;3&lt;/sup&gt;.&lt;SEP&gt;llama-2-13b-code-alpaca is a model fine-tuned for code generation, enhancing understanding of programming tasks.&lt;SEP&gt;llama-2-13b-code-alpaca is a task-specific fine-tuned LLM designed for code generation tasks.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="LiveBench">
  <data key="d0">LiveBench</data>
  <data key="d1">event</data>
  <data key="d2">LiveBench is a dynamic benchmark for large language models, featuring frequently updated questions and diverse tasks to assess OOD robustness.&lt;SEP&gt;LiveBench is a framework used to evaluate the performance of language models across different tasks, including instruction following and coding.&lt;SEP&gt;LiveBench is an evaluation framework used to test the robustness of models, including those merged using the M&lt;sup&gt;3&lt;/sup&gt; technique.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="PromptBench">
  <data key="d0">PromptBench</data>
  <data key="d1">organization</data>
  <data key="d2">PromptBench is a framework used for validating the performance of machine learning models, particularly in the context of M&lt;sup&gt;3&lt;/sup&gt; and model merging.&lt;SEP&gt;PromptBench is a unified library designed for evaluating large language models, providing a standardized framework for prompt construction and adversarial testing.&lt;SEP&gt;PromptBench is another evaluation framework that assesses the performance of models, including the M&lt;sup&gt;3&lt;/sup&gt; method's impact on robustness.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-c8ff785e8102dfde59dbc9309abb6b6a&lt;SEP&gt;chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Ding et al.">
  <data key="d0">Ding et al.</data>
  <data key="d1">person</data>
  <data key="d2">Ding et al. refers to a group of authors contributing to research in the field, cited in the context of model merging and computational resources.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Xia et al.">
  <data key="d0">Xia et al.</data>
  <data key="d1">person</data>
  <data key="d2">Xia et al. are researchers mentioned in the text, contributing to the discussion on model merging and its implications.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Brown et al.">
  <data key="d0">Brown et al.</data>
  <data key="d1">person</data>
  <data key="d2">Brown et al. are cited authors discussing the computational requirements of SFT, contributing to the analysis of model merging.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Chang et al.">
  <data key="d0">Chang et al.</data>
  <data key="d1">person</data>
  <data key="d2">Chang et al. is another group of authors referenced in the context of model merging and fine-tuning.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Yang et al.">
  <data key="d0">Yang et al.</data>
  <data key="d1">person</data>
  <data key="d2">Yang et al. are researchers cited for their contributions to the development of the M&lt;sup&gt;3&lt;/sup&gt; technique.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Akiba et al.">
  <data key="d0">Akiba et al.</data>
  <data key="d1">person</data>
  <data key="d2">Akiba et al. are authors mentioned in relation to the advancements in model merging techniques.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Wortsman et al.">
  <data key="d0">Wortsman et al.</data>
  <data key="d1">person</data>
  <data key="d2">Wortsman et al. are researchers associated with the development of model merging techniques and methodologies.&lt;SEP&gt;Wortsman et al. are researchers referenced for their work on parameter fusion strategies in model merging.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Ilharco et al.">
  <data key="d0">Ilharco et al.</data>
  <data key="d1">person</data>
  <data key="d2">Ilharco et al. are authors mentioned in the context of limitations in existing model merging methods.&lt;SEP&gt;Ilharco et al. are researchers who have worked on model merging and related techniques.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Matena and Rafefel">
  <data key="d0">Matena and Rafefel</data>
  <data key="d1">person</data>
  <data key="d2">Matena and Rafefel are researchers cited for their contributions to the discussion on model merging limitations.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jin et al.">
  <data key="d0">Jin et al.</data>
  <data key="d1">person</data>
  <data key="d2">Jin et al. are authors referenced regarding the exploration of parameter space in model merging.&lt;SEP&gt;Jin et al. are researchers who have contributed to the field of model merging.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Yadav et al.">
  <data key="d0">Yadav et al.</data>
  <data key="d1">person</data>
  <data key="d2">Yadav et al. are researchers mentioned in the context of model merging and its challenges.&lt;SEP&gt;Yadav et al. are researchers who proposed TIES-Merging as a solution to task conflicts in model merging.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Zhang">
  <data key="d0">Zhang</data>
  <data key="d1">person</data>
  <data key="d2">Zhang is cited for introducing the Mixup technique, which inspired the development of the M&lt;sup&gt;3&lt;/sup&gt; method.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="White et al.">
  <data key="d0">White et al.</data>
  <data key="d1">person</data>
  <data key="d2">White et al. are researchers involved in the evaluation of model robustness using LiveBench.&lt;SEP&gt;White et al. is mentioned with respect to the LiveBench framework for evaluating out-of-distribution robustness.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Zhu et al.">
  <data key="d0">Zhu et al.</data>
  <data key="d1">person</data>
  <data key="d2">Zhu et al. are authors referenced for their work on evaluating the effectiveness of M&lt;sup&gt;3&lt;/sup&gt; using PromptBench.&lt;SEP&gt;Zhu et al. are researchers who contributed to the development of methods for validating the potential of M&lt;sup&gt;3&lt;/sup&gt; in machine learning.&lt;SEP&gt;Zhu et al. is noted for their work on the PromptBench module used for assessing LLMs against adversarial prompts.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-c8ff785e8102dfde59dbc9309abb6b6a&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Super Mario">
  <data key="d0">Super Mario</data>
  <data key="d1">event</data>
  <data key="d2">Super Mario is referenced as an analogy for the model merging process, illustrating how abilities are gained through integration.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Harry Potter">
  <data key="d0">Harry Potter</data>
  <data key="d1">event</data>
  <data key="d2">Harry Potter is used as an analogy to describe the mixing process in M&lt;sup&gt;3&lt;/sup&gt;, highlighting the dynamic control of parameters.</data>
  <data key="d3">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Fisher Merging">
  <data key="d0">Fisher Merging</data>
  <data key="d1">category</data>
  <data key="d2">Fisher Merging is a method that applies weights derived from the Fisher information matrix for precise parameter integration during model merging.</data>
  <data key="d3">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Mixup">
  <data key="d0">Mixup</data>
  <data key="d1">category</data>
  <data key="d2">Mixup is a data augmentation technique that generates new training samples by creating linear combinations of existing samples and their labels.&lt;SEP&gt;Mixup is a data augmentation technique that improves the generalization ability of deep learning models by creating virtual examples through linear interpolation.</data>
  <data key="d3">chunk-c8ff785e8102dfde59dbc9309abb6b6a&lt;SEP&gt;chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Empirical Risk Minimization (ERM)">
  <data key="d0">Empirical Risk Minimization (ERM)</data>
  <data key="d1">category</data>
  <data key="d2">Empirical Risk Minimization is a traditional framework in machine learning for training models based on minimizing empirical risk.</data>
  <data key="d3">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Vicinal Risk Minimization (VRM)">
  <data key="d0">Vicinal Risk Minimization (VRM)</data>
  <data key="d1">category</data>
  <data key="d2">VRM is a principle that introduces a vicinal distribution to enhance data diversity and model generalization.&lt;SEP&gt;Vicinal Risk Minimization is a principle that underlies the Mixup technique, focusing on improving model generalization.</data>
  <data key="d3">chunk-6c3c559e2fcc2c84ba368ddf5b7db117&lt;SEP&gt;chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Matena and Raffel">
  <data key="d0">Matena and Raffel</data>
  <data key="d1">person</data>
  <data key="d2">Matena and Raffel are researchers known for their contributions to Fisher Merging in machine learning.</data>
  <data key="d3">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Yu et al.">
  <data key="d0">Yu et al.</data>
  <data key="d1">person</data>
  <data key="d2">Yu et al. are researchers who have explored the capabilities of large language models (LLMs) in enhancing model merging.</data>
  <data key="d3">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Lin et al.">
  <data key="d0">Lin et al.</data>
  <data key="d1">person</data>
  <data key="d2">Lin et al. are researchers contributing to the understanding and advancements in model merging techniques.</data>
  <data key="d3">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Mixup Model Merge">
  <data key="d0">Mixup Model Merge</data>
  <data key="d1">category</data>
  <data key="d2">Mixup Model Merge (M&lt;sup&gt;3&lt;/sup&gt;) is a novel model merging method that combines parameters from two fine-tuned language models using random interpolation to enhance model performance.</data>
  <data key="d3">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Fine-Tuned LLMs">
  <data key="d0">Fine-Tuned LLMs</data>
  <data key="d1">category</data>
  <data key="d2">Fine-tuned LLMs are language models that have undergone additional training on specific tasks to improve their performance in those areas.</data>
  <data key="d3">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Average Merging">
  <data key="d0">Average Merging</data>
  <data key="d1">category</data>
  <data key="d2">Average Merging is a model merging method that is analyzed in the context of performance improvement with M&lt;sup&gt;3&lt;/sup&gt;.&lt;SEP&gt;Average Merging is a process that reformulates the merging of language model parameters, allowing for a weighted combination of fine-tuned models.&lt;SEP&gt;Average Merging is an established model merging method that combines the parameters of two models by averaging them.&lt;SEP&gt;Average Merging is one of the merging techniques evaluated for enhancing the performance of LLMs.</data>
  <data key="d3">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff&lt;SEP&gt;chunk-6c3c559e2fcc2c84ba368ddf5b7db117&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Beta Distribution">
  <data key="d0">Beta Distribution</data>
  <data key="d1">category</data>
  <data key="d2">Beta Distribution is a probability distribution used in statistics to model the behavior of random variables limited to a range of values.&lt;SEP&gt;The Beta distribution is a statistical distribution used in M&lt;sup&gt;3&lt;/sup&gt; to sample interpolation ratios between two fine-tuned models.</data>
  <data key="d3">chunk-6c3c559e2fcc2c84ba368ddf5b7db117&lt;SEP&gt;chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Llama 2">
  <data key="d0">Llama 2</data>
  <data key="d1">organization</data>
  <data key="d2">Llama 2 is a pre-trained language model that serves as a backbone for model merging processes.&lt;SEP&gt;Llama 2 refers to a series of models developed for chat and instruction-following tasks in AI.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2&lt;SEP&gt;chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Hyperparameter">
  <data key="d0">Hyperparameter</data>
  <data key="d1">category</data>
  <data key="d2">Hyperparameters are configuration settings used to control the learning process of machine learning algorithms, such as the interpolation ratio in Mixup.</data>
  <data key="d3">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="LLMs">
  <data key="d0">LLMs</data>
  <data key="d1">category</data>
  <data key="d2">LLMs, or Large Language Models, are advanced neural network architectures designed to understand and generate human-like text based on vast datasets.</data>
  <data key="d3">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Computational Cost">
  <data key="d0">Computational Cost</data>
  <data key="d1">category</data>
  <data key="d2">Computational Cost refers to the resources required, such as time and processing power, to execute algorithms or models.</data>
  <data key="d3">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Training Instability">
  <data key="d0">Training Instability</data>
  <data key="d1">category</data>
  <data key="d2">Training Instability describes the unpredictable behavior of a model during training, which can lead to poor performance or failure to converge.</data>
  <data key="d3">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Interpolation Ratio">
  <data key="d0">Interpolation Ratio</data>
  <data key="d1">category</data>
  <data key="d2">Interpolation Ratio is a hyperparameter that determines the weight of each sample in generating new samples during Mixup.</data>
  <data key="d3">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Synthetic Sample">
  <data key="d0">Synthetic Sample</data>
  <data key="d1">category</data>
  <data key="d2">Synthetic Sample refers to the new data points created through methods like Mixup, combining features and labels from existing samples.</data>
  <data key="d3">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Parameters">
  <data key="d0">Parameters</data>
  <data key="d1">category</data>
  <data key="d2">Parameters are the internal variables of a model that are adjusted during training to minimize error and improve performance.</data>
  <data key="d3">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Fine-Tuned Models">
  <data key="d0">Fine-Tuned Models</data>
  <data key="d1">category</data>
  <data key="d2">Fine-Tuned Models are pre-trained models that have been further trained on specific tasks to enhance their performance in those areas.</data>
  <data key="d3">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Pre-Trained Backbone">
  <data key="d0">Pre-Trained Backbone</data>
  <data key="d1">category</data>
  <data key="d2">Pre-Trained Backbone refers to the foundational model that has been trained on a broad dataset before being fine-tuned for specific tasks.</data>
  <data key="d3">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="SFT">
  <data key="d0">SFT</data>
  <data key="d1">category</data>
  <data key="d2">SFT stands for Supervised Fine-Tuning, a process that adjusts language model parameters for specific tasks.</data>
  <data key="d3">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="$\theta^t_{SFT}$">
  <data key="d0">$\theta^t_{SFT}$</data>
  <data key="d1">category</data>
  <data key="d2">$\theta^t_{SFT}$ represents the parameters of a language model after it has undergone Supervised Fine-Tuning for a specific task.</data>
  <data key="d3">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="$\theta_{PRE}$">
  <data key="d0">$\theta_{PRE}$</data>
  <data key="d1">category</data>
  <data key="d2">$\theta_{PRE}$ denotes the parameters of a language model prior to any fine-tuning process.</data>
  <data key="d3">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="$\delta^t$">
  <data key="d0">$\delta^t$</data>
  <data key="d1">category</data>
  <data key="d2">$\delta^t$ is the difference between the parameters of language models before and after Supervised Fine-Tuning.</data>
  <data key="d3">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="$\lambda_m$">
  <data key="d0">$\lambda_m$</data>
  <data key="d1">category</data>
  <data key="d2">$\lambda_m$ is a hyperparameter that determines the linear interpolation ratio between two fine-tuned language models.</data>
  <data key="d3">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="$\alpha$">
  <data key="d0">$\alpha$</data>
  <data key="d1">category</data>
  <data key="d2">$\alpha$ is a hyperparameter that controls the shape of the Beta distribution used in sampling the interpolation ratio $\lambda_m$.</data>
  <data key="d3">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Figure 3">
  <data key="d0">Figure 3</data>
  <data key="d1">event</data>
  <data key="d2">Figure 3 illustrates the Beta distribution visualization for different values of the hyperparameter $\alpha$.</data>
  <data key="d3">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Task-Specific Fine-Tuned LLMs">
  <data key="d0">Task-Specific Fine-Tuned LLMs</data>
  <data key="d1">category</data>
  <data key="d2">Task-Specific Fine-Tuned LLMs refers to large language models that are specifically fine-tuned for distinct tasks such as instruction following, mathematical reasoning, and code generation.</data>
  <data key="d3">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="AlpacaEval">
  <data key="d0">AlpacaEval</data>
  <data key="d1">event</data>
  <data key="d2">AlpacaEval is a benchmark for evaluating language models' performance on instruction-following tasks.&lt;SEP&gt;AlpacaEval is a benchmark used to evaluate the performance of language models, particularly in the context of model merging.&lt;SEP&gt;AlpacaEval is an LLM-based automated evaluation metric designed to assess model performance on instruction-following tasks.&lt;SEP&gt;AlpacaEval is an LLM-based automated evaluation metric for assessing model performance on instruction-following tasks.&lt;SEP&gt;AlpacaEval is an evaluation benchmark used to assess the performance of instruction-following tasks.</data>
  <data key="d3">chunk-f21dc353d188ca08cba26298157ff09d&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-9fbd0c0622e4d5fb84be83781e1c4521&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="GSM8K">
  <data key="d0">GSM8K</data>
  <data key="d1">event</data>
  <data key="d2">GSM8K is a dataset containing 8.5K high-quality, linguistically diverse grade school math word problems, used to evaluate multi-step mathematical reasoning abilities of large language models.&lt;SEP&gt;GSM8K is a dataset designed to evaluate multi-step mathematical reasoning abilities of large language models with 8.5K word problems.&lt;SEP&gt;GSM8K is a dataset used for evaluating zero-shot accuracy in mathematical problem-solving tasks.&lt;SEP&gt;GSM8K is a dataset used for training language models in mathematical reasoning tasks.&lt;SEP&gt;GSM8K is an evaluation dataset used for testing the performance of mathematical reasoning tasks.&lt;SEP&gt;GSM8K is another benchmark for assessing the capabilities of language models, often used in conjunction with others like AlpacaEval.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-9fbd0c0622e4d5fb84be83781e1c4521&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7&lt;SEP&gt;chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="MATH">
  <data key="d0">MATH</data>
  <data key="d1">dataset</data>
  <data key="d2">MATH is a dataset consisting of 12,500 competition-level math problems, aimed at evaluating and enhancing the problem-solving abilities of machine learning models.&lt;SEP&gt;MATH is a dataset containing competition-level math problems aimed at enhancing problem-solving abilities of machine learning models.&lt;SEP&gt;MATH is a dataset utilized for improving the mathematical reasoning capabilities of language models.&lt;SEP&gt;MATH is a dataset utilized to evaluate the performance of LLMs in solving mathematical problems.&lt;SEP&gt;MATH is an evaluation dataset designed to assess mathematical reasoning capabilities.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-9fbd0c0622e4d5fb84be83781e1c4521&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="HumanEval">
  <data key="d0">HumanEval</data>
  <data key="d1">dataset</data>
  <data key="d2">HumanEval is a benchmark used for evaluating code generation tasks.&lt;SEP&gt;HumanEval is a benchmark used to assess the code generation capabilities of language models.&lt;SEP&gt;HumanEval is a dataset of 164 hand-written programming problems used to evaluate the functional correctness of code generation models.&lt;SEP&gt;HumanEval is a dataset of programming problems used to evaluate the functional correctness of code generation models.</data>
  <data key="d3">chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-9fbd0c0622e4d5fb84be83781e1c4521&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="MBPP">
  <data key="d0">MBPP</data>
  <data key="d1">event</data>
  <data key="d2">MBPP is a benchmark for assessing the performance of code-generating tasks.&lt;SEP&gt;MBPP is a benchmark that evaluates programming tasks and problem-solving abilities of language models.&lt;SEP&gt;MBPP is a benchmark used for evaluating the performance of LLMs on coding tasks.&lt;SEP&gt;MBPP is a dataset containing 974 programming problems designed to assess a model's ability to synthesize Python programs from natural language descriptions.&lt;SEP&gt;MBPP is a dataset that evaluates a model's ability to synthesize Python programs from natural language descriptions.&lt;SEP&gt;MBPP is a dataset where the Math &amp; Code model achieved a specific performance score when combined with DARE.&lt;SEP&gt;MBPP is a dataset where the Math &amp; Code model achieved a specific score when combined with DARE.&lt;SEP&gt;MBPP refers to a benchmark for evaluating the performance of programming models, which shows improvement through the M&lt;sup&gt;3&lt;/sup&gt; approach.</data>
  <data key="d3">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-9fbd0c0622e4d5fb84be83781e1c4521&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7&lt;SEP&gt;chunk-f21dc353d188ca08cba26298157ff09d&lt;SEP&gt;chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="_{SFT}^{t_1}">
  <data key="d0">_{SFT}^{t_1}</data>
  <data key="d1">category</data>
  <data key="d2">_{SFT}^{t_1} is a parameter representation used in the context of model training, specifically in the linear interpolation of model parameters.</data>
  <data key="d3">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="_{SFT}^{t_2}">
  <data key="d0">_{SFT}^{t_2}</data>
  <data key="d1">category</data>
  <data key="d2">_{SFT}^{t_2} is another parameter representation that, along with _{SFT}^{t_1}, is utilized for linear interpolation in model training.</data>
  <data key="d3">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="_m">
  <data key="d0">_m</data>
  <data key="d1">category</data>
  <data key="d2">_m is the interpolation ratio used to combine parameters from different models in the M&lt;sup&gt;3&lt;/sup&gt; method.</data>
  <data key="d3">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="P_{}(_M)">
  <data key="d0">P_{}(_M)</data>
  <data key="d1">category</data>
  <data key="d2">P_{}(_M) represents the distribution resulting from the linear interpolation of model parameters in the M&lt;sup&gt;3&lt;/sup&gt; method.</data>
  <data key="d3">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Dirac Delta Function">
  <data key="d0">Dirac Delta Function</data>
  <data key="d1">category</data>
  <data key="d2">The Dirac delta function is a mathematical function used to enforce conditions in distributions, ensuring that _M satisfies the linear interpolation rule.</data>
  <data key="d3">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Neighborhood Distribution">
  <data key="d0">Neighborhood Distribution</data>
  <data key="d1">category</data>
  <data key="d2">Neighborhood distribution refers to the new distribution formed by merging knowledge from different models through linear interpolation.</data>
  <data key="d3">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Decision Boundaries">
  <data key="d0">Decision Boundaries</data>
  <data key="d1">category</data>
  <data key="d2">Decision boundaries are the thresholds that separate different tasks, which the M&lt;sup&gt;3&lt;/sup&gt; method aims to make smoother.</data>
  <data key="d3">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Occam's Razor">
  <data key="d0">Occam's Razor</data>
  <data key="d1">category</data>
  <data key="d2">Occam's Razor is a principle suggesting that simpler solutions tend to generalize better, which is applied in the M&lt;sup&gt;3&lt;/sup&gt; method for model merging.</data>
  <data key="d3">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Task Conflicts">
  <data key="d0">Task Conflicts</data>
  <data key="d1">category</data>
  <data key="d2">Task conflicts refer to the issues that arise when different tasks require conflicting parameter values, which the M&lt;sup&gt;3&lt;/sup&gt; method aims to mitigate.</data>
  <data key="d3">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Experiments">
  <data key="d0">Experiments</data>
  <data key="d1">event</data>
  <data key="d2">Experiments refer to the structured evaluations conducted to assess the performance of the M&lt;sup&gt;3&lt;/sup&gt; method across different tasks.</data>
  <data key="d3">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Instruction Following">
  <data key="d0">Instruction Following</data>
  <data key="d1">event</data>
  <data key="d2">Instruction Following is a task category in LiveBench that assesses a model's ability to follow given instructions correctly.&lt;SEP&gt;Instruction following is a specific task that the WizardLM-13B model is designed to perform.</data>
  <data key="d3">chunk-b650b88d4bebbc45d20b8908b045eff8&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Mathematical Reasoning">
  <data key="d0">Mathematical Reasoning</data>
  <data key="d1">event</data>
  <data key="d2">Mathematical reasoning is a task that the WizardMath-13B model is specifically fine-tuned for.</data>
  <data key="d3">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Code Generation">
  <data key="d0">Code Generation</data>
  <data key="d1">event</data>
  <data key="d2">Code generation is a task that the llama-2-13b-code-alpaca model is designed to perform.</data>
  <data key="d3">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Austin et al.">
  <data key="d0">Austin et al.</data>
  <data key="d1">person</data>
  <data key="d2">Austin et al. is referenced in the context of evaluating large language models (LLMs) and their datasets.</data>
  <data key="d3">chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="SST2">
  <data key="d0">SST2</data>
  <data key="d1">event</data>
  <data key="d2">SST2 (Stanford Sentiment Treebank 2) is a dataset used for sentiment analysis in natural language processing.&lt;SEP&gt;SST2 is a dataset used for evaluating sentiment analysis models, referenced in the context of model performance metrics.&lt;SEP&gt;SST2 is a dataset used for evaluating sentiment analysis models, referenced in the context of model performance.&lt;SEP&gt;SST2 is a dataset used for evaluating the performance of models in sentiment analysis tasks.&lt;SEP&gt;SST2 is a sentiment analysis dataset designed to assess whether a given sentence conveys a positive or negative sentiment.&lt;SEP&gt;SST2 is a sentiment analysis dataset referenced for evaluating accuracy in LLMs.</data>
  <data key="d3">chunk-070652f4f36e86620289fcafaaa29599&lt;SEP&gt;chunk-407b385e429f3495cdcf3f23f7e80f73&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-47449b4eaab59a26168fa9d821973678&lt;SEP&gt;chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="CoLA">
  <data key="d0">CoLA</data>
  <data key="d1">event</data>
  <data key="d2">CoLA (Corpus of Linguistic Acceptability) is a dataset used for evaluating the linguistic acceptability of sentences.&lt;SEP&gt;CoLA is a dataset for grammar correctness, where the model must determine whether a sentence is grammatically acceptable.&lt;SEP&gt;CoLA is a dataset that tests grammar correctness, mentioned in relation to model performance metrics.&lt;SEP&gt;CoLA is a dataset used for evaluating the performance of models in linguistic acceptability tasks.&lt;SEP&gt;CoLA is a dataset used to assess grammatical correctness in language models.</data>
  <data key="d3">chunk-070652f4f36e86620289fcafaaa29599&lt;SEP&gt;chunk-407b385e429f3495cdcf3f23f7e80f73&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-47449b4eaab59a26168fa9d821973678&lt;SEP&gt;chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="NVIDIA GeForce RTX 4090">
  <data key="d0">NVIDIA GeForce RTX 4090</data>
  <data key="d1">equipment</data>
  <data key="d2">NVIDIA GeForce RTX 4090 is a high-performance GPU utilized for conducting experiments in LLM model merging.&lt;SEP&gt;NVIDIA GeForce RTX 4090 is a high-performance graphics processing unit used for running experiments on LLMs.</data>
  <data key="d3">chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Appendix A.1">
  <data key="d0">Appendix A.1</data>
  <data key="d1">category</data>
  <data key="d2">Appendix A.1 contains details about large language models (LLMs) and datasets used in the evaluation process.</data>
  <data key="d3">chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Adversarial Prompt Attacks">
  <data key="d0">Adversarial Prompt Attacks</data>
  <data key="d1">category</data>
  <data key="d2">Adversarial Prompt Attacks are methods used to evaluate the robustness of language models against misleading prompts.&lt;SEP&gt;Adversarial Prompt Attacks is a module within PromptBench designed to assess the robustness of LLMs against adversarial prompts.</data>
  <data key="d3">chunk-b650b88d4bebbc45d20b8908b045eff8&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="DeepWordBug">
  <data key="d0">DeepWordBug</data>
  <data key="d1">category</data>
  <data key="d2">DeepWordBug is a character-level attack method used to evaluate LLMs' robustness against adversarial prompts.&lt;SEP&gt;DeepWordBug is a method that introduces subtle character-level perturbations to deceive language models, testing their robustness against small typographical errors.&lt;SEP&gt;DeepWordBug is a prompt attack method used to evaluate the adversarial robustness of models.</data>
  <data key="d3">chunk-070652f4f36e86620289fcafaaa29599&lt;SEP&gt;chunk-407b385e429f3495cdcf3f23f7e80f73&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="BERTAttack">
  <data key="d0">BERTAttack</data>
  <data key="d1">category</data>
  <data key="d2">BERTAttack is a method used to evaluate the adversarial robustness of models by applying prompt attacks.&lt;SEP&gt;BERTAttack is a word-level attack method employed to test the resilience of LLMs against adversarial inputs.&lt;SEP&gt;BERTAttack is another prompt attack method that assesses the robustness of models against adversarial inputs.&lt;SEP&gt;BERTAttack manipulates text at the word level by replacing words with contextually similar synonyms to mislead language models.</data>
  <data key="d3">chunk-070652f4f36e86620289fcafaaa29599&lt;SEP&gt;chunk-568ea9e09f85a7b95cfe61d48b0ba893&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="StressTest">
  <data key="d0">StressTest</data>
  <data key="d1">category</data>
  <data key="d2">StressTest appends irrelevant or redundant sentences to prompts to distract language models and assess their ability to maintain accuracy.&lt;SEP&gt;StressTest is a prompt attack method designed to evaluate the performance drop rate of models under adversarial conditions.&lt;SEP&gt;StressTest is a sentence-level attack method used to evaluate the robustness of LLMs in handling adversarial prompts.</data>
  <data key="d3">chunk-070652f4f36e86620289fcafaaa29599&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Matthews Correlation Coefficient (MCC)">
  <data key="d0">Matthews Correlation Coefficient (MCC)</data>
  <data key="d1">category</data>
  <data key="d2">Matthews Correlation Coefficient (MCC) is a metric used to evaluate the performance of models on the CoLA dataset for grammatical correctness.</data>
  <data key="d3">chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="llama-2-13b-codealpaca">
  <data key="d0">llama-2-13b-codealpaca</data>
  <data key="d1">organization</data>
  <data key="d2">llama-2-13b-codealpaca is a large language model aimed at coding and programming tasks in machine learning.&lt;SEP&gt;llama-2-13b-codealpaca is a model that has been identified as not being well fine-tuned for code generation, impacting its merging effectiveness.</data>
  <data key="d3">chunk-17c651f0ad506eb37b0f1742513e2650&lt;SEP&gt;chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="LiveBench-Instruction">
  <data key="d0">LiveBench-Instruction</data>
  <data key="d1">event</data>
  <data key="d2">LiveBench-Instruction is a benchmark event that assesses the performance of models in instruction-following tasks.&lt;SEP&gt;LiveBench-Instruction is a benchmarking event assessing language models' performance in following instructions.&lt;SEP&gt;LiveBench-Instruction is a dataset used for evaluating model performance on instruction-following tasks.</data>
  <data key="d3">chunk-17c651f0ad506eb37b0f1742513e2650&lt;SEP&gt;chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff&lt;SEP&gt;chunk-070652f4f36e86620289fcafaaa29599</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="LiveBench-Coding">
  <data key="d0">LiveBench-Coding</data>
  <data key="d1">event</data>
  <data key="d2">LiveBench-Coding is a benchmark event utilized to evaluate coding-related model performance.&lt;SEP&gt;LiveBench-Coding is a benchmarking event that evaluates the performance of language models on coding tasks.&lt;SEP&gt;LiveBench-Coding is a dataset used for evaluating model performance specifically on coding tasks.</data>
  <data key="d3">chunk-17c651f0ad506eb37b0f1742513e2650&lt;SEP&gt;chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff&lt;SEP&gt;chunk-070652f4f36e86620289fcafaaa29599</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="LiveBench-TypoFixing">
  <data key="d0">LiveBench-TypoFixing</data>
  <data key="d1">event</data>
  <data key="d2">LiveBench-TypoFixing is a benchmark event used to evaluate model performance in typo fixing tasks.&lt;SEP&gt;LiveBench-TypoFixing is a benchmarking event that tests the ability of language models to correct typographical errors.&lt;SEP&gt;LiveBench-TypoFixing is a dataset that assesses model performance on correcting typographical errors in code.</data>
  <data key="d3">chunk-17c651f0ad506eb37b0f1742513e2650&lt;SEP&gt;chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff&lt;SEP&gt;chunk-070652f4f36e86620289fcafaaa29599</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="OOD Datasets">
  <data key="d0">OOD Datasets</data>
  <data key="d1">category</data>
  <data key="d2">OOD Datasets refer to out-of-distribution datasets that are used to evaluate the robustness of machine learning models.</data>
  <data key="d3">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Model Robustness">
  <data key="d0">Model Robustness</data>
  <data key="d1">category</data>
  <data key="d2">Model Robustness refers to the ability of a machine learning model to perform well on unseen or out-of-distribution data.</data>
  <data key="d3">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Merging Ratio">
  <data key="d0">Merging Ratio</data>
  <data key="d1">category</data>
  <data key="d2">Merging Ratio is a critical factor in determining the performance of a merged model, influencing how different models are combined.</data>
  <data key="d3">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Performance Metrics">
  <data key="d0">Performance Metrics</data>
  <data key="d1">category</data>
  <data key="d2">Performance Metrics refer to the various measurements used to evaluate the effectiveness and accuracy of language models during testing events.&lt;SEP&gt;Performance metrics refer to the criteria used to evaluate the effectiveness of models, including accuracy and MCC.</data>
  <data key="d3">chunk-17c651f0ad506eb37b0f1742513e2650&lt;SEP&gt;chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Accuracy Improvement">
  <data key="d0">Accuracy Improvement</data>
  <data key="d1">category</data>
  <data key="d2">Accuracy Improvement refers to the enhancement in the precision of language models as measured by their performance in specific tasks.</data>
  <data key="d3">chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Win Rate">
  <data key="d0">Win Rate</data>
  <data key="d1">category</data>
  <data key="d2">Win Rate is a performance indicator that reflects the success rate of a language model in competitive scenarios against benchmarks.</data>
  <data key="d3">chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Pass@1">
  <data key="d0">Pass@1</data>
  <data key="d1">category</data>
  <data key="d2">Pass@1 is a specific evaluation metric that indicates whether a model's first response to a task is correct.</data>
  <data key="d3">chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Math &amp; Code">
  <data key="d0">Math &amp; Code</data>
  <data key="d1">organization</data>
  <data key="d2">Math &amp; Code is a merged model that utilizes mathematical and coding strategies to enhance performance on various tasks.&lt;SEP&gt;Math &amp; Code is a model configuration tested on the SST2 and CoLA datasets for performance evaluation.&lt;SEP&gt;Math &amp; Code is a model that demonstrated significant performance improvements when combined with M&lt;sup&gt;3&lt;/sup&gt; in model merging.&lt;SEP&gt;Math &amp; Code is a model that demonstrated significant performance improvements when merged with M&lt;sup&gt;3&lt;/sup&gt; in model merging experiments.</data>
  <data key="d3">chunk-070652f4f36e86620289fcafaaa29599&lt;SEP&gt;chunk-407b385e429f3495cdcf3f23f7e80f73&lt;SEP&gt;chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="LM &amp; Math">
  <data key="d0">LM &amp; Math</data>
  <data key="d1">category</data>
  <data key="d2">LM &amp; Math is a merged model that combines language modeling with mathematical strategies to improve task performance.&lt;SEP&gt;LM &amp; Math is another model configuration evaluated on the SST2 and CoLA datasets, focusing on language model performance.</data>
  <data key="d3">chunk-070652f4f36e86620289fcafaaa29599&lt;SEP&gt;chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="LM &amp; Code">
  <data key="d0">LM &amp; Code</data>
  <data key="d1">organization</data>
  <data key="d2">LM &amp; Code is a merged model that integrates language modeling with coding strategies to achieve better task outcomes.&lt;SEP&gt;LM &amp; Code is a model setup tested on the SST2 and CoLA datasets, assessing its robustness against adversarial attacks.&lt;SEP&gt;LM &amp; Code is a model that achieved notable accuracy increases when enhanced with M&lt;sup&gt;3&lt;/sup&gt; during merging.&lt;SEP&gt;LM &amp; Code is another model that achieved notable accuracy increases when enhanced with M&lt;sup&gt;3&lt;/sup&gt; during merging.</data>
  <data key="d3">chunk-070652f4f36e86620289fcafaaa29599&lt;SEP&gt;chunk-407b385e429f3495cdcf3f23f7e80f73&lt;SEP&gt;chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Performance Drop Rate (PDR)">
  <data key="d0">Performance Drop Rate (PDR)</data>
  <data key="d1">category</data>
  <data key="d2">Performance Drop Rate (PDR) is a metric used to evaluate the adversarial robustness of models under attack.&lt;SEP&gt;Performance Drop Rate (PDR) is a metric used to evaluate the adversarial robustness of models, with a lower PDR indicating stronger robustness.</data>
  <data key="d3">chunk-070652f4f36e86620289fcafaaa29599&lt;SEP&gt;chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Model Merging Techniques">
  <data key="d0">Model Merging Techniques</data>
  <data key="d1">category</data>
  <data key="d2">Model merging techniques include Average Merging, Task Arithmetic, and TIES-Merging, which are evaluated for their effectiveness in improving model performance.&lt;SEP&gt;Model merging techniques include Average Merging, Task Arithmetic, and TIES-Merging, which are evaluated for their effectiveness.</data>
  <data key="d3">chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Parameter Linear Interpolation Process">
  <data key="d0">Parameter Linear Interpolation Process</data>
  <data key="d1">category</data>
  <data key="d2">The parameter linear interpolation process is a method used in M&lt;sup&gt;3&lt;/sup&gt; to dynamically adjust merging ratios during model merging.</data>
  <data key="d3">chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Adversarial Robustness">
  <data key="d0">Adversarial Robustness</data>
  <data key="d1">category</data>
  <data key="d2">Adversarial Robustness is a property of machine learning models that measures their ability to maintain performance when faced with adversarial inputs or attacks.&lt;SEP&gt;Adversarial robustness refers to the ability of merged models to maintain performance under adversarial conditions, which M&lt;sup&gt;3&lt;/sup&gt; aims to enhance.</data>
  <data key="d3">chunk-407b385e429f3495cdcf3f23f7e80f73&lt;SEP&gt;chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Ouyang et al.">
  <data key="d0">Ouyang et al.</data>
  <data key="d1">person</data>
  <data key="d2">Ouyang et al. is referenced in the context of developing models that incorporate Reinforcement Learning with Human Feedback for improved alignment.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jacob Austin">
  <data key="d0">Jacob Austin</data>
  <data key="d1">person</data>
  <data key="d2">Jacob Austin is one of the authors of the referenced paper on program synthesis with large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Augustus Odena">
  <data key="d0">Augustus Odena</data>
  <data key="d1">person</data>
  <data key="d2">Augustus Odena is a co-author of the paper discussing program synthesis with large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Maxwell Nye">
  <data key="d0">Maxwell Nye</data>
  <data key="d1">person</data>
  <data key="d2">Maxwell Nye is a contributor to the research on program synthesis with large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Maarten Bosma">
  <data key="d0">Maarten Bosma</data>
  <data key="d1">person</data>
  <data key="d2">Maarten Bosma is involved in the study of program synthesis with large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Henryk Michalewski">
  <data key="d0">Henryk Michalewski</data>
  <data key="d1">person</data>
  <data key="d2">Henryk Michalewski is a co-author of the referenced paper on program synthesis with large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="David Dohan">
  <data key="d0">David Dohan</data>
  <data key="d1">person</data>
  <data key="d2">David Dohan is one of the authors of the paper on program synthesis with large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Ellen Jiang">
  <data key="d0">Ellen Jiang</data>
  <data key="d1">person</data>
  <data key="d2">Ellen Jiang is a contributor to the research on program synthesis with large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Carrie Cai">
  <data key="d0">Carrie Cai</data>
  <data key="d1">person</data>
  <data key="d2">Carrie Cai is involved in the research on program synthesis with large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Michael Terry">
  <data key="d0">Michael Terry</data>
  <data key="d1">person</data>
  <data key="d2">Michael Terry is a co-author of the paper discussing program synthesis with large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Quoc Le">
  <data key="d0">Quoc Le</data>
  <data key="d1">person</data>
  <data key="d2">Quoc Le is one of the authors of the paper on program synthesis with large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Yupeng Chang">
  <data key="d0">Yupeng Chang</data>
  <data key="d1">person</data>
  <data key="d2">Yupeng Chang is involved in multiple papers on large language models and their evaluation.&lt;SEP&gt;Yupeng Chang is involved in the research on bias-alleviating low-rank adaptation for large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Edward Beeching">
  <data key="d0">Edward Beeching</data>
  <data key="d1">person</data>
  <data key="d2">Edward Beeching is a co-author of the paper on the Open LLM leaderboard.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Clmentine Fourrier">
  <data key="d0">Clmentine Fourrier</data>
  <data key="d1">person</data>
  <data key="d2">Clmentine Fourrier is involved in the research on the Open LLM leaderboard.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Nathan Habib">
  <data key="d0">Nathan Habib</data>
  <data key="d1">person</data>
  <data key="d2">Nathan Habib is a contributor to the study on the Open LLM leaderboard.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Sheon Han">
  <data key="d0">Sheon Han</data>
  <data key="d1">person</data>
  <data key="d2">Sheon Han is a co-author of the paper on the Open LLM leaderboard.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Nathan Lambert">
  <data key="d0">Nathan Lambert</data>
  <data key="d1">person</data>
  <data key="d2">Nathan Lambert is involved in the research on the Open LLM leaderboard.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Nazneen Rajani">
  <data key="d0">Nazneen Rajani</data>
  <data key="d1">person</data>
  <data key="d2">Nazneen Rajani is a contributor to the study on the Open LLM leaderboard.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Omar Sanseviero">
  <data key="d0">Omar Sanseviero</data>
  <data key="d1">person</data>
  <data key="d2">Omar Sanseviero is a co-author of the paper on the Open LLM leaderboard.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Lewis Tunstall">
  <data key="d0">Lewis Tunstall</data>
  <data key="d1">person</data>
  <data key="d2">Lewis Tunstall is involved in the research on the Open LLM leaderboard.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Thomas Wolf">
  <data key="d0">Thomas Wolf</data>
  <data key="d1">person</data>
  <data key="d2">Thomas Wolf is a co-author of the paper discussing the Open LLM leaderboard.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Tom Brown">
  <data key="d0">Tom Brown</data>
  <data key="d1">person</data>
  <data key="d2">Tom Brown is one of the authors of the paper on language models as few-shot learners.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Benjamin Mann">
  <data key="d0">Benjamin Mann</data>
  <data key="d1">person</data>
  <data key="d2">Benjamin Mann is a co-author of the paper discussing few-shot learning in language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Nick Ryder">
  <data key="d0">Nick Ryder</data>
  <data key="d1">person</data>
  <data key="d2">Nick Ryder is involved in the research on language models as few-shot learners.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Melanie Subbiah">
  <data key="d0">Melanie Subbiah</data>
  <data key="d1">person</data>
  <data key="d2">Melanie Subbiah is a co-author of the paper on language models as few-shot learners.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jared D Kaplan">
  <data key="d0">Jared D Kaplan</data>
  <data key="d1">person</data>
  <data key="d2">Jared D Kaplan is involved in the research on few-shot learning in language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Prafulla Dhariwal">
  <data key="d0">Prafulla Dhariwal</data>
  <data key="d1">person</data>
  <data key="d2">Prafulla Dhariwal is a co-author of the paper discussing language models as few-shot learners.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Arvind Neelakantan">
  <data key="d0">Arvind Neelakantan</data>
  <data key="d1">person</data>
  <data key="d2">Arvind Neelakantan is involved in the research on language models as few-shot learners.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Pranav Shyam">
  <data key="d0">Pranav Shyam</data>
  <data key="d1">person</data>
  <data key="d2">Pranav Shyam is a co-author of the paper on language models as few-shot learners.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Girish Sastry">
  <data key="d0">Girish Sastry</data>
  <data key="d1">person</data>
  <data key="d2">Girish Sastry is involved in the research on few-shot learning in language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Amanda Askell">
  <data key="d0">Amanda Askell</data>
  <data key="d1">person</data>
  <data key="d2">Amanda Askell is a co-author of the paper discussing language models as few-shot learners.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Xu Wang">
  <data key="d0">Xu Wang</data>
  <data key="d1">person</data>
  <data key="d2">Xu Wang is a co-author of the paper discussing evaluation of large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jindong Wang">
  <data key="d0">Jindong Wang</data>
  <data key="d1">person</data>
  <data key="d2">Jindong Wang is involved in the research on evaluation of large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Linyi Yang">
  <data key="d0">Linyi Yang</data>
  <data key="d1">person</data>
  <data key="d2">Linyi Yang is a co-author of the paper on evaluation of large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Kaijie Zhu">
  <data key="d0">Kaijie Zhu</data>
  <data key="d1">person</data>
  <data key="d2">Kaijie Zhu is involved in the research on evaluation of large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Hao Chen">
  <data key="d0">Hao Chen</data>
  <data key="d1">person</data>
  <data key="d2">Hao Chen is a co-author of the paper discussing evaluation of large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Xiaoyuan Yi">
  <data key="d0">Xiaoyuan Yi</data>
  <data key="d1">person</data>
  <data key="d2">Xiaoyuan Yi is involved in the research on evaluation of large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Cunxiang Wang">
  <data key="d0">Cunxiang Wang</data>
  <data key="d1">person</data>
  <data key="d2">Cunxiang Wang is a co-author of the paper on evaluation of large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Yidong Wang">
  <data key="d0">Yidong Wang</data>
  <data key="d1">person</data>
  <data key="d2">Yidong Wang is involved in the research on evaluation of large language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Karl Cobbe">
  <data key="d0">Karl Cobbe</data>
  <data key="d1">person</data>
  <data key="d2">Karl Cobbe is a co-author of the paper on training verifiers for math word problems.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Vineet Kosaraju">
  <data key="d0">Vineet Kosaraju</data>
  <data key="d1">person</data>
  <data key="d2">Vineet Kosaraju is involved in the research on training verifiers for math word problems.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Mohammad Bavarian">
  <data key="d0">Mohammad Bavarian</data>
  <data key="d1">person</data>
  <data key="d2">Mohammad Bavarian is a co-author of the paper on training verifiers for math word problems.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Mark Chen">
  <data key="d0">Mark Chen</data>
  <data key="d1">person</data>
  <data key="d2">Mark Chen is involved in the research on training verifiers for math word problems.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Heewoo Jun">
  <data key="d0">Heewoo Jun</data>
  <data key="d1">person</data>
  <data key="d2">Heewoo Jun is a co-author of the paper on training verifiers for math word problems.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Lukasz Kaiser">
  <data key="d0">Lukasz Kaiser</data>
  <data key="d1">person</data>
  <data key="d2">Lukasz Kaiser is involved in the research on training verifiers for math word problems.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Matthias Plappert">
  <data key="d0">Matthias Plappert</data>
  <data key="d1">person</data>
  <data key="d2">Matthias Plappert is a co-author of the paper on training verifiers for math word problems.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jerry Tworek">
  <data key="d0">Jerry Tworek</data>
  <data key="d1">person</data>
  <data key="d2">Jerry Tworek is involved in the research on training verifiers for math word problems.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jacob Hilton">
  <data key="d0">Jacob Hilton</data>
  <data key="d1">person</data>
  <data key="d2">Jacob Hilton is a co-author of the paper on training verifiers for math word problems.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Reiichiro Nakano">
  <data key="d0">Reiichiro Nakano</data>
  <data key="d1">person</data>
  <data key="d2">Reiichiro Nakano is involved in the research on training verifiers for math word problems.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Ronald A Fisher">
  <data key="d0">Ronald A Fisher</data>
  <data key="d1">person</data>
  <data key="d2">Ronald A Fisher is referenced for his foundational work in theoretical statistics.&lt;SEP&gt;Ronald A Fisher was a prominent statistician known for his contributions to the mathematical foundations of theoretical statistics, particularly in the early 20th century.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36&lt;SEP&gt;chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Philosophical Transactions of the Royal">
  <data key="d0">Philosophical Transactions of the Royal</data>
  <data key="d1">organization</data>
  <data key="d2">Philosophical Transactions of the Royal is a journal that published Ronald A Fisher's foundational work in theoretical statistics.&lt;SEP&gt;Philosophical Transactions of the Royal is a journal where Ronald A Fisher's work on theoretical statistics was published.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Reinforcement Learning with Human Feedback">
  <data key="d0">Reinforcement Learning with Human Feedback</data>
  <data key="d1">category</data>
  <data key="d2">Reinforcement Learning with Human Feedback (RLHF) is a method that combines reinforcement learning techniques with human feedback to improve model alignment and performance.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="arXiv preprint arXiv:2403.13187">
  <data key="d0">arXiv preprint arXiv:2403.13187</data>
  <data key="d1">organization</data>
  <data key="d2">arXiv preprint arXiv:2403.13187 is a research paper authored by Takuya Akiba and others discussing evolutionary optimization of model merging recipes.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="UCI Machine Learning Repository">
  <data key="d0">UCI Machine Learning Repository</data>
  <data key="d1">organization</data>
  <data key="d2">The UCI Machine Learning Repository is a widely-used collection of datasets for machine learning research, referenced in the context of model evaluation.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="arXiv preprint arXiv:2108.07732">
  <data key="d0">arXiv preprint arXiv:2108.07732</data>
  <data key="d1">organization</data>
  <data key="d2">arXiv preprint arXiv:2108.07732 is a research paper on program synthesis with large language models, authored by Jacob Austin and others.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="arXiv preprint arXiv:2408.04556">
  <data key="d0">arXiv preprint arXiv:2408.04556</data>
  <data key="d1">organization</data>
  <data key="d2">arXiv preprint arXiv:2408.04556 is a research paper by Yupeng Chang and others discussing bias-alleviating low-rank adaptation.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="ACM Transactions on Intelligent Systems and Technology">
  <data key="d0">ACM Transactions on Intelligent Systems and Technology</data>
  <data key="d1">organization</data>
  <data key="d2">ACM Transactions on Intelligent Systems and Technology is a journal that published a survey on the evaluation of large language models, authored by Yupeng Chang and others.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Code Alpaca">
  <data key="d0">Code Alpaca</data>
  <data key="d1">category</data>
  <data key="d2">Code Alpaca is an instruction-following LLaMA model for code generation, discussed in the context of recent advancements in language models.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Nature Machine Intelligence">
  <data key="d0">Nature Machine Intelligence</data>
  <data key="d1">organization</data>
  <data key="d2">Nature Machine Intelligence is a journal where research on parameter-efficient fine-tuning of large-scale pretrained language models is published, authored by Ning Ding and others.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="arXiv preprint arXiv:2404.04475">
  <data key="d0">arXiv preprint arXiv:2404.04475</data>
  <data key="d1">event</data>
  <data key="d2">This arXiv preprint discusses advancements in LLM fine-tuning and is part of the larger body of research available on arXiv.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Philosophical Transactions of the Royal Society of London">
  <data key="d0">Philosophical Transactions of the Royal Society of London</data>
  <data key="d1">organization</data>
  <data key="d2">This publication is a prestigious journal that contains papers of a mathematical or physical character, including works by Ronald A Fisher.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="IEEE Security and Privacy Workshops (SPW)">
  <data key="d0">IEEE Security and Privacy Workshops (SPW)</data>
  <data key="d1">event</data>
  <data key="d2">The IEEE Security and Privacy Workshops is an annual event that focuses on security and privacy topics, featuring research presentations and discussions.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Chbench">
  <data key="d0">Chbench</data>
  <data key="d1">category</data>
  <data key="d2">Chbench is a Chinese dataset designed for evaluating health in large language models, highlighting the intersection of language processing and health metrics.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Math Dataset">
  <data key="d0">Math Dataset</data>
  <data key="d1">category</data>
  <data key="d2">The Math Dataset is used for measuring mathematical problem-solving abilities and is a significant resource for evaluating LLM performance.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Lora">
  <data key="d0">Lora</data>
  <data key="d1">category</data>
  <data key="d2">Lora refers to a method for low-rank adaptation of large language models, enhancing their efficiency and performance.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Wizardmath">
  <data key="d0">Wizardmath</data>
  <data key="d1">category</data>
  <data key="d2">Wizardmath is a project aimed at empowering mathematical reasoning in large language models through reinforced instruction.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing">
  <data key="d0">Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</data>
  <data key="d1">event</data>
  <data key="d2">This conference focuses on empirical methods in natural language processing, featuring a range of studies and findings from the field.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Ji Gao">
  <data key="d0">Ji Gao</data>
  <data key="d1">person</data>
  <data key="d2">Ji Gao is a researcher known for contributions in the field of adversarial machine learning and text generation, particularly in the context of black-box generation of adversarial text sequences.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jack Lanchantin">
  <data key="d0">Jack Lanchantin</data>
  <data key="d1">person</data>
  <data key="d2">Jack Lanchantin is a researcher who has worked on adversarial machine learning, focusing on generating adversarial text sequences to evade classifiers.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Mary Lou Soffa">
  <data key="d0">Mary Lou Soffa</data>
  <data key="d1">person</data>
  <data key="d2">Mary Lou Soffa is a computer scientist recognized for her work in software engineering and computer security, contributing to the development of adversarial techniques in text generation.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Yanjun Qi">
  <data key="d0">Yanjun Qi</data>
  <data key="d1">person</data>
  <data key="d2">Yanjun Qi is a researcher involved in machine learning, particularly in the areas of adversarial learning and its applications in natural language processing.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Chenlu Guo">
  <data key="d0">Chenlu Guo</data>
  <data key="d1">person</data>
  <data key="d2">Chenlu Guo is a researcher focusing on health evaluation in large language models, contributing to the creation of datasets for this purpose.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Nuo Xu">
  <data key="d0">Nuo Xu</data>
  <data key="d1">person</data>
  <data key="d2">Nuo Xu is a researcher involved in developing datasets aimed at evaluating language models, particularly in the health domain.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Dan Hendrycks">
  <data key="d0">Dan Hendrycks</data>
  <data key="d1">person</data>
  <data key="d2">Dan Hendrycks is a researcher known for his work on evaluating machine learning models, including contributions to the Math Dataset.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Collin Burns">
  <data key="d0">Collin Burns</data>
  <data key="d1">person</data>
  <data key="d2">Collin Burns is a researcher who has collaborated on projects related to evaluating mathematical problem-solving in machine learning models.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Saurav Kadavath">
  <data key="d0">Saurav Kadavath</data>
  <data key="d1">person</data>
  <data key="d2">Saurav Kadavath is a researcher involved in machine learning evaluation, particularly in the context of mathematical datasets.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Akul Arora">
  <data key="d0">Akul Arora</data>
  <data key="d1">person</data>
  <data key="d2">Akul Arora is a researcher focused on machine learning evaluation techniques, particularly in mathematical problem-solving.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Steven Basart">
  <data key="d0">Steven Basart</data>
  <data key="d1">person</data>
  <data key="d2">Steven Basart is a researcher contributing to the evaluation of machine learning models, particularly in mathematical contexts.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Dawn Song">
  <data key="d0">Dawn Song</data>
  <data key="d1">person</data>
  <data key="d2">Dawn Song is a prominent researcher in the field of machine learning and artificial intelligence, contributing to various evaluation methodologies.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jacob Steinhardt">
  <data key="d0">Jacob Steinhardt</data>
  <data key="d1">person</data>
  <data key="d2">Jacob Steinhardt is a researcher known for his work on machine learning evaluation and safety, particularly in the context of mathematical datasets.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Edward J Hu">
  <data key="d0">Edward J Hu</data>
  <data key="d1">person</data>
  <data key="d2">Edward J Hu is a researcher focused on low-rank adaptation techniques for large language models, contributing to advancements in model efficiency.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Yelong Shen">
  <data key="d0">Yelong Shen</data>
  <data key="d1">person</data>
  <data key="d2">Yelong Shen is involved in research on adapting language models, particularly through low-rank techniques to improve performance.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Phillip Wallis">
  <data key="d0">Phillip Wallis</data>
  <data key="d1">person</data>
  <data key="d2">Phillip Wallis is a researcher contributing to the development of techniques for adapting large language models.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Zeyuan Allen-Zhu">
  <data key="d0">Zeyuan Allen-Zhu</data>
  <data key="d1">person</data>
  <data key="d2">Zeyuan Allen-Zhu is a researcher known for his work on optimization techniques in machine learning, particularly in model adaptation.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Yuanzhi Li">
  <data key="d0">Yuanzhi Li</data>
  <data key="d1">person</data>
  <data key="d2">Yuanzhi Li is a researcher contributing to the field of machine learning, particularly in the context of adapting large models.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Shean Wang">
  <data key="d0">Shean Wang</data>
  <data key="d1">person</data>
  <data key="d2">Shean Wang is a researcher focused on enhancing the performance of language models through adaptation techniques.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Lu Wang">
  <data key="d0">Lu Wang</data>
  <data key="d1">person</data>
  <data key="d2">Lu Wang is involved in research on machine learning, focusing on adaptation strategies for large language models.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Weizhu Chen">
  <data key="d0">Weizhu Chen</data>
  <data key="d1">person</data>
  <data key="d2">Weizhu Chen is a researcher known for contributions to the efficiency and performance of language models through innovative adaptation methods.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Wenxiang Jiao">
  <data key="d0">Wenxiang Jiao</data>
  <data key="d1">person</data>
  <data key="d2">Wenxiang Jiao is a researcher focused on evaluating language models, particularly in translation tasks.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Wenxuan Wang">
  <data key="d0">Wenxuan Wang</data>
  <data key="d1">person</data>
  <data key="d2">Wenxuan Wang is involved in research on language model evaluation and translation quality assessment.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jen-tse Huang">
  <data key="d0">Jen-tse Huang</data>
  <data key="d1">person</data>
  <data key="d2">Jen-tse Huang is a researcher contributing to the evaluation of language models, particularly in translation contexts.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Xing Wang">
  <data key="d0">Xing Wang</data>
  <data key="d1">person</data>
  <data key="d2">Xing Wang is a researcher focused on machine learning evaluation and its applications in translation tasks.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Zhaopeng Tu">
  <data key="d0">Zhaopeng Tu</data>
  <data key="d1">person</data>
  <data key="d2">Zhaopeng Tu is involved in research on evaluating machine learning models, particularly in language translation.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Xisen Jin">
  <data key="d0">Xisen Jin</data>
  <data key="d1">person</data>
  <data key="d2">Xisen Jin is a researcher focused on knowledge fusion and its applications in language models.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Xiang Ren">
  <data key="d0">Xiang Ren</data>
  <data key="d1">person</data>
  <data key="d2">Xiang Ren is a researcher known for contributions to natural language processing, particularly in knowledge fusion techniques.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Daniel Preotiuc-Pietro">
  <data key="d0">Daniel Preotiuc-Pietro</data>
  <data key="d1">person</data>
  <data key="d2">Daniel Preotiuc-Pietro is a researcher involved in the development of techniques for knowledge integration in language models.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Pengxiang Cheng">
  <data key="d0">Pengxiang Cheng</data>
  <data key="d1">person</data>
  <data key="d2">Pengxiang Cheng is a researcher focused on machine learning and knowledge integration techniques.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Alex Krizhevsky">
  <data key="d0">Alex Krizhevsky</data>
  <data key="d1">person</data>
  <data key="d2">Alex Krizhevsky is a researcher known for his work on deep learning and convolutional neural networks.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Geoffrey Hinton">
  <data key="d0">Geoffrey Hinton</data>
  <data key="d1">person</data>
  <data key="d2">Geoffrey Hinton is a prominent figure in artificial intelligence and deep learning, known for his contributions to neural networks.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Xuechen Li">
  <data key="d0">Xuechen Li</data>
  <data key="d1">person</data>
  <data key="d2">Xuechen Li is a researcher involved in evaluating instruction-following models and their performance.&lt;SEP&gt;Xuechen Li is a researcher who works on enhancing language models and AI systems.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Tianyi Zhang">
  <data key="d0">Tianyi Zhang</data>
  <data key="d1">person</data>
  <data key="d2">Tianyi Zhang is a researcher focused on evaluating machine learning models, particularly in instruction-following tasks.&lt;SEP&gt;Tianyi Zhang is involved in research related to language models and AI applications.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Yann Dubois">
  <data key="d0">Yann Dubois</data>
  <data key="d1">person</data>
  <data key="d2">Yann Dubois is a researcher contributing to the evaluation of language models and their capabilities.&lt;SEP&gt;Yann Dubois is a researcher focusing on AI methodologies and language understanding.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Rohan Taori">
  <data key="d0">Rohan Taori</data>
  <data key="d1">person</data>
  <data key="d2">Rohan Taori is a researcher known for his work on evaluating machine learning models, particularly in instruction-following contexts.&lt;SEP&gt;Rohan Taori is a researcher working on instruction-following models and AI methodologies.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Ishaan Gulrajani">
  <data key="d0">Ishaan Gulrajani</data>
  <data key="d1">person</data>
  <data key="d2">Ishaan Gulrajani is a researcher contributing to advancements in AI and language models.&lt;SEP&gt;Ishaan Gulrajani is a researcher focused on machine learning evaluation and performance assessment.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Carlos Guestrin">
  <data key="d0">Carlos Guestrin</data>
  <data key="d1">person</data>
  <data key="d2">Carlos Guestrin is a researcher known for contributions to machine learning and its evaluation methodologies.&lt;SEP&gt;Carlos Guestrin is a researcher known for his contributions to machine learning and AI.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Percy Liang">
  <data key="d0">Percy Liang</data>
  <data key="d1">person</data>
  <data key="d2">Percy Liang is a researcher involved in natural language processing and machine learning evaluation techniques.&lt;SEP&gt;Percy Liang is a researcher specializing in machine learning and natural language processing.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2&lt;SEP&gt;chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Tatsunori B Hashimoto">
  <data key="d0">Tatsunori B Hashimoto</data>
  <data key="d1">person</data>
  <data key="d2">Tatsunori B Hashimoto is a researcher focused on evaluating instruction-following models and their applications.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Norman Sadeh">
  <data key="d0">Norman Sadeh</data>
  <data key="d1">person</data>
  <data key="d2">Norman Sadeh is a researcher involved in natural language inference, contributing to academic publications.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Carolyn Rose">
  <data key="d0">Carolyn Rose</data>
  <data key="d1">person</data>
  <data key="d2">Carolyn Rose is a researcher in the field of natural language processing, co-authoring relevant studies.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Graham Neubig">
  <data key="d0">Graham Neubig</data>
  <data key="d1">person</data>
  <data key="d2">Graham Neubig is an academic who works on natural language processing and inference, contributing to various research papers.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Daye Nam">
  <data key="d0">Daye Nam</data>
  <data key="d1">person</data>
  <data key="d2">Daye Nam is a researcher who has worked on code understanding using language models.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Andrew Macvean">
  <data key="d0">Andrew Macvean</data>
  <data key="d1">person</data>
  <data key="d2">Andrew Macvean is a contributor to research on code understanding and programming languages.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Vincent Hellendoorn">
  <data key="d0">Vincent Hellendoorn</data>
  <data key="d1">person</data>
  <data key="d2">Vincent Hellendoorn is involved in research related to code understanding and artificial intelligence.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Bogdan Vasilescu">
  <data key="d0">Bogdan Vasilescu</data>
  <data key="d1">person</data>
  <data key="d2">Bogdan Vasilescu is a researcher focused on software engineering and programming.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Brad Myers">
  <data key="d0">Brad Myers</data>
  <data key="d1">person</data>
  <data key="d2">Brad Myers is a prominent figure in the field of human-computer interaction and software engineering.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="R OpenAI">
  <data key="d0">R OpenAI</data>
  <data key="d1">organization</data>
  <data key="d2">R OpenAI is an organization that conducts research in artificial intelligence, including the development of language models.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="GPT-4 Technical Report">
  <data key="d0">GPT-4 Technical Report</data>
  <data key="d1">event</data>
  <data key="d2">The GPT-4 Technical Report details the advancements and capabilities of the GPT-4 language model.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Stanford Alpaca">
  <data key="d0">Stanford Alpaca</data>
  <data key="d1">organization</data>
  <data key="d2">Stanford Alpaca is a model that serves as a basis for the Llama-2-13b-code-alpaca, focusing on instruction-following tasks.&lt;SEP&gt;Stanford Alpaca is a model that serves as a foundation for Llama-2-13b-code-alpaca, focusing on instruction-following tasks.&lt;SEP&gt;Stanford Alpaca is a project focused on instruction-following models in the field of AI.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2&lt;SEP&gt;chunk-9fbd0c0622e4d5fb84be83781e1c4521</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Imagenet Challenge">
  <data key="d0">Imagenet Challenge</data>
  <data key="d1">event</data>
  <data key="d2">The Imagenet Challenge is a significant competition in computer vision, assessing large-scale visual recognition.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Speech Commands Dataset">
  <data key="d0">Speech Commands Dataset</data>
  <data key="d1">event</data>
  <data key="d2">The Speech Commands Dataset is a collection of audio samples used for training speech recognition systems.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Chain-of-Thought Prompting">
  <data key="d0">Chain-of-Thought Prompting</data>
  <data key="d1">event</data>
  <data key="d2">Chain-of-Thought Prompting is a technique that enhances reasoning abilities in language models.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Richard Socher">
  <data key="d0">Richard Socher</data>
  <data key="d1">person</data>
  <data key="d2">Richard Socher is a researcher known for his contributions to natural language processing and deep learning.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Alex Perelygin">
  <data key="d0">Alex Perelygin</data>
  <data key="d1">person</data>
  <data key="d2">Alex Perelygin is a researcher who has worked on various aspects of natural language processing.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jean Wu">
  <data key="d0">Jean Wu</data>
  <data key="d1">person</data>
  <data key="d2">Jean Wu is a researcher involved in natural language processing, particularly in sentiment analysis.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jason Chuang">
  <data key="d0">Jason Chuang</data>
  <data key="d1">person</data>
  <data key="d2">Jason Chuang is a researcher who has contributed to the field of natural language processing and deep learning.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Christopher D Manning">
  <data key="d0">Christopher D Manning</data>
  <data key="d1">person</data>
  <data key="d2">Christopher D Manning is a prominent figure in the field of natural language processing, known for his work on linguistic structure and machine learning.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Andrew Y Ng">
  <data key="d0">Andrew Y Ng</data>
  <data key="d1">person</data>
  <data key="d2">Andrew Y Ng is a renowned AI researcher and educator, co-founder of Google Brain and Coursera.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Christopher Potts">
  <data key="d0">Christopher Potts</data>
  <data key="d1">person</data>
  <data key="d2">Christopher Potts is a researcher specializing in semantics and natural language processing.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Tatsunori B. Hashimoto">
  <data key="d0">Tatsunori B. Hashimoto</data>
  <data key="d1">person</data>
  <data key="d2">Tatsunori B. Hashimoto is a researcher focused on AI and its applications in language understanding.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Hugo Touvron">
  <data key="d0">Hugo Touvron</data>
  <data key="d1">person</data>
  <data key="d2">Hugo Touvron is a researcher working on advanced AI models and methodologies.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Louis Martin">
  <data key="d0">Louis Martin</data>
  <data key="d1">person</data>
  <data key="d2">Louis Martin is involved in research related to AI and machine learning.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Kevin Stone">
  <data key="d0">Kevin Stone</data>
  <data key="d1">person</data>
  <data key="d2">Kevin Stone is a researcher contributing to advancements in AI technologies.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Peter Albert">
  <data key="d0">Peter Albert</data>
  <data key="d1">person</data>
  <data key="d2">Peter Albert is a researcher focused on AI applications and model development.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Amjad Almahairi">
  <data key="d0">Amjad Almahairi</data>
  <data key="d1">person</data>
  <data key="d2">Amjad Almahairi is a researcher working on AI and machine learning methodologies.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Yasmine Babaei">
  <data key="d0">Yasmine Babaei</data>
  <data key="d1">person</data>
  <data key="d2">Yasmine Babaei is involved in AI research, particularly in model development.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Nikolay Bashlykov">
  <data key="d0">Nikolay Bashlykov</data>
  <data key="d1">person</data>
  <data key="d2">Nikolay Bashlykov is a researcher contributing to advancements in AI and machine learning.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Soumya Batra">
  <data key="d0">Soumya Batra</data>
  <data key="d1">person</data>
  <data key="d2">Soumya Batra is a researcher working on AI methodologies and applications.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Prajjwal Bhargava">
  <data key="d0">Prajjwal Bhargava</data>
  <data key="d1">person</data>
  <data key="d2">Prajjwal Bhargava is involved in AI research, particularly in model training and evaluation.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Shruti Bhosale">
  <data key="d0">Shruti Bhosale</data>
  <data key="d1">person</data>
  <data key="d2">Shruti Bhosale is a researcher focusing on AI technologies and their applications.</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="arXiv preprint arXiv:2410.09335">
  <data key="d0">arXiv preprint arXiv:2410.09335</data>
  <data key="d1">event</data>
  <data key="d2">This arXiv preprint discusses data selection at scale in machine learning, highlighting significant findings.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="ACM Transactions on Management Information Systems">
  <data key="d0">ACM Transactions on Management Information Systems</data>
  <data key="d1">organization</data>
  <data key="d2">This organization publishes research related to management information systems and technology.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="The Twelfth International Conference on Learning Representations">
  <data key="d0">The Twelfth International Conference on Learning Representations</data>
  <data key="d1">event</data>
  <data key="d2">This conference focuses on learning representations in machine learning and artificial intelligence.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="arXiv preprint arXiv:2306.01708">
  <data key="d0">arXiv preprint arXiv:2306.01708</data>
  <data key="d1">event</data>
  <data key="d2">This arXiv preprint presents research on resolving interference when merging models in machine learning.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="arXiv preprint arXiv:2408.07666">
  <data key="d0">arXiv preprint arXiv:2408.07666</data>
  <data key="d1">event</data>
  <data key="d2">This arXiv preprint discusses model merging in large language models and related applications.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Forty-first International Conference on Machine Learning">
  <data key="d0">Forty-first International Conference on Machine Learning</data>
  <data key="d1">event</data>
  <data key="d2">This conference is a major venue for presenting new research in the field of machine learning.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="mixup">
  <data key="d0">mixup</data>
  <data key="d1">category</data>
  <data key="d2">Mixup is a technique used in machine learning to improve model generalization by blending training examples.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Promptrobust">
  <data key="d0">Promptrobust</data>
  <data key="d1">category</data>
  <data key="d2">Promptrobust refers to a framework for evaluating the robustness of large language models against adversarial prompts.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Promptbench">
  <data key="d0">Promptbench</data>
  <data key="d1">category</data>
  <data key="d2">Promptbench is a unified library designed for evaluating large language models in various tasks.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Tingyu Xia">
  <data key="d0">Tingyu Xia</data>
  <data key="d1">person</data>
  <data key="d2">Tingyu Xia is a researcher who co-authored a paper on data selection at scale, emphasizing the effectiveness of random selection in machine learning.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Bowen Yu">
  <data key="d0">Bowen Yu</data>
  <data key="d1">person</data>
  <data key="d2">Bowen Yu is a co-author of the paper discussing data selection methods in machine learning.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Kai Dang">
  <data key="d0">Kai Dang</data>
  <data key="d1">person</data>
  <data key="d2">Kai Dang is a contributor to the research on data selection at scale in machine learning.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="An Yang">
  <data key="d0">An Yang</data>
  <data key="d1">person</data>
  <data key="d2">An Yang is a co-author of the paper focusing on data selection techniques in machine learning.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Yuan Tian">
  <data key="d0">Yuan Tian</data>
  <data key="d1">person</data>
  <data key="d2">Yuan Tian is a co-author of the research discussing random selection in machine learning.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Junyang Lin">
  <data key="d0">Junyang Lin</data>
  <data key="d1">person</data>
  <data key="d2">Junyang Lin is a co-author of the paper focusing on effective data selection methods for machine learning.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Frank Xing">
  <data key="d0">Frank Xing</data>
  <data key="d1">person</data>
  <data key="d2">Frank Xing is a researcher who authored a paper on designing heterogeneous language model agents for financial sentiment analysis.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Can Xu">
  <data key="d0">Can Xu</data>
  <data key="d1">person</data>
  <data key="d2">Can Xu is a co-author of the paper discussing empowering large pre-trained language models to follow complex instructions.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Qingfeng Sun">
  <data key="d0">Qingfeng Sun</data>
  <data key="d1">person</data>
  <data key="d2">Qingfeng Sun is a researcher involved in the study of large pre-trained language models for instruction following.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Kai Zheng">
  <data key="d0">Kai Zheng</data>
  <data key="d1">person</data>
  <data key="d2">Kai Zheng is a contributor to the research on enhancing language models to follow complex instructions.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Xiubo Geng">
  <data key="d0">Xiubo Geng</data>
  <data key="d1">person</data>
  <data key="d2">Xiubo Geng is a co-author of the paper on empowering language models for complex tasks.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Pu Zhao">
  <data key="d0">Pu Zhao</data>
  <data key="d1">person</data>
  <data key="d2">Pu Zhao is a researcher involved in the study of large language models and their capabilities.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jiazhan Feng">
  <data key="d0">Jiazhan Feng</data>
  <data key="d1">person</data>
  <data key="d2">Jiazhan Feng is a co-author of the research on instruction-following capabilities of language models.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Chongyang Tao">
  <data key="d0">Chongyang Tao</data>
  <data key="d1">person</data>
  <data key="d2">Chongyang Tao is a contributor to the research on enhancing language models for complex instructions.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Qingwei Lin">
  <data key="d0">Qingwei Lin</data>
  <data key="d1">person</data>
  <data key="d2">Qingwei Lin is a co-author of the paper discussing advancements in large language models.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Daxin Jiang">
  <data key="d0">Daxin Jiang</data>
  <data key="d1">person</data>
  <data key="d2">Daxin Jiang is a researcher involved in the study of empowering language models for better performance.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Evol-Instruct">
  <data key="d0">Evol-Instruct</data>
  <data key="d1">category</data>
  <data key="d2">Evol-Instruct is a method used to generate high-complexity instruction data to improve the performance of language models.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="PMLR">
  <data key="d0">PMLR</data>
  <data key="d1">organization</data>
  <data key="d2">PMLR is the publisher associated with the International Conference on Machine Learning, disseminating research in machine learning.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Proceedings of the 1st ACM Workshop on Large AI Systems and Models with Privacy and Safety Analysis">
  <data key="d0">Proceedings of the 1st ACM Workshop on Large AI Systems and Models with Privacy and Safety Analysis</data>
  <data key="d1">event</data>
  <data key="d2">This event focuses on the implications and challenges of large AI systems, including privacy and safety concerns.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Journal of Machine Learning Research">
  <data key="d0">Journal of Machine Learning Research</data>
  <data key="d1">organization</data>
  <data key="d2">The Journal of Machine Learning Research publishes significant research findings in the field of machine learning.</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="WizardMath-Mistral 7B">
  <data key="d0">WizardMath-Mistral 7B</data>
  <data key="d1">organization</data>
  <data key="d2">WizardMath-Mistral 7B is a mathematical model that excels in basic and advanced math problems, outperforming other open-source models.&lt;SEP&gt;WizardMath-Mistral 7B is a mathematical model that excels in both basic and advanced math problems, outperforming all open-source models with fewer training data.</data>
  <data key="d3">chunk-9fbd0c0622e4d5fb84be83781e1c4521</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="WizardMath 70B">
  <data key="d0">WizardMath 70B</data>
  <data key="d1">organization</data>
  <data key="d2">WizardMath 70B is a mathematical model that surpasses GPT-3.5-Turbo and Claude 2 in mathematical reasoning tasks.&lt;SEP&gt;WizardMath 70B is a mathematical model that surpasses GPT-3.5-Turbo, Claude 2, and early GPT-4 versions in mathematical reasoning tasks.</data>
  <data key="d3">chunk-9fbd0c0622e4d5fb84be83781e1c4521</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="GPT-3.5-Turbo">
  <data key="d0">GPT-3.5-Turbo</data>
  <data key="d1">organization</data>
  <data key="d2">GPT-3.5-Turbo is a language model compared against WizardMath 70B for its performance in mathematical reasoning tasks.&lt;SEP&gt;GPT-3.5-Turbo is a language model that is compared against WizardMath 70B in terms of mathematical reasoning capabilities.</data>
  <data key="d3">chunk-9fbd0c0622e4d5fb84be83781e1c4521</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Claude 2">
  <data key="d0">Claude 2</data>
  <data key="d1">organization</data>
  <data key="d2">Claude 2 is a language model evaluated alongside other models, including WizardMath 70B, for mathematical reasoning tasks.&lt;SEP&gt;Claude 2 is a language model that is evaluated alongside GPT-3.5-Turbo and WizardMath 70B for mathematical reasoning tasks.</data>
  <data key="d3">chunk-9fbd0c0622e4d5fb84be83781e1c4521</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Llama-2-13b-code-alpaca">
  <data key="d0">Llama-2-13b-code-alpaca</data>
  <data key="d1">organization</data>
  <data key="d2">Llama-2-13b-code-alpaca is a code generation model fine-tuned from Llama-2-13b to enhance code understanding and generation.&lt;SEP&gt;Llama-2-13b-code-alpaca is a code generation model fine-tuned to enhance code understanding and generation.</data>
  <data key="d3">chunk-9fbd0c0622e4d5fb84be83781e1c4521</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Hyperparameter Settings">
  <data key="d0">Hyperparameter Settings</data>
  <data key="d1">category</data>
  <data key="d2">Hyperparameter Settings refer to the configuration values used in machine learning models to control the learning process, such as scaling terms and retain ratios.</data>
  <data key="d3">chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Out-of-Distribution Dataset Selection">
  <data key="d0">Out-of-Distribution Dataset Selection</data>
  <data key="d1">event</data>
  <data key="d2">Out-of-Distribution Dataset Selection involves choosing datasets that help evaluate the robustness of models against data that differs from their training set.</data>
  <data key="d3">chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Adversarial Robustness Evaluation">
  <data key="d0">Adversarial Robustness Evaluation</data>
  <data key="d1">event</data>
  <data key="d2">Adversarial Robustness Evaluation refers to the process of testing how well models perform when faced with adversarial prompts designed to mislead them.</data>
  <data key="d3">chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Coding">
  <data key="d0">Coding</data>
  <data key="d1">category</data>
  <data key="d2">Coding is a task category in LiveBench that evaluates a model's capability to perform coding-related tasks.</data>
  <data key="d3">chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Language Comprehension">
  <data key="d0">Language Comprehension</data>
  <data key="d1">category</data>
  <data key="d2">Language Comprehension is a task category in LiveBench that tests a model's understanding of language and its nuances.</data>
  <data key="d3">chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Task Vectors">
  <data key="d0">Task Vectors</data>
  <data key="d1">category</data>
  <data key="d2">Task Vectors are mathematical representations used in machine learning to encapsulate information about tasks, which can be merged or manipulated for various purposes.</data>
  <data key="d3">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Mean Figure">
  <data key="d0">Mean Figure</data>
  <data key="d1">category</data>
  <data key="d2">Mean Figure refers to a statistical measure that represents the average of a set of values, often used in the context of data analysis.</data>
  <data key="d3">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Merged Task Vector">
  <data key="d0">Merged Task Vector</data>
  <data key="d1">category</data>
  <data key="d2">Merged Task Vector is a composite representation created by combining two or more task vectors, typically used to enhance model performance.</data>
  <data key="d3">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Parameter Value">
  <data key="d0">Parameter Value</data>
  <data key="d1">category</data>
  <data key="d2">Parameter Value denotes specific settings or configurations used in machine learning models that affect their behavior and outcomes.</data>
  <data key="d3">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Evaluation">
  <data key="d0">Evaluation</data>
  <data key="d1">event</data>
  <data key="d2">Evaluation refers to the systematic assessment of model performance using specific datasets and metrics to determine effectiveness.</data>
  <data key="d3">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Extraneous Information">
  <data key="d0">Extraneous Information</data>
  <data key="d1">category</data>
  <data key="d2">Extraneous Information refers to data or inputs that are not directly relevant to the task at hand, which can confuse or mislead models.</data>
  <data key="d3">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Sentiment Analysis Dataset">
  <data key="d0">Sentiment Analysis Dataset</data>
  <data key="d1">category</data>
  <data key="d2">Sentiment Analysis Dataset encompasses collections of textual data used for training models to classify sentiments as positive, negative, or neutral.</data>
  <data key="d3">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Grammar Correctness Dataset">
  <data key="d0">Grammar Correctness Dataset</data>
  <data key="d1">category</data>
  <data key="d2">Grammar Correctness Dataset is a collection of sentences used to evaluate the grammatical accuracy of language models.</data>
  <data key="d3">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Model Performance Metrics">
  <data key="d0">Model Performance Metrics</data>
  <data key="d1">category</data>
  <data key="d2">Model Performance Metrics are quantitative measures used to assess the effectiveness of machine learning models in various tasks.</data>
  <data key="d3">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Metric_no attack">
  <data key="d0">Metric_no attack</data>
  <data key="d1">category</data>
  <data key="d2">Metric_no attack denotes the performance metric of a model without any prompt attack applied.</data>
  <data key="d3">chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Metric_attack">
  <data key="d0">Metric_attack</data>
  <data key="d1">category</data>
  <data key="d2">Metric_attack represents the performance metric of a model when subjected to a prompt attack.</data>
  <data key="d3">chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="delta parameters">
  <data key="d0">delta parameters</data>
  <data key="d1">category</data>
  <data key="d2">Delta parameters refer to the differences between fine-tuned and pre-trained parameters in machine learning models.</data>
  <data key="d3">chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="model parameters">
  <data key="d0">model parameters</data>
  <data key="d1">category</data>
  <data key="d2">Model parameters are the variables in a model that are learned from training data and adjusted during the training process.</data>
  <data key="d3">chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="adversarial robustness">
  <data key="d0">adversarial robustness</data>
  <data key="d1">category</data>
  <data key="d2">Adversarial robustness refers to a model's ability to maintain performance when subjected to adversarial attacks.</data>
  <data key="d3">chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Super-Mario v2">
  <data key="d0">Super-Mario v2</data>
  <data key="d1">organization</data>
  <data key="d2">Super-Mario v2 is a language model that ranked first among models of the same scale on the Open LLM Leaderboard after being merged using DARE.</data>
  <data key="d3">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Marthe Ballon">
  <data key="d0">Marthe Ballon</data>
  <data key="d1">person</data>
  <data key="d2">Marthe Ballon is a researcher affiliated with the Data Analytics Lab at Vrije Universiteit Brussel, contributing to the study of reasoning and performance in language models.</data>
  <data key="d3">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Andres Algaba">
  <data key="d0">Andres Algaba</data>
  <data key="d1">person</data>
  <data key="d2">Andres Algaba is a researcher associated with both the Data Analytics Lab at Vrije Universiteit Brussel and the School of Engineering and Applied Sciences at Harvard University.</data>
  <data key="d3">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Vincent Ginis">
  <data key="d0">Vincent Ginis</data>
  <data key="d1">person</data>
  <data key="d2">Vincent Ginis is a researcher involved in the study of large language models, affiliated with the Data Analytics Lab and Harvard University.</data>
  <data key="d3">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Data Analytics Lab">
  <data key="d0">Data Analytics Lab</data>
  <data key="d1">organization</data>
  <data key="d2">The Data Analytics Lab at Vrije Universiteit Brussel conducts research on data analysis and machine learning, including studies on language models.</data>
  <data key="d3">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Vrije Universiteit Brussel">
  <data key="d0">Vrije Universiteit Brussel</data>
  <data key="d1">organization</data>
  <data key="d2">Vrije Universiteit Brussel is a university in Belgium known for its research and education in various fields, including data science.</data>
  <data key="d3">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Harvard University">
  <data key="d0">Harvard University</data>
  <data key="d1">organization</data>
  <data key="d2">Harvard University is a prestigious institution in the United States, recognized for its research and academic programs, including engineering and applied sciences.</data>
  <data key="d3">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Benjamin Mann&quot;&lt;||&gt;&quot;Tom Brown and Benjamin Mann are co-authors of a paper discussing language models as few-shot learners.">
  <data key="d0">Benjamin Mann"&lt;||&gt;"Tom Brown and Benjamin Mann are co-authors of a paper discussing language models as few-shot learners.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d2">co-authorship, research</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jared D Kaplan&quot;&lt;||&quot;Melanie Subbiah and Jared D Kaplan are co-authors of a paper discussing language models as few-shot learners.">
  <data key="d0">Jared D Kaplan"&lt;||"Melanie Subbiah and Jared D Kaplan are co-authors of a paper discussing language models as few-shot learners.</data>
  <data key="d3">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d2">co-authorship, research</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Eric Tang&quot;&lt;||&quot;Steven Basart and Eric Tang have worked together on projects evaluating machine learning models, particularly in mathematics.">
  <data key="d0">Eric Tang"&lt;||"Steven Basart and Eric Tang have worked together on projects evaluating machine learning models, particularly in mathematics.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d2">research collaboration, evaluation focus</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Jacob Steinhardt&quot;&lt;||&quot;Dawn Song and Jacob Steinhardt are both prominent researchers in machine learning, collaborating on evaluation methodologies.">
  <data key="d0">Jacob Steinhardt"&lt;||"Dawn Song and Jacob Steinhardt are both prominent researchers in machine learning, collaborating on evaluation methodologies.</data>
  <data key="d3">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d2">collaboration, research focus</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Stress test evaluation for natural language inference">
  <data key="d0">Stress test evaluation for natural language inference</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d2">Norman Sadeh contributed to the research on stress test evaluation in natural language inference, indicating his involvement in the study.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Using an llm to help with code understanding">
  <data key="d0">Using an llm to help with code understanding</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d2">Daye Nam is a contributor to research that explores the use of large language models for code understanding, indicating his role in software engineering research.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Instruction-following llama model">
  <data key="d0">Instruction-following llama model</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d2">Stanford Alpaca is a project that focuses on instruction-following models, showcasing advancements in AI technology.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Large scale visual recognition challenge">
  <data key="d0">Large scale visual recognition challenge</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d2">The Imagenet Challenge assesses large-scale visual recognition capabilities, indicating its significance in computer vision research.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Launching the speech commands dataset">
  <data key="d0">Launching the speech commands dataset</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d2">The Speech Commands Dataset was launched by R OpenAI, indicating their role in advancing speech recognition technology.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Chain-of-thought prompting elicits reasoning in large language models">
  <data key="d0">Chain-of-thought prompting elicits reasoning in large language models</data>
  <data key="d3">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d2">Chain-of-Thought Prompting is a technique used to enhance reasoning in AI, indicating its significance in model training.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Rethinking data selection at scale">
  <data key="d0">Rethinking data selection at scale</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d2">The preprint discusses data selection, a critical aspect of machine learning, reflecting ongoing research in the field.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Designing heterogeneous llm agents for financial sentiment analysis">
  <data key="d0">Designing heterogeneous llm agents for financial sentiment analysis</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d2">The research published in this journal includes advancements in designing agents for financial analysis.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Wizardlm: Empowering large pre-trained language models">
  <data key="d0">Wizardlm: Empowering large pre-trained language models</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d2">This conference features research on empowering language models, including the Wizardlm project.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Resolving interference when merging models">
  <data key="d0">Resolving interference when merging models</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d2">The preprint addresses challenges in model merging, relevant to the ongoing research in the field.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Model merging in llms, mllms, and beyond">
  <data key="d0">Model merging in llms, mllms, and beyond</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d2">This preprint discusses model merging techniques, contributing to the understanding of large language models.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="Language models are super mario">
  <data key="d0">Language models are super mario</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d2">The conference presents research on innovative techniques for language models, including the Super Mario analogy.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<node id="large AI systems">
  <data key="d0">large AI systems</data>
  <data key="d3">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d2">This event focuses on the implications of large AI systems, discussing privacy and safety analysis.</data>
  <data key="d1">UNKNOWN</data>
  <data key="d4">Mixup_model_merging.txt</data>
</node>
<edge source="Takuya Akiba" target="Sakana AI">
  <data key="d5">8.0</data>
  <data key="d6">Takuya Akiba is affiliated with Sakana AI, contributing to the organization's research in evolutionary model merging.</data>
  <data key="d7">research affiliation, collaboration</data>
  <data key="d8">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Takuya Akiba" target="Evolutionary Model Merging">
  <data key="d5">27.0</data>
  <data key="d6">Takuya Akiba initiated and led the project on Evolutionary Model Merging, contributing to its foundational methodology.&lt;SEP&gt;Takuya Akiba initiated the project on evolutionary model merging, significantly contributing to its research and development.</data>
  <data key="d7">project leadership, model development&lt;SEP&gt;research contribution, project initiation</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b&lt;SEP&gt;chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Takuya Akiba" target="Evolutionary Optimization of Model Merging Recipes">
  <data key="d5">9.0</data>
  <data key="d6">Takuya Akiba initiated the project on Evolutionary Optimization of Model Merging Recipes, playing a key role in its development.</data>
  <data key="d7">project initiation, leadership</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Takuya Akiba" target="Optuna">
  <data key="d5">20.0</data>
  <data key="d6">Takuya Akiba contributed to the creation of Optuna, which is integral to optimizing machine learning models.&lt;SEP&gt;Takuya Akiba is one of the key contributors to the Optuna framework, which is essential for hyperparameter optimization in AI models.</data>
  <data key="d7">framework development, machine learning&lt;SEP&gt;framework development, optimization</data>
  <data key="d8">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Takuya Akiba" target="Ouyang et al.">
  <data key="d5">5.0</data>
  <data key="d6">Ouyang et al. is referenced alongside Takuya Akiba in the context of model merging recipes.</data>
  <data key="d7">collaboration, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Takuya Akiba" target="Makoto Shing">
  <data key="d5">6.0</data>
  <data key="d6">Takuya Akiba and Makoto Shing are co-authors of a paper on model merging recipes.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Makoto Shing" target="Sakana AI">
  <data key="d5">8.0</data>
  <data key="d6">Makoto Shing works at Sakana AI, focusing on evolutionary optimization methods in AI development.</data>
  <data key="d7">research affiliation, collaboration</data>
  <data key="d8">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Makoto Shing" target="Evolutionary Model Merging">
  <data key="d5">8.0</data>
  <data key="d6">Makoto Shing's work on expanding model merging includes vision-language models, contributing to the project's scope.</data>
  <data key="d7">project contribution, model expansion</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Makoto Shing" target="Parameter Space Model Merging">
  <data key="d5">8.0</data>
  <data key="d6">Makoto Shing's work on expanding the parameter space model merging methodology is integral to the project's success.</data>
  <data key="d7">methodology expansion, project contribution</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Makoto Shing" target="Yujin Tang">
  <data key="d5">6.0</data>
  <data key="d6">Makoto Shing and Yujin Tang are co-authors of a paper on model merging recipes.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Yujin Tang" target="Sakana AI">
  <data key="d5">8.0</data>
  <data key="d6">Yujin Tang is part of Sakana AI, engaged in research on model merging techniques using evolutionary algorithms.</data>
  <data key="d7">research affiliation, collaboration</data>
  <data key="d8">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Yujin Tang" target="Evolutionary Model Merging">
  <data key="d5">8.0</data>
  <data key="d6">Yujin Tang's direction in data flow space model merging is a key aspect of the Evolutionary Model Merging project.</data>
  <data key="d7">project direction, model merging</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Yujin Tang" target="Neural Architecture Search">
  <data key="d5">8.0</data>
  <data key="d6">Yujin Tang incorporated ideas from neural architecture search into the data flow space model merging efforts.</data>
  <data key="d7">methodological integration, project contribution</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Yujin Tang" target="David Ha">
  <data key="d5">8.0</data>
  <data key="d6">Yujin Tang and David Ha are authors who have contributed to neuroevolution research, indicating a shared focus area.</data>
  <data key="d7">collaboration, research</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Yujin Tang" target="EvoJAX: Hardware-Accelerated Neuroevolution">
  <data key="d5">9.0</data>
  <data key="d6">Yujin Tang is the author of EvoJAX: Hardware-Accelerated Neuroevolution, indicating his contribution to the research.</data>
  <data key="d7">authorship, research collaboration</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Yujin Tang" target="Qi Sun">
  <data key="d5">6.0</data>
  <data key="d6">Yujin Tang and Qi Sun are co-authors of a paper on model merging recipes.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Qi Sun" target="Sakana AI">
  <data key="d5">8.0</data>
  <data key="d6">Qi Sun is affiliated with Sakana AI, contributing to the exploration of effective model merging strategies.</data>
  <data key="d7">research affiliation, collaboration</data>
  <data key="d8">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Qi Sun" target="Evolutionary Model Merging">
  <data key="d5">8.0</data>
  <data key="d6">Qi Sun's implementation efforts in the parameter space model merging framework are crucial to the project's success.</data>
  <data key="d7">implementation, project support</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Qi Sun" target="Model Evaluation">
  <data key="d5">8.0</data>
  <data key="d6">Qi Sun's contributions to the implementation of the framework include aspects of model evaluation and performance assessment.</data>
  <data key="d7">implementation, evaluation</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Qi Sun" target="Marc Pickett">
  <data key="d5">8.0</data>
  <data key="d6">Qi Sun and Marc Pickett co-authored a paper on transformer layers, indicating their work in AI model development.</data>
  <data key="d7">collaboration, research</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Qi Sun" target="Transformer Layers as Painters">
  <data key="d5">9.0</data>
  <data key="d6">Qi Sun is an author of the paper Transformer Layers as Painters, indicating his involvement in the research.</data>
  <data key="d7">authorship, research collaboration</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Qi Sun" target="David Ha">
  <data key="d5">6.0</data>
  <data key="d6">Qi Sun and David Ha are co-authors of a paper on model merging recipes.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="David Ha" target="Sakana AI">
  <data key="d5">8.0</data>
  <data key="d6">David Ha is a researcher at Sakana AI, involved in the development of advanced AI models through evolutionary methods.</data>
  <data key="d7">research affiliation, collaboration</data>
  <data key="d8">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="David Ha" target="Evolutionary Model Merging">
  <data key="d5">9.0</data>
  <data key="d6">David Ha's guidance and technical insight provide essential support for the Evolutionary Model Merging project.</data>
  <data key="d7">guidance, project support</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="David Ha" target="Ethical and Societal Impact">
  <data key="d5">9.0</data>
  <data key="d6">David Ha provided guidance on the ethical and societal impacts of the research, ensuring responsible AI deployment.</data>
  <data key="d7">guidance, ethical considerations</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="David Ha" target="arXiv preprint">
  <data key="d5">16.0</data>
  <data key="d6">David Ha's work on hypernetworks is documented in an arXiv preprint, contributing to the understanding of neural network architectures.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Sakana AI" target="Tokyo">
  <data key="d5">9.0</data>
  <data key="d6">Sakana AI is located in Tokyo, Japan, where it conducts its research and development activities.</data>
  <data key="d7">organization location, operational base</data>
  <data key="d8">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Sakana AI" target="Large Language Models">
  <data key="d5">8.0</data>
  <data key="d6">Sakana AI is involved in the development of Large Language Models, utilizing innovative techniques such as model merging to enhance capabilities.</data>
  <data key="d7">AI development, innovative techniques</data>
  <data key="d8">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Sakana AI" target="VLM Experiments">
  <data key="d5">16.0</data>
  <data key="d6">Sakana AI provides benchmark datasets for VLM Experiments, contributing to advancements in visual-language models.&lt;SEP&gt;Sakana AI provides benchmark datasets that are crucial for conducting VLM experiments in the research.</data>
  <data key="d7">dataset provision, model evaluation&lt;SEP&gt;dataset provision, research support</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Sakana AI" target="Japanese Stable LM Beta">
  <data key="d5">9.0</data>
  <data key="d6">Japanese Stable LM Beta is developed by Sakana AI, showcasing their efforts in language model advancements.</data>
  <data key="d7">model development, organizational effort</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Tokyo" target="Peace Tower">
  <data key="d5">8.0</data>
  <data key="d6">The Peace Tower is located in Tokyo, specifically in Shibuya Ward, making it a part of the city's landscape.</data>
  <data key="d7">location, significance</data>
  <data key="d8">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Model Merge" target="Model Merging">
  <data key="d5">9.0</data>
  <data key="d6">Evolutionary Model Merge is a specific approach to model merging that automates the discovery of effective combinations of models.</data>
  <data key="d7">methodology, optimization</data>
  <data key="d8">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Model Merge" target="Japanese LLM">
  <data key="d5">26.0</data>
  <data key="d6">The Evolutionary Model Merge approach utilizes the Japanese LLM to solve math problems, demonstrating its application in merging models across different domains.&lt;SEP&gt;The Japanese LLM was developed using the Evolutionary Model Merge approach, demonstrating its effectiveness in creating advanced models.</data>
  <data key="d7">model application, domain merging&lt;SEP&gt;model development, application</data>
  <data key="d8">chunk-72a25142a6a0a013507aa055b73531c4&lt;SEP&gt;chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Model Merge" target="Japanese VLM">
  <data key="d5">10.0</data>
  <data key="d6">The Japanese VLM was also generated through the Evolutionary Model Merge methodology, showcasing its capabilities in cultural contexts.</data>
  <data key="d7">model development, application</data>
  <data key="d8">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Model Merge" target="Japanese Math LLM">
  <data key="d5">10.0</data>
  <data key="d6">The Japanese Math LLM was developed using the Evolutionary Model Merge approach, showcasing the capabilities of this methodology in enhancing model performance.</data>
  <data key="d7">model development, optimization technique</data>
  <data key="d8">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Model Merge" target="Culturally-Aware Japanese VLM">
  <data key="d5">10.0</data>
  <data key="d6">The Culturally-Aware Japanese VLM was generated through the Evolutionary Model Merge methodology, demonstrating its effectiveness in producing culturally relevant models.</data>
  <data key="d7">model development, cultural relevance</data>
  <data key="d8">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese LLM" target="Math LLMs">
  <data key="d5">20.0</data>
  <data key="d6">The Japanese LLM is merged with Math LLMs to create a model capable of solving math problems in Japanese, indicating collaboration between different types of models.</data>
  <data key="d7">collaboration, model merging</data>
  <data key="d8">chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese LLM" target="Section 3.1">
  <data key="d5">8.0</data>
  <data key="d6">Section 3.1 details the experimental setup for evolving the Japanese LLM, highlighting its role in the merging process.</data>
  <data key="d7">experimental setup, model development</data>
  <data key="d8">chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese LLM" target="Section 3.3">
  <data key="d5">9.0</data>
  <data key="d6">Section 3.3 describes the merging of the Japanese LLM with an English VLM, illustrating cross-linguistic model integration.</data>
  <data key="d7">cross-linguistic integration, model merging</data>
  <data key="d8">chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese LLM" target="MGSM-JA task">
  <data key="d5">9.0</data>
  <data key="d6">The integration of a Japanese LLM in Model 6 contributes to its proficiency in tackling problems in the MGSM-JA task.</data>
  <data key="d7">language integration, task performance</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese LLM" target="Evolutionary Model Merging">
  <data key="d5">16.0</data>
  <data key="d6">The Japanese LLM utilizes evolutionary model merging to enhance its capabilities in math reasoning and cultural content.</data>
  <data key="d7">model enhancement, AI development</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese VLM" target="Evolutionary Model Merging">
  <data key="d5">16.0</data>
  <data key="d6">The Japanese VLM benefits from evolutionary model merging, which improves its performance on vision-language tasks.</data>
  <data key="d7">model enhancement, AI development</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Open LLM Leaderboard">
  <data key="d5">9.0</data>
  <data key="d6">Model Merging has gained prominence as a technique showcased on the Open LLM Leaderboard, demonstrating its effectiveness in creating competitive models.</data>
  <data key="d7">model performance, competitive ranking</data>
  <data key="d8">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Transfer Learning">
  <data key="d5">16.0</data>
  <data key="d6">Model Merging offers a different approach compared to Transfer Learning, focusing on combining multiple models rather than fine-tuning a single model.</data>
  <data key="d7">model techniques, machine learning</data>
  <data key="d8">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Machine Learning Community">
  <data key="d5">18.0</data>
  <data key="d6">The Machine Learning Community actively engages in developing and applying techniques like Model Merging to enhance model performance.</data>
  <data key="d7">collaboration, community engagement</data>
  <data key="d8">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Stable Diffusion">
  <data key="d5">20.0</data>
  <data key="d6">Stable Diffusion has inspired enthusiasts to create merged models by combining strengths from various fine-tuned versions.</data>
  <data key="d7">model merging, generative models</data>
  <data key="d8">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Task Arithmetic">
  <data key="d5">23.0</data>
  <data key="d6">Task Arithmetic is a specific method that enhances the process of merging models, particularly for language models.&lt;SEP&gt;Task Arithmetic is utilized in model merging to create new models through linear combinations of existing models.</data>
  <data key="d7">method, integration&lt;SEP&gt;model enhancement, merging techniques</data>
  <data key="d8">chunk-5fcb50e3e44399e7bec0bc28c47a74f7&lt;SEP&gt;chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Latent Diffusion Models">
  <data key="d5">8.0</data>
  <data key="d6">Latent Diffusion Models can benefit from model merging techniques to enhance their generative capabilities.</data>
  <data key="d7">generative models, enhancement</data>
  <data key="d8">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Parameter Interference">
  <data key="d5">9.0</data>
  <data key="d6">Model merging techniques must address parameter interference to ensure optimal performance of the merged model.</data>
  <data key="d7">performance issues, optimization</data>
  <data key="d8">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="TIES-Merging">
  <data key="d5">31.0</data>
  <data key="d6">Model Merging encompasses the TIES-Merging technique, which is a specific method for combining models to enhance performance.&lt;SEP&gt;TIES-Merging is a specific technique within the broader category of model merging that aims to improve merging performance.&lt;SEP&gt;TIES-Merging is another approach under the umbrella of model merging, addressing task conflicts during the process.</data>
  <data key="d7">methodology, performance enhancement&lt;SEP&gt;methodology, performance improvement&lt;SEP&gt;task conflict resolution, model merging</data>
  <data key="d8">chunk-c8ff785e8102dfde59dbc9309abb6b6a&lt;SEP&gt;chunk-67c2717229a96dfe2b74a6bceef933f8&lt;SEP&gt;chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="DARE">
  <data key="d5">8.0</data>
  <data key="d6">DARE is another method that enhances the process of model merging by addressing differences between models.</data>
  <data key="d7">technique, enhancement</data>
  <data key="d8">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="ComPEFT">
  <data key="d5">7.0</data>
  <data key="d6">ComPEFT focuses on the compression aspect of model merging, particularly regarding fine-tuned updates.</data>
  <data key="d7">compression, efficiency</data>
  <data key="d8">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Multimodal Model Development">
  <data key="d5">8.0</data>
  <data key="d6">Model merging is applied in multimodal model development to enhance the integration of diverse data types.</data>
  <data key="d7">data integration, application</data>
  <data key="d8">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Image Generation">
  <data key="d5">8.0</data>
  <data key="d6">Model merging techniques are increasingly used in image generation to create advanced models for producing new images.</data>
  <data key="d7">application, technology</data>
  <data key="d8">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="LLM Communities">
  <data key="d5">9.0</data>
  <data key="d6">LLM communities are actively engaged in the practice of model merging to create innovative language models.</data>
  <data key="d7">community engagement, innovation</data>
  <data key="d8">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Transformer Blocks">
  <data key="d5">9.0</data>
  <data key="d6">Model merging involves utilizing transformer blocks to create new architectures that leverage pre-trained capabilities for improved performance.</data>
  <data key="d7">model integration, performance enhancement</data>
  <data key="d8">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Frankenmerging">
  <data key="d5">8.0</data>
  <data key="d6">Frankenmerging is a specific technique used within the broader context of model merging, illustrating a method of combining models.</data>
  <data key="d7">merging techniques, model integration</data>
  <data key="d8">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Hugging Face Blog">
  <data key="d5">9.0</data>
  <data key="d6">The Hugging Face Blog features articles and discussions on model merging techniques, reflecting advancements in AI.</data>
  <data key="d7">technology discussion, community engagement</data>
  <data key="d8">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Mixup Model Merge (M3)">
  <data key="d5">9.0</data>
  <data key="d6">M3 is a specific approach to model merging that enhances the performance of merged models using randomized linear interpolation.</data>
  <data key="d7">methodology, model integration</data>
  <data key="d8">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="M&lt;sup&gt;3&lt;/sup&gt;">
  <data key="d5">27.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; is a method specifically designed to enhance the process of model merging by improving robustness.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; is a specific technique within the broader category of Model Merging that enhances model performance through a unique approach.</data>
  <data key="d7">model integration, robustness enhancement&lt;SEP&gt;model technique, performance enhancement</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Fisher Merging">
  <data key="d5">16.0</data>
  <data key="d6">Fisher Merging is a specific approach within the broader category of model merging that focuses on precise parameter integration.</data>
  <data key="d7">parameter integration, method classification</data>
  <data key="d8">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Wortsman et al.">
  <data key="d5">7.0</data>
  <data key="d6">Wortsman et al. have researched model merging techniques that relate to the concepts of M&lt;sup&gt;3&lt;/sup&gt;.</data>
  <data key="d7">research connection, model merging</data>
  <data key="d8">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Ilharco et al.">
  <data key="d5">7.0</data>
  <data key="d6">Ilharco et al. have explored various methods in model merging, contributing to the understanding of M&lt;sup&gt;3&lt;/sup&gt;.</data>
  <data key="d7">research influence, model integration</data>
  <data key="d8">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Lin et al.">
  <data key="d5">7.0</data>
  <data key="d6">Lin et al. have contributed to advancements in model merging techniques that inform the development of M&lt;sup&gt;3&lt;/sup&gt;.</data>
  <data key="d7">research contribution, model development</data>
  <data key="d8">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Mixup">
  <data key="d5">8.0</data>
  <data key="d6">Mixup is a technique that can be integrated into Model Merging to enhance the diversity of training samples.</data>
  <data key="d7">data augmentation, model improvement</data>
  <data key="d8">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Model Merging" target="Fine-Tuned Models">
  <data key="d5">8.0</data>
  <data key="d6">Model Merging often involves combining Fine-Tuned Models to create a more versatile model for various tasks.</data>
  <data key="d7">model integration, task versatility</data>
  <data key="d8">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Open LLM Leaderboard" target="mergekit">
  <data key="d5">7.0</data>
  <data key="d6">The Open LLM Leaderboard features top models that are frequently developed using techniques and tools provided by Mergekit.".</data>
  <data key="d7">model ranking, community development</data>
  <data key="d8">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Open LLM Leaderboard" target="Evolutionary Model Merging">
  <data key="d5">7.0</data>
  <data key="d6">The Open LLM Leaderboard serves as a reference point for evaluating models developed through evolutionary model merging.</data>
  <data key="d7">evaluation, performance metrics</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Open LLM Leaderboard" target="HuggingFace">
  <data key="d5">9.0</data>
  <data key="d6">HuggingFace hosts the Open LLM Leaderboard, which evaluates large language models and promotes advancements in the field.</data>
  <data key="d7">evaluation platform, AI research</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Open LLM Leaderboard" target="Hugging Face">
  <data key="d5">9.0</data>
  <data key="d6">Hugging Face hosts the Open LLM leaderboard, which ranks various large language models based on their performance.</data>
  <data key="d7">platform, model evaluation</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Open LLM Leaderboard" target="Super-Mario v2">
  <data key="d5">16.0</data>
  <data key="d6">Super-Mario v2's performance is evaluated and recognized on the Open LLM Leaderboard, highlighting its competitive edge.</data>
  <data key="d7">model ranking, performance evaluation</data>
  <data key="d8">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Evolutionary Algorithms" target="Foundation Model Development">
  <data key="d5">9.0</data>
  <data key="d6">Evolutionary Algorithms play a crucial role in Foundation Model Development, particularly in discovering effective model combinations for enhanced capabilities.</data>
  <data key="d7">optimization, model enhancement</data>
  <data key="d8">chunk-72a25142a6a0a013507aa055b73531c4</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Algorithms" target="Deep Learning">
  <data key="d5">9.0</data>
  <data key="d6">Evolutionary algorithms are applied within deep learning frameworks to optimize model architectures and enhance performance.</data>
  <data key="d7">optimization, deep learning</data>
  <data key="d8">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Machine Learning Community" target="Open Source Software Development">
  <data key="d5">14.0</data>
  <data key="d6">The principles of Open Source Software Development are reflected in the collaborative nature of the Machine Learning Community.</data>
  <data key="d7">collaborative principles, software development</data>
  <data key="d8">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Task Arithmetic" target="Mixup Model Merge">
  <data key="d5">8.0</data>
  <data key="d6">Mixup Model Merge builds upon Task Arithmetic by introducing randomness in the merging process.</data>
  <data key="d7">model merging methods, randomness</data>
  <data key="d8">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Task Arithmetic" target="Average Merging">
  <data key="d5">8.0</data>
  <data key="d6">Average Merging and Task Arithmetic are both processes that reformulate how language model parameters are combined for improved performance.</data>
  <data key="d7">parameter merging, model enhancement</data>
  <data key="d8">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Task Arithmetic" target="M&lt;sup&gt;3&lt;/sup&gt;">
  <data key="d5">26.0</data>
  <data key="d6">Combining Task Arithmetic with M&lt;sup&gt;3&lt;/sup&gt; leads to significant improvements in model performance on various datasets.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; shows improvements in Task Arithmetic when merging fine-tuned LLMs, indicating its effectiveness in this context."|</data>
  <data key="d7">method integration, performance boost&lt;SEP&gt;model merging, performance enhancement</data>
  <data key="d8">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Task Arithmetic" target="TIES-Merging">
  <data key="d5">7.0</data>
  <data key="d6">Task Arithmetic and TIES-Merging are both methods used for merging model parameters in machine learning.".</data>
  <data key="d7">model merging methods, parameter adjustment</data>
  <data key="d8">chunk-9fbd0c0622e4d5fb84be83781e1c4521</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Model Soup" target="Weighted Model Averaging">
  <data key="d5">9.0</data>
  <data key="d6">Model Soup is a practical application of Weighted Model Averaging to improve machine learning model performance.</data>
  <data key="d7">model techniques, performance improvement</data>
  <data key="d8">chunk-5fcb50e3e44399e7bec0bc28c47a74f7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="TIES-Merging" target="DARE">
  <data key="d5">46.0</data>
  <data key="d6">Both TIES-Merging and DARE are methods focused on improving the performance and effectiveness of model merging in machine learning.".&lt;SEP&gt;TIES-Merging and DARE are both methods employed for merging models to enhance their capabilities.&lt;SEP&gt;TIES-Merging enhances its functionality through the integration of DARE, allowing for more precise model merging techniques.&lt;SEP&gt;DARE complements TIES-Merging by reducing parameter interference during the merging of models, enhancing overall task performance.</data>
  <data key="d7">model enhancement, integration&lt;SEP&gt;model merging techniques, performance enhancement&lt;SEP&gt;model merging, optimization techniques&lt;SEP&gt;sparsification, model performance</data>
  <data key="d8">chunk-568ea9e09f85a7b95cfe61d48b0ba893&lt;SEP&gt;chunk-67c2717229a96dfe2b74a6bceef933f8&lt;SEP&gt;chunk-77432ea4d82a2648008d6d26c50ccdc9&lt;SEP&gt;chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</edge>
<edge source="TIES-Merging" target="Parameter Space (PS)">
  <data key="d5">8.0</data>
  <data key="d6">TIES-Merging operates within the parameter space to refine how model weights are integrated during merging processes.</data>
  <data key="d7">model optimization, parameter integration</data>
  <data key="d8">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="TIES-Merging" target="Data Flow Space (DFS)">
  <data key="d5">7.0</data>
  <data key="d6">Merging in the Data Flow Space (DFS) complements TIES-Merging by preserving original weights while optimizing inference paths.</data>
  <data key="d7">model merging, optimization</data>
  <data key="d8">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="TIES-Merging" target="Prateek Yadav">
  <data key="d5">9.0</data>
  <data key="d6">Prateek Yadav is an author of TIES-Merging, indicating his involvement in the research.</data>
  <data key="d7">authorship, research collaboration</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="TIES-Merging" target="Yadav et al.">
  <data key="d5">9.0</data>
  <data key="d6">Yadav et al. proposed TIES-Merging as a method to resolve task conflicts in model merging.</data>
  <data key="d7">methodology development, task resolution</data>
  <data key="d8">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="TIES-Merging" target="M&lt;sup&gt;3&lt;/sup&gt;">
  <data key="d5">27.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; is an approach that can be integrated into the TIES-Merging framework for improved model performance during merging.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; is utilized in TIES-Merging, leading to significant performance gains in LLMs."|</data>
  <data key="d7">model integration, performance enhancement&lt;SEP&gt;model merging, performance improvement</data>
  <data key="d8">chunk-568ea9e09f85a7b95cfe61d48b0ba893&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="TIES-Merging" target="WizardLM-13B">
  <data key="d5">8.0</data>
  <data key="d6">TIES-Merging was applied to WizardLM-13B to evaluate its impact on performance metrics during testing.</data>
  <data key="d7">performance enhancement, model improvement</data>
  <data key="d8">chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="TIES-Merging" target="Performance Metrics">
  <data key="d5">8.0</data>
  <data key="d6">TIES-Merging is assessed through various performance metrics to evaluate its effectiveness in enhancing language models.</data>
  <data key="d7">performance evaluation, model enhancement</data>
  <data key="d8">chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="TIES-Merging" target="Hyperparameter Settings">
  <data key="d5">8.0</data>
  <data key="d6">TIES-Merging involves specific hyperparameter settings that dictate how model outputs are merged in machine learning processes.</data>
  <data key="d7">model optimization, configuration</data>
  <data key="d8">chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="DARE" target="Mixup Model Merge (M3)">
  <data key="d5">8.0</data>
  <data key="d6">DARE can be combined with M3 to achieve superior results in model merging, enhancing overall performance.</data>
  <data key="d7">performance enhancement, technique combination</data>
  <data key="d8">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="DARE" target="M&lt;sup&gt;3&lt;/sup&gt;">
  <data key="d5">30.0</data>
  <data key="d6">DARE is used alongside M&lt;sup&gt;3&lt;/sup&gt; to improve the performance of model merging techniques.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; and DARE are combined to explore their effects on model merging performance, showing that they can complement each other.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; and DARE are combined to explore their effects on model merging performance.</data>
  <data key="d7">model enhancement, performance improvement&lt;SEP&gt;performance improvement, sparsification</data>
  <data key="d8">chunk-c8ff785e8102dfde59dbc9309abb6b6a&lt;SEP&gt;chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="DARE" target="delta parameters">
  <data key="d5">7.0</data>
  <data key="d6">Delta parameters are reduced in redundancy by DARE to enhance model performance during tasks.</data>
  <data key="d7">parameter reduction, model efficiency</data>
  <data key="d8">chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="DARE" target="model parameters">
  <data key="d5">8.0</data>
  <data key="d6">DARE targets model parameters to optimize their use in fine-tuned models, improving task performance.</data>
  <data key="d7">parameter optimization, task performance</data>
  <data key="d8">chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="DARE" target="Super-Mario v2">
  <data key="d5">18.0</data>
  <data key="d6">DARE was used to merge models, resulting in the creation of Super-Mario v2, which achieved top ranking on the Open LLM Leaderboard.</data>
  <data key="d7">model merging, performance enhancement</data>
  <data key="d8">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="DARE" target="AlpacaEval">
  <data key="d5">14.0</data>
  <data key="d6">DARE's effectiveness in model merging is demonstrated through evaluations on benchmarks like AlpacaEval.</data>
  <data key="d7">benchmark evaluation, model performance</data>
  <data key="d8">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="DARE" target="GSM8K">
  <data key="d5">14.0</data>
  <data key="d6">DARE contributes to improved task performance as shown in evaluations on GSM8K, a benchmark for language models.</data>
  <data key="d7">benchmark evaluation, model performance</data>
  <data key="d8">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="DARE" target="MBPP">
  <data key="d5">14.0</data>
  <data key="d6">The performance of models merged with DARE is assessed on MBPP, showcasing its capabilities in programming tasks.</data>
  <data key="d7">benchmark evaluation, model performance</data>
  <data key="d8">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="mergekit" target="Mistral">
  <data key="d5">9.0</data>
  <data key="d6">Mergekit provides tools and recipes for merging models, including those based on the Mistral architecture, facilitating the creation of new models.".</data>
  <data key="d7">toolkit, model architecture</data>
  <data key="d8">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="mergekit" target="Frankenmerging">
  <data key="d5">8.0</data>
  <data key="d6">Frankenmerging is one of the experimental methods supported by Mergekit, enabling users to create new model architectures.".</data>
  <data key="d7">experimental techniques, model creation</data>
  <data key="d8">chunk-689927e59bcb702105a164ca12f7f6b1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="mergekit" target="Charles O. Goddard">
  <data key="d5">9.0</data>
  <data key="d6">Charles O. Goddard is associated with the mergekit project, which focuses on AI model development and optimization.</data>
  <data key="d7">project involvement, AI development</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Frankenmerging" target="MGSM-JA">
  <data key="d5">9.0</data>
  <data key="d6">Frankenmerging resulted in a score of 0 in the MGSM-JA task, indicating poor model performance.</data>
  <data key="d7">model evaluation, performance</data>
  <data key="d8">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Frankenmerging" target="Abel 7B 002">
  <data key="d5">7.0</data>
  <data key="d6">Abel 7B 002 was utilized in the Frankenmerging method, contributing to the performance evaluations.</data>
  <data key="d7">model source, comparative analysis</data>
  <data key="d8">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Frankenmerging" target="TIES-Merge">
  <data key="d5">8.0</data>
  <data key="d6">Both TIES-Merge and Frankenmerging were compared in terms of performance in model merging experiments.</data>
  <data key="d7">model comparison, performance evaluation</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Neural Architecture Search" target="Neural Architecture Search (NAS)">
  <data key="d5">8.0</data>
  <data key="d6">Both evolutionary neural architecture search and NAS aim to discover optimal neural network architectures, with evolutionary methods providing a systematic approach.</data>
  <data key="d7">architecture discovery, optimization</data>
  <data key="d8">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Neural Architecture Search (NAS)" target="Weight Agnostic Neural Networks">
  <data key="d5">7.0</data>
  <data key="d6">NAS methods often focus on discovering architectures that can be optimized without reliance on weight training, aligning with the principles of weight agnostic networks.</data>
  <data key="d7">architecture optimization, training efficiency</data>
  <data key="d8">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Neural Architecture Search (NAS)" target="SMASH">
  <data key="d5">9.0</data>
  <data key="d6">SMASH is a specific NAS method that enhances the efficiency of architecture search by avoiding extensive training, showcasing a relationship with broader NAS techniques.</data>
  <data key="d7">efficiency, architecture search</data>
  <data key="d8">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Neural Architecture Search (NAS)" target="Hypernetwork">
  <data key="d5">9.0</data>
  <data key="d6">Hypernetworks are utilized in NAS methods to estimate weights for candidate architectures, improving the efficiency of architecture search.</data>
  <data key="d7">architecture search, weight estimation</data>
  <data key="d8">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="NEAT" target="Weight Agnostic Neural Networks">
  <data key="d5">8.0</data>
  <data key="d6">NEAT and weight agnostic neural networks both focus on evolving neural network structures without conventional training methods, highlighting innovative approaches to neural network design.</data>
  <data key="d7">evolutionary algorithms, neural design</data>
  <data key="d8">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Data Flow Space (DFS)" target="Merging Configuration Parameters">
  <data key="d5">7.0</data>
  <data key="d6">Data flow space interacts with merging configuration parameters to determine how data is processed in merged models.</data>
  <data key="d7">data processing, model configuration</data>
  <data key="d8">chunk-cb6f1dcffa51e91fd8d2edb8ee1b44b7</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Data Flow Space (DFS)" target="Inference Path">
  <data key="d5">8.0</data>
  <data key="d6">The Inference Path is optimized within the Data Flow Space (DFS) during model merging to improve processing efficiency.</data>
  <data key="d7">processing optimization, model efficiency</data>
  <data key="d8">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Merging Configuration Parameters" target="CMA-ES">
  <data key="d5">9.0</data>
  <data key="d6">CMA-ES optimizes the merging configuration parameters to improve model performance based on task-specific metrics.</data>
  <data key="d7">optimization, performance improvement</data>
  <data key="d8">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Merging Configuration Parameters" target="Evolutionary Search">
  <data key="d5">8.0</data>
  <data key="d6">Evolutionary Search is employed to navigate the search space of merging configuration parameters, enhancing the merging process.</data>
  <data key="d7">search optimization, parameter tuning</data>
  <data key="d8">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="CMA-ES" target="EvoJAX">
  <data key="d5">18.0</data>
  <data key="d6">EvoJAX employs the CMA-ES algorithm for optimizing model parameters during the merging process, enhancing model performance.&lt;SEP&gt;EvoJAX utilizes the CMA-ES algorithm for optimizing model parameters in the merging process.</data>
  <data key="d7">optimization, algorithm utilization</data>
  <data key="d8">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="CMA-ES" target="Model #5">
  <data key="d5">9.0</data>
  <data key="d6">Model #5 employs CMA-ES as part of its optimization strategy to enhance search efficiency and flexibility in layer management.</data>
  <data key="d7">optimization strategy, model performance</data>
  <data key="d8">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Layer-wise Merging" target="Granular Merging">
  <data key="d5">8.0</data>
  <data key="d6">Layer-wise Merging is a type of Granular Merging that focuses on optimizing individual layers for better performance.</data>
  <data key="d7">optimization, model refinement</data>
  <data key="d8">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Input/Output Embedding Layers" target="Transformer Block">
  <data key="d5">9.0</data>
  <data key="d6">Input/Output Embedding Layers are integral parts of the Transformer Block, facilitating data transformation in neural networks.</data>
  <data key="d7">model architecture, data processing</data>
  <data key="d8">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Sparsification" target="Weight Mixing">
  <data key="d5">7.0</data>
  <data key="d6">Sparsification can be used in conjunction with Weight Mixing to enhance the efficiency of model merging.</data>
  <data key="d7">efficiency, model optimization</data>
  <data key="d8">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Search Space" target="Layer Indices">
  <data key="d5">8.0</data>
  <data key="d6">The Search Space is defined by the Layer Indices, which represent the possible configurations to explore during model optimization.</data>
  <data key="d7">model exploration, optimization</data>
  <data key="d8">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Layer Indices" target="Distribution Shift">
  <data key="d5">6.0</data>
  <data key="d6">Distribution Shift may occur when inputs are processed through different Layer Indices, affecting model performance.</data>
  <data key="d7">performance impact, model dynamics</data>
  <data key="d8">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Sequential Order" target="Indicator Array">
  <data key="d5">7.0</data>
  <data key="d6">The Sequential Order of layers is managed by the Indicator Array during the merging process to determine layer inclusion.</data>
  <data key="d7">layer management, merging process</data>
  <data key="d8">chunk-67c2717229a96dfe2b74a6bceef933f8</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Indicator Array" target="Layer Stacking">
  <data key="d5">7.0</data>
  <data key="d6">The Indicator Array is used to manage Layer Stacking in the models, helping to track layer configurations during processing.</data>
  <data key="d7">data structure, model management</data>
  <data key="d8">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="PS Merging" target="DFS Merging">
  <data key="d5">18.0</data>
  <data key="d6">PS Merging is applied first to optimize models before DFS Merging is used to refine them, indicating a sequential relationship in the merging process.</data>
  <data key="d7">model optimization, sequential methods</data>
  <data key="d8">chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="PS Merging" target="Hybrid Model">
  <data key="d5">8.0</data>
  <data key="d6">The Hybrid Model incorporates PS Merging as one of its strategies to enhance performance on tasks.</data>
  <data key="d7">model strategy, performance enhancement</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="DFS Merging" target="Hybrid Model">
  <data key="d5">8.0</data>
  <data key="d6">DFS Merging is a critical component of the Hybrid Model that contributes to its overall performance improvements.</data>
  <data key="d7">model strategy, performance enhancement</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="DFS Merging" target="PS+DFS">
  <data key="d5">16.0</data>
  <data key="d6">DFS Merging is a method that is part of the PS+DFS approach, combining models for enhanced performance.</data>
  <data key="d7">methodology, model integration</data>
  <data key="d8">chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="DFS Merging" target="Ablation Studies">
  <data key="d5">7.0</data>
  <data key="d6">Ablation Studies help understand the effectiveness of DFS Merging by analyzing different configurations and their impact on performance.</data>
  <data key="d7">experimental analysis, model evaluation</data>
  <data key="d8">chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MGSM Dataset" target="GSM8k Dataset">
  <data key="d5">16.0</data>
  <data key="d6">The MGSM Dataset is derived from the GSM8k Dataset, indicating a direct relationship in their usage for evaluating language models on math problems.</data>
  <data key="d7">dataset relationship, evaluation</data>
  <data key="d8">chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="CMA-ES Algorithm" target="Optuna">
  <data key="d5">14.0</data>
  <data key="d6">The CMA-ES Algorithm is implemented within Optuna for hyperparameter optimization, showing how the two are interconnected in the model optimization process.</data>
  <data key="d7">optimization techniques, software integration</data>
  <data key="d8">chunk-dcdf43ea95113f10f30710b35f3ca7f1</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Optuna" target="Stability AI">
  <data key="d5">17.0</data>
  <data key="d6">Stability AI is associated with the Optuna framework, which is utilized for hyperparameter optimization in AI models.&lt;SEP&gt;Stability AI utilizes the Optuna framework for optimizing its AI models, enhancing their performance and efficiency.</data>
  <data key="d7">AI development, optimization&lt;SEP&gt;AI optimization, framework usage</data>
  <data key="d8">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MGSM" target="Model 1">
  <data key="d5">9.0</data>
  <data key="d6">MGSM provides the test set that evaluates Model 1's performance in Japanese Math tasks.</data>
  <data key="d7">evaluation, performance testing</data>
  <data key="d8">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MGSM" target="Model 2">
  <data key="d5">9.0</data>
  <data key="d6">MGSM is used to assess Model 2's mathematical proficiency in the Japanese context.</data>
  <data key="d7">evaluation, performance testing</data>
  <data key="d8">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MGSM" target="Model 3">
  <data key="d5">9.0</data>
  <data key="d6">MGSM benchmarks Model 3's capabilities, revealing its low scores in Japanese language proficiency.</data>
  <data key="d7">evaluation, performance testing</data>
  <data key="d8">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model 1" target="Model 4">
  <data key="d5">16.0</data>
  <data key="d6">Model 4, as an optimized merged model, is compared against Model 1 to demonstrate performance improvements in the MGSM-JA benchmark.&lt;SEP&gt;Model 4, being an optimized merged model, is compared against Model 1 to demonstrate performance improvements.</data>
  <data key="d7">model comparison, performance enhancement</data>
  <data key="d8">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model 1" target="MGSM's Test Set">
  <data key="d5">9.0</data>
  <data key="d6">MGSM's Test Set is used to evaluate Model 1's performance in Japanese Math tasks, providing a benchmark for its capabilities.</data>
  <data key="d7">evaluation, benchmark</data>
  <data key="d8">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model 2" target="Model 4">
  <data key="d5">16.0</data>
  <data key="d6">Model 4's performance is evaluated against Model 2, showcasing advancements in accuracy and proficiency in Japanese Math tasks.&lt;SEP&gt;Model 4's performance is evaluated in relation to Model 2, showcasing advancements in accuracy.</data>
  <data key="d7">model comparison, performance enhancement</data>
  <data key="d8">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model 2" target="MGSM's Test Set">
  <data key="d5">9.0</data>
  <data key="d6">MGSM's Test Set is utilized to assess Model 2's mathematical abilities in the context of Japanese language tasks.</data>
  <data key="d7">evaluation, benchmark</data>
  <data key="d8">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model 3" target="Model 5">
  <data key="d5">16.0</data>
  <data key="d6">Model 5 shows enhanced performance compared to Model 3, indicating successful merging strategies and improved mathematical capabilities.&lt;SEP&gt;Model 5 shows enhanced performance compared to Model 3, indicating successful merging strategies.</data>
  <data key="d7">model comparison, performance enhancement</data>
  <data key="d8">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model 3" target="MGSM's Test Set">
  <data key="d5">9.0</data>
  <data key="d6">MGSM's Test Set benchmarks Model 3's performance, revealing its limitations in Japanese language proficiency.</data>
  <data key="d7">evaluation, benchmark</data>
  <data key="d8">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model 4" target="Model 6">
  <data key="d5">16.0</data>
  <data key="d6">Model 6 integrates both merging strategies and is compared to Model 4 for further performance improvements on the benchmark tasks.&lt;SEP&gt;Model 6 integrates both merging strategies and is compared to Model 4 for performance improvements.</data>
  <data key="d7">model comparison, performance enhancement</data>
  <data key="d8">chunk-48d7bed100b753d40cd980d00870f149</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model 6" target="MGSM-JA task">
  <data key="d5">8.0</data>
  <data key="d6">Model 6 shows enhancements in performance when applied to the MGSM-JA task, indicating its effectiveness in mathematical problem-solving.</data>
  <data key="d7">performance enhancement, model evaluation</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MGSM-JA task" target="Ours (DFS)">
  <data key="d5">7.0</data>
  <data key="d6">Ours (DFS) is tested on the MGSM-JA task, demonstrating its capabilities in mathematical problem-solving.</data>
  <data key="d7">task application, model evaluation</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Shisa Gamma 7b v1" target="JP-LMEH">
  <data key="d5">8.0</data>
  <data key="d6">Shisa Gamma 7b v1 serves as a baseline in the JP-LMEH benchmark, highlighting its role in evaluating language models.</data>
  <data key="d7">benchmark comparison, evaluation standard</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="WizardMath 7B V1.1" target="JP-LMEH">
  <data key="d5">7.0</data>
  <data key="d6">WizardMath 7B V1.1 is evaluated in the JP-LMEH tasks, providing insights into its language capabilities.</data>
  <data key="d7">benchmark evaluation, language model</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Abel 7B 002" target="JP-LMEH">
  <data key="d5">6.0</data>
  <data key="d6">Abel 7B 002 is also evaluated within the JP-LMEH framework, contributing to the understanding of its performance.</data>
  <data key="d7">model assessment, language proficiency</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Ours (PS)" target="JP-LMEH">
  <data key="d5">9.0</data>
  <data key="d6">Ours (PS) achieved high scores in the JP-LMEH benchmark, showcasing its effectiveness in Japanese language tasks.</data>
  <data key="d7">evaluation success, model performance</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Ours (PS)" target="Ours (DFS)">
  <data key="d5">18.0</data>
  <data key="d6">Ours (PS) and Ours (DFS) are merged models that compile performance metrics from different sources.".</data>
  <data key="d7">model integration, performance metrics</data>
  <data key="d8">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Ours (PS)" target="Ours (PS+DFS)">
  <data key="d5">18.0</data>
  <data key="d6">Ours (PS) contributes to the data used in Ours (PS+DFS), indicating a relationship in model performance.".</data>
  <data key="d7">model integration, performance metrics</data>
  <data key="d8">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Ours (PS)" target="7B merged models">
  <data key="d5">10.0</data>
  <data key="d6">Ours (PS) is a part of the 7B merged models category, indicating its integration of various 7B models.".</data>
  <data key="d7">category classification, model type</data>
  <data key="d8">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Ours (DFS)" target="CMA-ES Optimization Results">
  <data key="d5">16.0</data>
  <data key="d6">The CMA-ES optimization results are derived from the merging efforts of Ours (DFS), showcasing the effectiveness of the model.")</data>
  <data key="d7">model optimization, performance evaluation</data>
  <data key="d8">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Hybrid Model" target="MGSM Scores">
  <data key="d5">9.0</data>
  <data key="d6">The effectiveness of the Hybrid Model is evaluated using MGSM Scores, which reflect its performance on mathematical tasks.</data>
  <data key="d7">performance evaluation, task assessment</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Hybrid Model" target="Appendix A">
  <data key="d5">7.0</data>
  <data key="d6">Appendix A provides additional details and comparisons relevant to the Hybrid Model's performance and evaluations.</data>
  <data key="d7">additional information, model assessment</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Hybrid Model" target="Appendix C">
  <data key="d5">8.0</data>
  <data key="d6">Appendix C showcases examples that illustrate the capabilities of the Hybrid Model in specific tasks.</data>
  <data key="d7">demonstration, model utility</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese StableLM 70B" target="MGSM Scores">
  <data key="d5">9.0</data>
  <data key="d6">Japanese StableLM 70B serves as a benchmark for comparing the MGSM Scores of other models, indicating its significance in evaluations.</data>
  <data key="d7">benchmark comparison, evaluation standard</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="JSQuAD" target="JP-LMEH">
  <data key="d5">8.0</data>
  <data key="d6">JSQuAD is one of the tasks evaluated within the JP-LMEH benchmark, measuring model performance in question answering.</data>
  <data key="d7">benchmark task, evaluation</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="JNLI" target="JP-LMEH">
  <data key="d5">8.0</data>
  <data key="d6">JNLI is included in the JP-LMEH framework, assessing models' capabilities in natural language inference in Japanese.</data>
  <data key="d7">benchmark task, evaluation</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MARC" target="JP-LMEH">
  <data key="d5">7.0</data>
  <data key="d6">MARC is another task within the JP-LMEH benchmark that evaluates models' reading comprehension abilities.</data>
  <data key="d7">benchmark task, evaluation</data>
  <data key="d8">chunk-294b5f8663a40bb700f33a7cbc7c4344</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Ours (PS+DFS)" target="CMA-ES Optimization Results">
  <data key="d5">16.0</data>
  <data key="d6">The results of CMA-ES optimization are influenced by the configurations used in Ours (PS+DFS), indicating a connection between the two.")</data>
  <data key="d7">model optimization, performance relationship</data>
  <data key="d8">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Ours (PS+DFS)" target="DARE-TIES">
  <data key="d5">7.0</data>
  <data key="d6">Ours (PS+DFS) utilizes DARE-TIES parameters for evolutionary merging, indicating a methodological relationship.")</data>
  <data key="d7">methodology, parameter utilization</data>
  <data key="d8">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Llama 2 70B" target="Parameter Configuration Post PS Merging">
  <data key="d5">14.0</data>
  <data key="d6">The performance metrics of Llama 2 70B are part of the parameter configuration derived from PS merging efforts.")</data>
  <data key="d7">model performance, parameter analysis</data>
  <data key="d8">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese Stable LM 70B" target="CMA-ES Optimization Results">
  <data key="d5">18.0</data>
  <data key="d6">The Japanese Stable LM 70B is noted for its significant contribution to the CMA-ES optimization results, indicating its importance in the analysis.")</data>
  <data key="d7">model contribution, optimization results</data>
  <data key="d8">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese Stable LM 70B" target="Mistral Base Model">
  <data key="d5">9.0</data>
  <data key="d6">The Japanese Stable LM 70B is built on the Mistral Base Model, indicating a direct lineage in model development.")</data>
  <data key="d7">model lineage, development</data>
  <data key="d8">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Swallow 70B" target="Parameter Configuration Post PS Merging">
  <data key="d5">14.0</data>
  <data key="d6">Swallow 70B's performance metrics are included in the parameter configuration analysis post PS merging.")</data>
  <data key="d7">model performance, parameter analysis</data>
  <data key="d8">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Mistral Base Model" target="Shisa-Gamma-7B-v1">
  <data key="d5">8.0</data>
  <data key="d6">Shisa-Gamma-7B-v1 is based on the Mistral Base Model and benefits from its extensive pretraining.")</data>
  <data key="d7">model foundation, fine-tuning</data>
  <data key="d8">chunk-f96b71f942f04756e590fdfa1786ae98</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="DARE-TIES" target="JP-LMEH">
  <data key="d5">6.0</data>
  <data key="d6">DARE-TIES was compared in terms of average performance metrics in the JP-LMEH benchmark.</data>
  <data key="d7">model evaluation, performance</data>
  <data key="d8">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="DARE-TIES" target="WizardMath 7B v1.1">
  <data key="d5">8.0</data>
  <data key="d6">WizardMath 7B v1.1 served as a source model for DARE-TIES, facilitating direct performance comparisons.</data>
  <data key="d7">model source, comparative analysis</data>
  <data key="d8">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="DARE-TIES" target="PS model">
  <data key="d5">7.0</data>
  <data key="d6">DARE-TIES performed better than other baseline models, including the PS model, in specific comparisons.</data>
  <data key="d7">model performance, comparative analysis</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="DARE-TIES" target="Evolutionary Model Merging">
  <data key="d5">8.0</data>
  <data key="d6">DARE-TIES is a method applied in the context of evolutionary model merging to enhance model performance.</data>
  <data key="d7">technique, model integration</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MGSM-JA" target="TIES-Merge">
  <data key="d5">5.0</data>
  <data key="d6">TIES-Merge was evaluated based on its performance in the MGSM-JA task, showing low effectiveness.</data>
  <data key="d7">model evaluation, performance</data>
  <data key="d8">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MGSM-JA" target="PS model">
  <data key="d5">9.0</data>
  <data key="d6">The PS model achieved a higher MGSM-JA score compared to other models, indicating better performance.</data>
  <data key="d7">performance metrics, model evaluation</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MGSM-JA" target="Llama-2-13b">
  <data key="d5">16.0</data>
  <data key="d6">Llama-2-13b's performance is evaluated using the MGSM-JA scoring metric, reflecting its effectiveness in English tasks.</data>
  <data key="d7">performance evaluation, model scoring</data>
  <data key="d8">chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MGSM-JA" target="MetaMath-13B-V1.0">
  <data key="d5">14.0</data>
  <data key="d6">MetaMath-13B-V1.0's performance is also evaluated using the MGSM-JA metric, focusing on its capabilities in math tasks.</data>
  <data key="d7">performance evaluation, model scoring</data>
  <data key="d8">chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MGSM-JA" target="WizardMath-7B-V1.1">
  <data key="d5">20.0</data>
  <data key="d6">WizardMath-7B-V1.1 achieves a high score on the GSM8k benchmark, indicating its strong performance in math-related tasks.</data>
  <data key="d7">performance evaluation, model scoring</data>
  <data key="d8">chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MGSM-JA" target="Performance Comparison">
  <data key="d5">9.0</data>
  <data key="d6">Performance Comparison involves using the MGSM-JA metric to evaluate and compare the effectiveness of various models.</data>
  <data key="d7">evaluation methodology, model assessment</data>
  <data key="d8">chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="JP-LMEH" target="fine-tuning">
  <data key="d5">6.0</data>
  <data key="d6">Fine-tuning often led to significant decreases in JP-LMEH scores, indicating its impact on model performance.</data>
  <data key="d7">fine-tuning effects, performance issues</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="JP-LMEH" target="ELYZA-japanese-Llama-2-13b-instruct">
  <data key="d5">16.0</data>
  <data key="d6">The performance of ELYZA-japanese-Llama-2-13b-instruct is assessed using the JP-LMEH metric, indicating its effectiveness in Japanese tasks.</data>
  <data key="d7">performance evaluation, model scoring</data>
  <data key="d8">chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="TIES-Merge" target="Shisa Gamma 7B v1">
  <data key="d5">8.0</data>
  <data key="d6">Shisa Gamma 7B v1 was used as a source model for the TIES-Merge method, allowing for performance comparison.</data>
  <data key="d7">model source, comparative analysis</data>
  <data key="d8">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="W_{ij}" target="10B model">
  <data key="d5">9.0</data>
  <data key="d6">The scaling parameters W_{ij} are crucial for the performance of the 10B model, as indicated by the study's findings.</data>
  <data key="d7">model performance, parameter significance</data>
  <data key="d8">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="W_{ij}" target="Ablation Studies">
  <data key="d5">8.0</data>
  <data key="d6">Ablation Studies were conducted to assess the impact of the scaling parameters W_{ij} on model performance.</data>
  <data key="d7">parameter analysis, model evaluation</data>
  <data key="d8">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="A and B" target="10B model">
  <data key="d5">8.0</data>
  <data key="d6">Models A and B were merged to create the 10B model, which was evaluated for performance.</data>
  <data key="d7">model merging, evaluation</data>
  <data key="d8">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="PS" target="10B model">
  <data key="d5">7.0</data>
  <data key="d6">The PS method was used in the creation of the 10B model, influencing its performance outcomes.</data>
  <data key="d7">method application, model performance</data>
  <data key="d8">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="DFS" target="10B model">
  <data key="d5">7.0</data>
  <data key="d6">The DFS method also contributed to the development of the 10B model, impacting its evaluation results.</data>
  <data key="d7">method application, model performance</data>
  <data key="d8">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Unoptimized Model Merging" target="Table 3">
  <data key="d5">6.0</data>
  <data key="d6">Unoptimized Model Merging methods are summarized in Table 3, showcasing their performance metrics.</data>
  <data key="d7">performance summary, model comparison</data>
  <data key="d8">chunk-d8ed04169bf3a7ed0de2249060ee26fb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Ablation Studies" target="GPU Memory Requirement">
  <data key="d5">6.0</data>
  <data key="d6">GPU Memory Requirement impacts the feasibility of conducting Ablation Studies, as certain configurations may require more resources.</data>
  <data key="d7">resource constraints, experimental limitations</data>
  <data key="d8">chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="PS model" target="PS+DFS">
  <data key="d5">9.0</data>
  <data key="d6">The PS model is part of the hybrid merging method PS+DFS, which aims to achieve superior performance on target tasks.</data>
  <data key="d7">model enhancement, performance improvement</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="PS model" target="MGSM-JA Score">
  <data key="d5">9.0</data>
  <data key="d6">The PS model achieved a higher MGSM-JA score, indicating its effectiveness compared to other models.</data>
  <data key="d7">performance evaluation, model comparison</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="shisa-gamma-7b-v1" target="HuggingFace">
  <data key="d5">8.0</data>
  <data key="d6">Shisa-gamma-7b-v1 is available on HuggingFace, which hosts various models for public use.</data>
  <data key="d7">model hosting, platform availability</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="shisa-gamma-7b-v1" target="Mistral-7B-v0.1">
  <data key="d5">16.0</data>
  <data key="d6">Both Mistral-7B-v0.1 and shisa-gamma-7b-v1 are categorized as 7B source models, showcasing performance metrics.".</data>
  <data key="d7">model comparison, performance metrics</data>
  <data key="d8">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="shisa-gamma-7b-v1" target="7B source models">
  <data key="d5">10.0</data>
  <data key="d6">shisa-gamma-7b-v1 belongs to the 7B source models category, showcasing similar performance metrics.".</data>
  <data key="d7">category classification, model type</data>
  <data key="d8">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="WizardMath-7B-V1.1" target="EvoLLM-JP">
  <data key="d5">9.0</data>
  <data key="d6">EvoLLM-JP is designed to outperform WizardMath-7B-V1.1 in mathematical reasoning, especially in a Japanese context.</data>
  <data key="d7">model comparison, performance</data>
  <data key="d8">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="WizardMath-7B-V1.1" target="Mistral-7B-v0.1">
  <data key="d5">16.0</data>
  <data key="d6">Mistral-7B-v0.1 and WizardMath-7B-V1.1 are both 7B source models that report performance metrics.".</data>
  <data key="d7">model comparison, performance metrics</data>
  <data key="d8">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Abel-7B-002" target="Mistral-7B-v0.1">
  <data key="d5">16.0</data>
  <data key="d6">Mistral-7B-v0.1 and Abel-7B-002 are part of the 7B source models, indicating similar performance measurement.".</data>
  <data key="d7">model comparison, performance metrics</data>
  <data key="d8">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="HuggingFace" target="EvoLLM-JP">
  <data key="d5">7.0</data>
  <data key="d6">Models used in EvoLLM-JP were found on HuggingFace, indicating collaboration and resource sharing.</data>
  <data key="d7">model sourcing, collaboration</data>
  <data key="d8">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Epochs" target="Learning Rates">
  <data key="d5">8.0</data>
  <data key="d6">Learning rates were tested over a set number of epochs to determine optimal performance during fine-tuning.</data>
  <data key="d7">training parameters, optimization</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Distraction Effect" target="Irrelevant Models">
  <data key="d5">7.0</data>
  <data key="d6">The distraction effect was observed when irrelevant models were included in the selection process, impacting performance.</data>
  <data key="d7">model selection, performance impact</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Distraction Effect" target="German Model">
  <data key="d5">6.0</data>
  <data key="d6">The German model is an example of an irrelevant model that contributed to the distraction effect in the experiments.</data>
  <data key="d7">irrelevant model, experimental impact</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Distraction Effect" target="Spanish Model">
  <data key="d5">6.0</data>
  <data key="d6">The Spanish model is included as an irrelevant model affecting the distraction effect in the experiments.</data>
  <data key="d7">irrelevant model, experimental impact</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Distraction Effect" target="Chinese Model">
  <data key="d5">6.0</data>
  <data key="d6">The Chinese model is classified as an irrelevant model contributing to the distraction effect in the experiments.</data>
  <data key="d7">irrelevant model, experimental impact</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Distraction Effect" target="Biomedical Model">
  <data key="d5">6.0</data>
  <data key="d6">The biomedical model is an irrelevant model included in the distraction experiments, influencing performance outcomes.</data>
  <data key="d7">irrelevant model, experimental impact</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Distraction Effect" target="French Model">
  <data key="d5">6.0</data>
  <data key="d6">The French model is included as an irrelevant model affecting the distraction effect observed in the experiments.</data>
  <data key="d7">irrelevant model, experimental impact</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Distraction Effect" target="Korean Model">
  <data key="d5">6.0</data>
  <data key="d6">The Korean model is an irrelevant model that contributed to the distraction effect in the experiments.</data>
  <data key="d7">irrelevant model, experimental impact</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Distraction Effect" target="Arabic Model">
  <data key="d5">6.0</data>
  <data key="d6">The Arabic model is classified as an irrelevant model impacting the distraction effect in the experiments.</data>
  <data key="d7">irrelevant model, experimental impact</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Distraction Effect" target="Italian Model">
  <data key="d5">6.0</data>
  <data key="d6">The Italian model is included as an irrelevant model affecting the distraction effect observed in the experiments.</data>
  <data key="d7">irrelevant model, experimental impact</data>
  <data key="d8">chunk-ef78ab420a549ea9f241c278218c7a0a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Llama-2-13b" target="Mistral-7B-v0.1">
  <data key="d5">18.0</data>
  <data key="d6">Mistral-7B-v0.1 outperforms Llama-2-13b in basic mathematical abilities, showcasing a comparative analysis between the two models.</data>
  <data key="d7">model comparison, performance analysis</data>
  <data key="d8">chunk-77300cedbd95cdfcbafc5dec2d0ec643</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Llama-2-13b" target="LiveBench">
  <data key="d5">8.0</data>
  <data key="d6">LiveBench evaluates the OOD robustness of the Llama-2-13b model through various tasks and benchmarks.</data>
  <data key="d7">evaluation, model testing</data>
  <data key="d8">chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="ELYZA-japanese-Llama-2-13b-instruct" target="Llama-2-13b-hf">
  <data key="d5">16.0</data>
  <data key="d6">Both Llama-2-13b-hf and ELYZA-japanese-Llama-2-13b-instruct are 13B models with reported performance metrics.".</data>
  <data key="d7">model comparison, performance metrics</data>
  <data key="d8">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MetaMath-13B-V1.0" target="DFS-Merged Model">
  <data key="d5">9.0</data>
  <data key="d6">The DFS-Merged Model utilizes configurations from MetaMath-13B-V1.0, particularly analyzing the effects of skipping certain layers for performance improvement.</data>
  <data key="d7">model analysis, performance improvement</data>
  <data key="d8">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MetaMath-13B-V1.0" target="Layer #30">
  <data key="d5">10.0</data>
  <data key="d6">Layer #30 from MetaMath-13B-V1.0 was identified as redundant, leading to its removal in the DFS-Merged Model for better outcomes.</data>
  <data key="d7">layer optimization, model efficiency</data>
  <data key="d8">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="MetaMath-13B-V1.0" target="Llama-2-13b-hf">
  <data key="d5">16.0</data>
  <data key="d6">Llama-2-13b-hf and MetaMath-13B-V1.0 are both categorized as 13B source models, showcasing their performance metrics.".</data>
  <data key="d7">model comparison, performance metrics</data>
  <data key="d8">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Mistral-7B-v0.1" target="7B source models">
  <data key="d5">10.0</data>
  <data key="d6">Mistral-7B-v0.1 is classified under the 7B source models category, indicating its performance metrics.".</data>
  <data key="d7">category classification, model type</data>
  <data key="d8">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="DFS-Merged Model" target="Model A">
  <data key="d5">8.0</data>
  <data key="d6">The DFS-Merged Model incorporates layers from Model A, leveraging its initial performance characteristics during the DFS process.</data>
  <data key="d7">model integration, performance enhancement</data>
  <data key="d8">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="DFS-Merged Model" target="Model B">
  <data key="d5">7.0</data>
  <data key="d6">The performance of the DFS-Merged Model is influenced by layers from Model B, which may have undergone extensive fine-tuning.</data>
  <data key="d7">model integration, fine-tuning impact</data>
  <data key="d8">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="DFS-Merged Model" target="Japanese General Model">
  <data key="d5">9.0</data>
  <data key="d6">The DFS-Merged Model incorporates layers from the Japanese General Model to enhance its capabilities in processing Japanese language tasks.</data>
  <data key="d7">language processing, model enhancement</data>
  <data key="d8">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="DFS-Merged Model" target="Task Scenarios">
  <data key="d5">8.0</data>
  <data key="d6">The Task Scenarios illustrate specific instances where the DFS-Merged Model outperformed its source models in reasoning tasks.</data>
  <data key="d7">performance comparison, model evaluation</data>
  <data key="d8">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model A" target="Model #5">
  <data key="d5">8.0</data>
  <data key="d6">Model #5 incorporates layers from Model A, allowing it to utilize the strengths of this source model in its performance evaluations.</data>
  <data key="d7">model integration, performance enhancement</data>
  <data key="d8">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model B" target="Model #5">
  <data key="d5">7.0</data>
  <data key="d6">The performance of Model #5 is influenced by the layers from Model B, which may have undergone significant changes affecting output quality.</data>
  <data key="d7">model integration, performance evaluation</data>
  <data key="d8">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model #5" target="Scaling Matrix W">
  <data key="d5">8.0</data>
  <data key="d6">The Scaling Matrix W is utilized in Model #5 to control the outputs of layers, impacting the overall performance of the model.</data>
  <data key="d7">layer management, performance optimization</data>
  <data key="d8">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model #5" target="Performance Analysis">
  <data key="d5">9.0</data>
  <data key="d6">Performance Analysis is conducted on Model #5 to evaluate its effectiveness in various tasks and scenarios.</data>
  <data key="d7">model evaluation, performance assessment</data>
  <data key="d8">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model #5" target="Layer Removal">
  <data key="d5">10.0</data>
  <data key="d6">The process of Layer Removal is applied to Model #5 to improve its performance based on findings from performance analysis.</data>
  <data key="d7">model optimization, performance enhancement</data>
  <data key="d8">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Model #6" target="Model #7">
  <data key="d5">8.0</data>
  <data key="d6">Model #6 and Model #7 are compared in performance analysis to understand the impact of different configurations on outcomes.</data>
  <data key="d7">model comparison, performance evaluation</data>
  <data key="d8">chunk-10ec50dbb94517b979f409f78b996890</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="" target="">
  <data key="d5">16.0</data>
  <data key="d6">8DVD</data>
  <data key="d7"></data>
  <data key="d8">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="" target="DVD">
  <data key="d5">18.0</data>
  <data key="d6">DVD</data>
  <data key="d7"></data>
  <data key="d8">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="" target="">
  <data key="d5">14.0</data>
  <data key="d6">DVD</data>
  <data key="d7"></data>
  <data key="d8">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="" target="">
  <data key="d5">18.0</data>
  <data key="d6"></data>
  <data key="d7"></data>
  <data key="d8">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="" target="">
  <data key="d5">7.0</data>
  <data key="d6"></data>
  <data key="d7"></data>
  <data key="d8">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="" target="">
  <data key="d5">16.0</data>
  <data key="d6"></data>
  <data key="d7"></data>
  <data key="d8">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="" target="">
  <data key="d5">9.0</data>
  <data key="d6"></data>
  <data key="d7"></data>
  <data key="d8">chunk-d99d6d927354837b15faabd9cc428437</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Grandma Jones" target="Pies">
  <data key="d5">16.0</data>
  <data key="d6">Grandma Jones cut the pies into pieces for the guests to enjoy.</data>
  <data key="d7">food preparation, event hosting</data>
  <data key="d8">chunk-3ca398087b4d5dd48a0c1fe169e9b7fd</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Guests" target="Pieces of Pie">
  <data key="d5">18.0</data>
  <data key="d6">Guests consumed the pieces of pie that were prepared by Grandma Jones.</data>
  <data key="d7">food consumption, event participation</data>
  <data key="d8">chunk-3ca398087b4d5dd48a0c1fe169e9b7fd</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Guests" target="Event of Pie Serving">
  <data key="d5">8.0</data>
  <data key="d6">The event of pie serving involves guests who partake in eating the pies prepared by Grandma Jones.</data>
  <data key="d7">event participation, social gathering</data>
  <data key="d8">chunk-3ca398087b4d5dd48a0c1fe169e9b7fd</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Total Pieces" target="Eaten Pieces">
  <data key="d5">10.0</data>
  <data key="d6">Total pieces minus remaining pieces equals the number of eaten pieces, illustrating consumption during the event.</data>
  <data key="d7">calculation, food consumption</data>
  <data key="d8">chunk-3ca398087b4d5dd48a0c1fe169e9b7fd</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Total Pieces" target="Remaining Pieces">
  <data key="d5">9.0</data>
  <data key="d6">Total pieces and remaining pieces are related through subtraction to determine how many were eaten.</data>
  <data key="d7">mathematical relationship, event outcome</data>
  <data key="d8">chunk-3ca398087b4d5dd48a0c1fe169e9b7fd</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="GPT-4-V" target="LLaVA-1.6-Mistral-7B">
  <data key="d5">16.0</data>
  <data key="d6">GPT-4-V is utilized in conjunction with LLaVA-1.6-Mistral-7B to generate captions and improve model outputs.</data>
  <data key="d7">model collaboration, caption generation</data>
  <data key="d8">chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="LLaVA-1.6-Mistral-7B" target="Japanese Stable VLM">
  <data key="d5">18.0</data>
  <data key="d6">LLaVA-1.6-Mistral-7B serves as a source model for the Japanese Stable VLM, influencing its training process.</data>
  <data key="d7">model training, source influence</data>
  <data key="d8">chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="LLaVA-1.6-Mistral-7B" target="Fasttext">
  <data key="d5">10.0</data>
  <data key="d6">Fasttext is used for language detection in the context of LLaVA-1.6-Mistral-7B experiments.</data>
  <data key="d7">language detection, experimental methodology</data>
  <data key="d8">chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="LLaVA-1.6-Mistral-7B" target="Japanese Culture">
  <data key="d5">8.0</data>
  <data key="d6">LLaVA-1.6-Mistral-7B's performance reflects its understanding of Japanese cultural nuances.</data>
  <data key="d7">cultural understanding, language model performance</data>
  <data key="d8">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese Stable VLM" target="JA-VG-VQA-500">
  <data key="d5">12.0</data>
  <data key="d6">The Japanese Stable VLM cannot be evaluated on the JA-VG-VQA-500 dataset because it was trained on this dataset.</data>
  <data key="d7">evaluation limitation, dataset training</data>
  <data key="d8">chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese Stable VLM" target="JA-VLM-Bench-In-the-Wild">
  <data key="d5">20.0</data>
  <data key="d6">The performance of the Japanese Stable VLM on the JA-VLM-Bench-In-the-Wild demonstrates its ability to handle culturally-specific content.</data>
  <data key="d7">cultural context, model performance</data>
  <data key="d8">chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese Stable VLM" target="Culturally-Specific Content">
  <data key="d5">9.0</data>
  <data key="d6">The Japanese Stable VLM is designed to handle culturally-specific content, enhancing its relevance in Japanese contexts.</data>
  <data key="d7">cultural relevance, model design</data>
  <data key="d8">chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese Stable VLM" target="LM Benchmark">
  <data key="d5">7.0</data>
  <data key="d6">Japanese Stable VLM is related to the LM Benchmark initiative, focusing on multilingual models and benchmarks.</data>
  <data key="d7">research initiative, multilingual models</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Techniques" target="Model Performance Comparison">
  <data key="d5">8.0</data>
  <data key="d6">Evolutionary techniques are applied to discover optimal ways to combine models, which impacts the results of model performance comparison.</data>
  <data key="d7">model optimization, performance evaluation</data>
  <data key="d8">chunk-77432ea4d82a2648008d6d26c50ccdc9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Model Merging" target="EvoSDXL">
  <data key="d5">18.0</data>
  <data key="d6">EvoSDXL demonstrates the effectiveness of evolutionary model merging in the domain of image generation.</data>
  <data key="d7">application, model integration</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Model Merging" target="EvoVLM-JP-v2">
  <data key="d5">14.0</data>
  <data key="d6">EvoVLM-JP-v2 showcases the adaptability of evolutionary model merging in developing advanced models.</data>
  <data key="d7">model development, adaptability</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Model Merging" target="EvoUkiyoe">
  <data key="d5">14.0</data>
  <data key="d6">EvoUkiyoe is another example of a model developed through evolutionary model merging, illustrating its potential across different applications.</data>
  <data key="d7">model development, adaptability</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Model Merging" target="MergeKit">
  <data key="d5">16.0</data>
  <data key="d6">MergeKit implements evolutionary model merging, making the method widely available for practical use.</data>
  <data key="d7">software application, accessibility</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Model Merging" target="Optuna Hub">
  <data key="d5">16.0</data>
  <data key="d6">Optuna Hub incorporates evolutionary model merging, facilitating its application in various AI projects.</data>
  <data key="d7">software application, accessibility</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Model Merging" target="SLERP">
  <data key="d5">8.0</data>
  <data key="d6">SLERP is a technique utilized in evolutionary model merging to create effective model combinations.</data>
  <data key="d7">technique, model integration</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Model Merging" target="Foundation Models">
  <data key="d5">9.0</data>
  <data key="d6">Foundation Models are improved through evolutionary model merging, allowing for the development of more capable AI systems.</data>
  <data key="d7">model enhancement, AI development</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Model Merging" target="Image Diffusion Models">
  <data key="d5">9.0</data>
  <data key="d6">Image Diffusion Models benefit from evolutionary model merging techniques, enhancing their generative capabilities.</data>
  <data key="d7">model enhancement, generative AI</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Model Merging" target="Benchmark Tasks">
  <data key="d5">7.0</data>
  <data key="d6">Benchmark Tasks are utilized to assess the effectiveness of models developed through evolutionary model merging.</data>
  <data key="d7">evaluation, performance metrics</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="EvoSDXL" target="SDXL-Lightning">
  <data key="d5">20.0</data>
  <data key="d6">EvoSDXL successfully merges SDXL-Lightning with standard SDXL fine-tunes to enhance image generation capabilities.</data>
  <data key="d7">model integration, performance improvement</data>
  <data key="d8">chunk-e1d5d1b984a0641dcc85ca2f2eb00b6b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="LLM Experiments" target="Hugging Face">
  <data key="d5">7.0</data>
  <data key="d6">Datasets for LLM Experiments are available on Hugging Face, indicating a collaboration in AI research.</data>
  <data key="d7">dataset access, research collaboration</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="LLM Experiments" target="GitHub">
  <data key="d5">7.0</data>
  <data key="d6">GitHub hosts datasets used in LLM Experiments, facilitating open-source contributions to AI research.</data>
  <data key="d7">dataset access, open-source collaboration</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Hugging Face" target="Datasets">
  <data key="d5">8.0</data>
  <data key="d6">Hugging Face provides datasets that are essential for the LLM experiments conducted in the research.</data>
  <data key="d7">dataset provision, research support</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="GitHub" target="Datasets">
  <data key="d5">8.0</data>
  <data key="d6">GitHub hosts various datasets used for LLM experiments, facilitating open-source collaboration in AI research.</data>
  <data key="d7">open-source collaboration, dataset access</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="GitHub" target="EvoLLM-JP-A">
  <data key="d5">20.0</data>
  <data key="d6">EvoLLM-JP-A will be released on GitHub, making it accessible to the open-source community.</data>
  <data key="d7">public release, accessibility</data>
  <data key="d8">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese Stable LM Beta" target="Stability AI">
  <data key="d5">9.0</data>
  <data key="d6">Stability AI developed the Japanese Stable LM Beta, showcasing their efforts in advancing language models.</data>
  <data key="d7">model development, organizational effort</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Evolutionary Optimization of Model Merging Recipes" target="OpenAI">
  <data key="d5">8.0</data>
  <data key="d6">OpenAI's research and developments contribute to the methodologies explored in the Evolutionary Optimization of Model Merging Recipes project.</data>
  <data key="d7">research contribution, methodology development</data>
  <data key="d8">chunk-3d9244715091995dd6afa5bd49e8dc52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Stability AI" target="JP Language Model Evaluation Harness">
  <data key="d5">9.0</data>
  <data key="d6">Stability AI developed the JP Language Model Evaluation Harness to evaluate the performance of Japanese language models.</data>
  <data key="d7">AI development, language evaluation</data>
  <data key="d8">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Emanuele Aiello" target="Qwen-VL">
  <data key="d5">18.0</data>
  <data key="d6">Emanuele Aiello is involved in the research and development of Qwen-VL, a vision-language model.&lt;SEP&gt;Emanuele Aiello is involved in the research and development of Qwen-VL, contributing to its capabilities in multimodal understanding.</data>
  <data key="d7">research contribution, model development</data>
  <data key="d8">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="shisa-gamma-7b" target="Stable Diffusion WebUI">
  <data key="d5">14.0</data>
  <data key="d6">Shisa-gamma-7b can be integrated with the Stable Diffusion WebUI for generating outputs based on user inputs.&lt;SEP&gt;Shisa-gamma-7b can be utilized within the Stable Diffusion WebUI for generating content based on user inputs.</data>
  <data key="d7">model application, user interface</data>
  <data key="d8">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Qwen-VL" target="Lili Yu">
  <data key="d5">8.0</data>
  <data key="d6">Lili Yu contributes to the development of Qwen-VL, enhancing its performance in vision-language tasks.</data>
  <data key="d7">collaboration, AI research</data>
  <data key="d8">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Qwen-VL" target="Yixin Nie">
  <data key="d5">8.0</data>
  <data key="d6">Yixin Nie's research efforts are directed towards improving the functionalities of Qwen-VL in multimodal applications.</data>
  <data key="d7">research focus, model enhancement</data>
  <data key="d8">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Qwen-VL" target="Armen Aghajanyan">
  <data key="d5">8.0</data>
  <data key="d6">Armen Aghajanyan collaborates on the development of Qwen-VL, focusing on its training methodologies.</data>
  <data key="d7">collaboration, AI model training</data>
  <data key="d8">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Qwen-VL" target="Barlas Oguz">
  <data key="d5">8.0</data>
  <data key="d6">Barlas Oguz is involved in the research for Qwen-VL, contributing to its overall development and performance improvements.</data>
  <data key="d7">research involvement, model development</data>
  <data key="d8">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Generative AI for Math: Abel" target="InstructBLIP">
  <data key="d5">12.0</data>
  <data key="d6">Generative AI for Math: Abel and InstructBLIP are both projects that leverage generative AI technologies for different applications, showcasing the versatility of AI.&lt;SEP&gt;Generative AI for Math: Abel and InstructBLIP are both projects that utilize generative AI technologies for different applications.</data>
  <data key="d7">AI applications, project comparison</data>
  <data key="d8">chunk-605e26052d578b60e0e6849cb0d635d9</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Daniel M Roy" target="arXiv preprint">
  <data key="d5">16.0</data>
  <data key="d6">Daniel M Roy's work on generalization bounds for neural networks is published as an arXiv preprint, contributing to the field of AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Adam Gaier" target="arXiv preprint">
  <data key="d5">16.0</data>
  <data key="d6">Adam Gaier's research on weight agnostic neural networks appears as an arXiv preprint, showcasing advancements in neural processing systems.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Leo Gao" target="arXiv preprint">
  <data key="d5">16.0</data>
  <data key="d6">Leo Gao's framework for few-shot language model evaluation is shared as an arXiv preprint, reflecting ongoing research in language models.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Mistral 7B" target="arXiv preprint">
  <data key="d5">7.0</data>
  <data key="d6">Mistral 7B is discussed in the context of a research paper, indicating its relevance in the study of language models.</data>
  <data key="d7">research relevance, language model</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Baber Abbasi">
  <data key="d5">8.0</data>
  <data key="d6">Baber Abbasi's contributions to few-shot language model evaluation are published as an arXiv preprint, indicating involvement in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Stella Biderman">
  <data key="d5">8.0</data>
  <data key="d6">Stella Biderman's work on few-shot language model evaluation is documented in an arXiv preprint, showcasing advancements in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Sid Black">
  <data key="d5">8.0</data>
  <data key="d6">Sid Black's research contributions to few-shot language model evaluation are published as an arXiv preprint, reflecting his involvement in AI advancements.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Anthony DiPofi">
  <data key="d5">8.0</data>
  <data key="d6">Anthony DiPofi's work on few-shot language model evaluation is shared as an arXiv preprint, contributing to the field of AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Charles Foster">
  <data key="d5">8.0</data>
  <data key="d6">Charles Foster's contributions to few-shot language model evaluation are documented in an arXiv preprint, indicating his role in AI advancements.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Laurence Golding">
  <data key="d5">8.0</data>
  <data key="d6">Laurence Golding's research on few-shot language model evaluation is published as an arXiv preprint, showcasing his involvement in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Jeffrey Hsu">
  <data key="d5">8.0</data>
  <data key="d6">Jeffrey Hsu's work on few-shot language model evaluation is documented in an arXiv preprint, contributing to advancements in AI.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Alain Le Noac'h">
  <data key="d5">8.0</data>
  <data key="d6">Alain Le Noac'h's contributions to few-shot language model evaluation are published as an arXiv preprint, indicating his involvement in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Haonan Li">
  <data key="d5">8.0</data>
  <data key="d6">Haonan Li's work on few-shot language model evaluation is shared as an arXiv preprint, showcasing advancements in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Kyle McDonell">
  <data key="d5">8.0</data>
  <data key="d6">Kyle McDonell's contributions to few-shot language model evaluation are documented in an arXiv preprint, reflecting his role in AI advancements.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Niklas Muennighoff">
  <data key="d5">8.0</data>
  <data key="d6">Niklas Muennighoff's work on few-shot language model evaluation is published as an arXiv preprint, indicating his involvement in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Chris Ociepa">
  <data key="d5">8.0</data>
  <data key="d6">Chris Ociepa's contributions to few-shot language model evaluation are documented in an arXiv preprint, showcasing his involvement in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Jason Phang">
  <data key="d5">8.0</data>
  <data key="d6">Jason Phang's work on few-shot language model evaluation is shared as an arXiv preprint, contributing to advancements in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Laria Reynolds">
  <data key="d5">8.0</data>
  <data key="d6">Laria Reynolds' contributions to few-shot language model evaluation are published as an arXiv preprint, indicating her involvement in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Hailey Schoelkopf">
  <data key="d5">8.0</data>
  <data key="d6">Hailey Schoelkopf's work on few-shot language model evaluation is documented in an arXiv preprint, showcasing her role in AI advancements.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Aviya Skowron">
  <data key="d5">8.0</data>
  <data key="d6">Aviya Skowron's contributions to few-shot language model evaluation are published as an arXiv preprint, indicating her involvement in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Lintang Sutawika">
  <data key="d5">8.0</data>
  <data key="d6">Lintang Sutawika's work on few-shot language model evaluation is shared as an arXiv preprint, contributing to advancements in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Eric Tang">
  <data key="d5">8.0</data>
  <data key="d6">Eric Tang's contributions to few-shot language model evaluation are documented in an arXiv preprint, showcasing his involvement in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Anish Thite">
  <data key="d5">8.0</data>
  <data key="d6">Anish Thite's work on few-shot language model evaluation is shared as an arXiv preprint, contributing to advancements in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Ben Wang">
  <data key="d5">8.0</data>
  <data key="d6">Ben Wang's contributions to few-shot language model evaluation are documented in an arXiv preprint, indicating his role in AI advancements.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Kevin Wang">
  <data key="d5">8.0</data>
  <data key="d6">Kevin Wang's work on few-shot language model evaluation is published as an arXiv preprint, showcasing his involvement in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Andy Zou">
  <data key="d5">8.0</data>
  <data key="d6">Andy Zou's contributions to few-shot language model evaluation are documented in an arXiv preprint, indicating his involvement in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Mor Geva">
  <data key="d5">8.0</data>
  <data key="d6">Mor Geva's work on transformer feed-forward layers is published as an arXiv preprint, contributing to AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Avi Caciularu">
  <data key="d5">8.0</data>
  <data key="d6">Avi Caciularu's contributions to transformer models are documented in an arXiv preprint, showcasing advancements in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Kevin Ro Wang">
  <data key="d5">8.0</data>
  <data key="d6">Kevin Ro Wang's work on transformer models is shared as an arXiv preprint, contributing to advancements in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Yoav Goldberg">
  <data key="d5">8.0</data>
  <data key="d6">Yoav Goldberg's contributions to transformer models are documented in an arXiv preprint, showcasing his role in AI advancements.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Gabriel Ilharco">
  <data key="d5">8.0</data>
  <data key="d6">Gabriel Ilharco's work on editing models is published as an arXiv preprint, contributing to advancements in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Marco Tulio Ribeiro">
  <data key="d5">8.0</data>
  <data key="d6">Marco Tulio Ribeiro's contributions to model interpretability are documented in an arXiv preprint, showcasing his role in AI advancements.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Mitchell Wortsman">
  <data key="d5">8.0</data>
  <data key="d6">Mitchell Wortsman's work on model editing is published as an arXiv preprint, contributing to advancements in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Suchin Gururangan">
  <data key="d5">8.0</data>
  <data key="d6">Suchin Gururangan's contributions to AI model editing are documented in an arXiv preprint, showcasing his role in AI advancements.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Ludwig Schmidt">
  <data key="d5">8.0</data>
  <data key="d6">Ludwig Schmidt's work on model editing is shared as an arXiv preprint, contributing to advancements in AI research.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Hannaneh Hajishirzi">
  <data key="d5">8.0</data>
  <data key="d6">Hannaneh Hajishirzi's contributions to AI research are documented in an arXiv preprint, showcasing her role in model interpretability.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv preprint" target="Ali Farhadi">
  <data key="d5">8.0</data>
  <data key="d6">Ali Farhadi's work on AI models is published as an arXiv preprint, contributing to advancements in the field.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-6dacb14670f826534e89e51fc52ea26f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Gabriel Ilharco" target="Marco Tulio Ribeiro">
  <data key="d5">8.0</data>
  <data key="d6">Gabriel Ilharco and Marco Tulio Ribeiro have collaborated on editing models and adversarial techniques in machine learning.</data>
  <data key="d7">collaboration, adversarial techniques</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Gabriel Ilharco" target="Model Soups">
  <data key="d5">8.0</data>
  <data key="d6">Gabriel Ilharco collaborates with Mitchell Wortsman in the research on model soups, contributing to the findings.</data>
  <data key="d7">research collaboration, model improvement</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Mitchell Wortsman" target="Tom White">
  <data key="d5">8.0</data>
  <data key="d6">Tom White and Mitchell Wortsman are involved in research on generative networks, indicating their contributions to AI advancements.</data>
  <data key="d7">collaboration, research</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Mitchell Wortsman" target="Model Soups">
  <data key="d5">18.0</data>
  <data key="d6">Mitchell Wortsman is a lead author discussing the concept of model soups which average weights of multiple models to improve accuracy.&lt;SEP&gt;Mitchell Wortsman is an author of Model Soups, indicating his involvement in the research.</data>
  <data key="d7">authorship, research collaboration&lt;SEP&gt;research contribution, model improvement</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt&lt;SEP&gt;Mixup_model_merging.txt</data>
</edge>
<edge source="Mitchell Wortsman" target="Suchin Gururangan">
  <data key="d5">8.0</data>
  <data key="d6">Mitchell Wortsman and Suchin Gururangan are both involved in research focused on adversarial techniques in natural language processing.</data>
  <data key="d7">research collaboration, adversarial learning</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Ludwig Schmidt" target="Hannaneh Hajishirzi">
  <data key="d5">9.0</data>
  <data key="d6">Ludwig Schmidt and Hannaneh Hajishirzi are prominent researchers collaborating on adversarial robustness in machine learning models.</data>
  <data key="d7">collaboration, research focus</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Ali Farhadi" target="Wenxiang Jiao">
  <data key="d5">7.0</data>
  <data key="d6">Ali Farhadi and Wenxiang Jiao are both involved in machine learning evaluation, particularly in translation tasks.</data>
  <data key="d7">research collaboration, evaluation focus</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Ali Farhadi" target="Model Soups">
  <data key="d5">8.0</data>
  <data key="d6">Ali Farhadi collaborates on the research regarding model soups and their effectiveness in machine learning.</data>
  <data key="d7">research collaboration, model improvement</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Neural Information Processing Systems" target="Nitish Shirish Keskar">
  <data key="d5">16.0</data>
  <data key="d6">Nitish Shirish Keskar contributed to the Neural Information Processing Systems conference through his research on deep learning.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Neural Information Processing Systems" target="Dheevatsa Mudigere">
  <data key="d5">16.0</data>
  <data key="d6">Dheevatsa Mudigere's work on large-batch training was presented at the Neural Information Processing Systems conference.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Neural Information Processing Systems" target="Jorge Nocedal">
  <data key="d5">14.0</data>
  <data key="d6">Jorge Nocedal's research on optimization techniques is relevant to the discussions held at the Neural Information Processing Systems conference.</data>
  <data key="d7">research contribution, conference relevance</data>
  <data key="d8">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Neural Information Processing Systems" target="Mikhail Smelyanskiy">
  <data key="d5">16.0</data>
  <data key="d6">Mikhail Smelyanskiy's contributions in AI are showcased at the Neural Information Processing Systems conference.</data>
  <data key="d7">research contribution, conference relevance</data>
  <data key="d8">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Neural Information Processing Systems" target="Ping Tak Peter Tang">
  <data key="d5">16.0</data>
  <data key="d6">Ping Tak Peter Tang's work in deep learning is featured in the Neural Information Processing Systems conference proceedings.</data>
  <data key="d7">research contribution, conference relevance</data>
  <data key="d8">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Neural Information Processing Systems" target="Large-Batch Training">
  <data key="d5">8.0</data>
  <data key="d6">Research on large-batch training is often discussed at the Neural Information Processing Systems conference, indicating its significance in the field.</data>
  <data key="d7">research focus, conference theme</data>
  <data key="d8">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Maxime Labonne" target="International Conference on Learning Representations">
  <data key="d5">18.0</data>
  <data key="d6">Maxime Labonne presented his work at the International Conference on Learning Representations, highlighting advancements in model merging.</data>
  <data key="d7">research presentation, conference participation</data>
  <data key="d8">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Maxime Labonne" target="Hugging Face Blog">
  <data key="d5">16.0</data>
  <data key="d6">Maxime Labonne shared insights on model merging via the Hugging Face Blog, contributing to community knowledge.</data>
  <data key="d7">knowledge sharing, community contribution</data>
  <data key="d8">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Maxime Labonne" target="arXiv">
  <data key="d5">18.0</data>
  <data key="d6">Maxime Labonne's research papers are available on arXiv, contributing to the dissemination of knowledge in AI.</data>
  <data key="d7">research dissemination, open access</data>
  <data key="d8">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="arXiv" target="CoRR">
  <data key="d5">14.0</data>
  <data key="d6">CoRR and arXiv are both repositories that facilitate the sharing of computer science research papers.</data>
  <data key="d7">repository comparison, research sharing</data>
  <data key="d8">chunk-979d654e8dd7d60ff9ad975c80ca86bb</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Henning Petzka" target="Advances in Neural Information Processing Systems">
  <data key="d5">8.0</data>
  <data key="d6">Henning Petzka is an author of a paper presented at the Advances in Neural Information Processing Systems conference.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Michael Kamp" target="Advances in Neural Information Processing Systems">
  <data key="d5">8.0</data>
  <data key="d6">Michael Kamp is a co-author of a paper presented at the Advances in Neural Information Processing Systems conference.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Advances in Neural Information Processing Systems" target="Colin Raffel">
  <data key="d5">7.0</data>
  <data key="d6">Colin Raffel's work on machine learning models relates to discussions at the Advances in Neural Information Processing Systems conference.</data>
  <data key="d7">thematic relevance, research influence</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Advances in Neural Information Processing Systems" target="Proceedings of the AAAI Conference on Artificial Intelligence">
  <data key="d5">9.0</data>
  <data key="d6">Both are significant publications in the field of artificial intelligence and machine learning research.</data>
  <data key="d7">academic publications, AI research</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Advances in Neural Information Processing Systems" target="Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition">
  <data key="d5">9.0</data>
  <data key="d6">Both are significant publications in the field of computer vision and machine learning research.</data>
  <data key="d7">academic publications, computer vision research</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Advances in Neural Information Processing Systems" target="International Conference on Computational Linguistics">
  <data key="d5">9.0</data>
  <data key="d6">Both are important conferences for presenting research in computational linguistics and AI.</data>
  <data key="d7">academic conferences, AI research</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Colin Raffel" target="LM Benchmark">
  <data key="d5">6.0</data>
  <data key="d6">Colin Raffel's advocacy for open-source methodologies aligns with the goals of the LM Benchmark initiative.</data>
  <data key="d7">methodological alignment, research initiative</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Kigali" target="The Eleventh International Conference on Learning Representations">
  <data key="d5">9.0</data>
  <data key="d6">The Eleventh International Conference on Learning Representations is hosted in Kigali, Rwanda.</data>
  <data key="d7">event location, international conference</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="The Eleventh International Conference on Learning Representations" target="Freda Shi">
  <data key="d5">8.0</data>
  <data key="d6">Freda Shi is a co-author of a paper presented at The Eleventh International Conference on Learning Representations.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="The Eleventh International Conference on Learning Representations" target="Mirac Suzgun">
  <data key="d5">8.0</data>
  <data key="d6">Mirac Suzgun is a co-author of a paper presented at The Eleventh International Conference on Learning Representations.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="The Eleventh International Conference on Learning Representations" target="Markus Freitag">
  <data key="d5">8.0</data>
  <data key="d6">Markus Freitag is a co-author of a paper presented at The Eleventh International Conference on Learning Representations.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="The Eleventh International Conference on Learning Representations" target="Xuezhi Wang">
  <data key="d5">8.0</data>
  <data key="d6">Xuezhi Wang is a co-author of a paper presented at The Eleventh International Conference on Learning Representations.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="The Eleventh International Conference on Learning Representations" target="Suraj Srivats">
  <data key="d5">8.0</data>
  <data key="d6">Suraj Srivats is a co-author of a paper presented at The Eleventh International Conference on Learning Representations.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="The Eleventh International Conference on Learning Representations" target="Soroush Vosoughi">
  <data key="d5">8.0</data>
  <data key="d6">Soroush Vosoughi is a co-author of a paper presented at The Eleventh International Conference on Learning Representations.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="The Eleventh International Conference on Learning Representations" target="Hyung Won Chung">
  <data key="d5">8.0</data>
  <data key="d6">Hyung Won Chung is a co-author of a paper presented at The Eleventh International Conference on Learning Representations.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="The Eleventh International Conference on Learning Representations" target="Yi Tay">
  <data key="d5">8.0</data>
  <data key="d6">Yi Tay is a co-author of a paper presented at The Eleventh International Conference on Learning Representations.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="The Eleventh International Conference on Learning Representations" target="Sebastian Ruder">
  <data key="d5">8.0</data>
  <data key="d6">Sebastian Ruder is a co-author of a paper presented at The Eleventh International Conference on Learning Representations.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="The Eleventh International Conference on Learning Representations" target="Denny Zhou">
  <data key="d5">8.0</data>
  <data key="d6">Denny Zhou is a co-author of a paper presented at The Eleventh International Conference on Learning Representations.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="The Eleventh International Conference on Learning Representations" target="Dipanjan Das">
  <data key="d5">8.0</data>
  <data key="d6">Dipanjan Das is a co-author of a paper presented at The Eleventh International Conference on Learning Representations.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="The Eleventh International Conference on Learning Representations" target="Jason Wei">
  <data key="d5">8.0</data>
  <data key="d6">Jason Wei is a co-author of a paper presented at The Eleventh International Conference on Learning Representations.</data>
  <data key="d7">research contribution, conference participation</data>
  <data key="d8">chunk-93a35580125b7b2777b31e49cd86578a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Kenneth O Stanley" target="Risto Miikkulainen">
  <data key="d5">8.0</data>
  <data key="d6">Kenneth O Stanley and Risto Miikkulainen co-authored a paper on evolving neural networks, indicating collaboration in AI research.</data>
  <data key="d7">collaboration, research</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Kenneth O Stanley" target="Evolving Neural Networks Through Augmenting Topologies">
  <data key="d5">9.0</data>
  <data key="d6">Kenneth O Stanley is a co-author of the paper Evolving Neural Networks Through Augmenting Topologies, indicating his contribution to the research.</data>
  <data key="d7">authorship, research collaboration</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Risto Miikkulainen" target="Evolving Neural Networks Through Augmenting Topologies">
  <data key="d5">9.0</data>
  <data key="d6">Risto Miikkulainen is a co-author of the paper Evolving Neural Networks Through Augmenting Topologies, indicating his contribution to the research.</data>
  <data key="d7">authorship, research collaboration</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Marc Pickett" target="Transformer Layers as Painters">
  <data key="d5">9.0</data>
  <data key="d6">Marc Pickett is a co-author of the paper Transformer Layers as Painters, indicating his contribution to the research.</data>
  <data key="d7">authorship, research collaboration</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Yi-Lin Sung" target="Linjie Li">
  <data key="d5">8.0</data>
  <data key="d6">Yi-Lin Sung and Linjie Li collaborated on a study of multimodal model merging, showcasing their joint research efforts in AI.</data>
  <data key="d7">collaboration, research</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Yi-Lin Sung" target="An Empirical Study of Multimodal Model Merging">
  <data key="d5">9.0</data>
  <data key="d6">Yi-Lin Sung is an author of the paper An Empirical Study of Multimodal Model Merging, indicating his involvement in the research.</data>
  <data key="d7">authorship, research collaboration</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Linjie Li" target="An Empirical Study of Multimodal Model Merging">
  <data key="d5">9.0</data>
  <data key="d6">Linjie Li is a co-author of the paper An Empirical Study of Multimodal Model Merging, indicating his contribution to the research.</data>
  <data key="d7">authorship, research collaboration</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Tom White" target="Sampling Generative Networks">
  <data key="d5">9.0</data>
  <data key="d6">Tom White is the author of Sampling Generative Networks, indicating his contribution to the research.</data>
  <data key="d7">authorship, research collaboration</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Samir Ya Gadre" target="Model Soups">
  <data key="d5">8.0</data>
  <data key="d6">Samir Ya Gadre is a co-author on the research paper discussing model soups, contributing to its insights.</data>
  <data key="d7">research collaboration, model improvement</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Rebecca Roelofs" target="Model Soups">
  <data key="d5">8.0</data>
  <data key="d6">Rebecca Roelofs is involved in the research on model soups, enhancing the understanding of model averaging.</data>
  <data key="d7">research collaboration, model improvement</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Raphael Gontijo-Lopes" target="Model Soups">
  <data key="d5">8.0</data>
  <data key="d6">Raphael Gontijo-Lopes co-authors the study on model soups, focusing on accuracy improvements.</data>
  <data key="d7">research collaboration, model improvement</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Ari S Morcos" target="Model Soups">
  <data key="d5">8.0</data>
  <data key="d6">Ari S Morcos contributes to the research on model soups, emphasizing its significance in machine learning.</data>
  <data key="d7">research collaboration, model improvement</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Hongseok Namkoong" target="Model Soups">
  <data key="d5">8.0</data>
  <data key="d6">Hongseok Namkoong is a co-author discussing the impact of model soups on model performance.</data>
  <data key="d7">research collaboration, model improvement</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Yair Carmon" target="Model Soups">
  <data key="d5">8.0</data>
  <data key="d6">Yair Carmon is a co-author of the paper on model soups, contributing to the research findings.</data>
  <data key="d7">research collaboration, model improvement</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Simon Kornblith" target="Model Soups">
  <data key="d5">8.0</data>
  <data key="d6">Simon Kornblith is involved in the research on model soups, enhancing the understanding of model merging techniques.</data>
  <data key="d7">research collaboration, model improvement</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Prateek Yadav" target="Leshem Choshen">
  <data key="d5">8.0</data>
  <data key="d6">Prateek Yadav and Leshem Choshen are authors who collaborated on compression techniques for AI models, indicating joint research efforts.</data>
  <data key="d7">collaboration, research</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Prateek Yadav" target="Compeft">
  <data key="d5">9.0</data>
  <data key="d6">Prateek Yadav is the author of Compeft, indicating his contribution to the research.</data>
  <data key="d7">authorship, research collaboration</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="International Conference on Machine Learning" target="Advances in Neural Information Processing Systems 36">
  <data key="d5">9.0</data>
  <data key="d6">Both conferences are significant events for presenting advancements in AI and machine learning research.</data>
  <data key="d7">event significance, research</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="International Conference on Machine Learning" target="Model Soups">
  <data key="d5">9.0</data>
  <data key="d6">The research on model soups is presented at the International Conference on Machine Learning, showcasing its relevance.</data>
  <data key="d7">conference presentation, research dissemination</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="International Conference on Machine Learning" target="PMLR">
  <data key="d5">9.0</data>
  <data key="d6">PMLR publishes research presented at the International Conference on Machine Learning, disseminating findings in the field.</data>
  <data key="d7">publication, research dissemination</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Model Soups" target="mixup">
  <data key="d5">7.0</data>
  <data key="d6">Mixup is a technique that may relate to the concepts discussed in model soups, enhancing model performance.</data>
  <data key="d7">technique application, model improvement</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Model Soups" target="Promptrobust">
  <data key="d5">7.0</data>
  <data key="d6">Promptrobust relates to the evaluation of models, which can be influenced by the techniques discussed in model soups.</data>
  <data key="d7">evaluation framework, model improvement</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Model Soups" target="Promptbench">
  <data key="d5">7.0</data>
  <data key="d6">Promptbench provides a framework for evaluating models, relevant to the discussions on model soups.</data>
  <data key="d7">evaluation framework, model improvement</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Yi: Open Foundation Models by 01.ai" target="Alex Young">
  <data key="d5">9.0</data>
  <data key="d6">Alex Young is an author of Yi: Open Foundation Models by 01.ai, indicating his contribution to the research.</data>
  <data key="d7">authorship, research collaboration</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Language Models are Super Mario" target="Le Yu">
  <data key="d5">9.0</data>
  <data key="d6">Le Yu is an author of Language Models are Super Mario, indicating his involvement in the research.</data>
  <data key="d7">authorship, research collaboration</data>
  <data key="d8">chunk-144f32db3d945848c593404dfe014f4f</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Stability AI Japan" target="EvoLLM-JP">
  <data key="d5">16.0</data>
  <data key="d6">Stability AI Japan developed the EvoLLM-JP model, which is based on their evaluation harness.</data>
  <data key="d7">model development, organizational contribution</data>
  <data key="d8">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Stability AI Japan" target="lm-eval-harness">
  <data key="d5">8.0</data>
  <data key="d6">Stability AI Japan developed a fork of the lm-eval-harness for evaluating language models in Japanese.</data>
  <data key="d7">model evaluation, organizational development</data>
  <data key="d8">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="EvoLLM-JP" target="EvoLLM-JP-A">
  <data key="d5">18.0</data>
  <data key="d6">EvoLLM-JP-A is an improved version of EvoLLM-JP, showcasing advancements in language model performance.</data>
  <data key="d7">model improvement, evolution</data>
  <data key="d8">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="EvoLLM-JP" target="Setsubun">
  <data key="d5">14.0</data>
  <data key="d6">EvoLLM-JP demonstrates improved understanding of Japanese culture, as evidenced by its performance on questions about Setsubun.</data>
  <data key="d7">cultural understanding, language capability</data>
  <data key="d8">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="EvoLLM-JP" target="Japanese Language Model Evaluation Harness">
  <data key="d5">8.0</data>
  <data key="d6">EvoLLM-JP is evaluated using the Japanese Language Model Evaluation Harness to measure its proficiency.</data>
  <data key="d7">evaluation, language proficiency</data>
  <data key="d8">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Setsubun" target="Risshun">
  <data key="d5">29.0</data>
  <data key="d6">Setsubun is celebrated the day before Risshun, marking a significant cultural event in Japan.&lt;SEP&gt;Setsubun marks the day before Risshun, which signifies the start of spring according to the lunisolar calendar.</data>
  <data key="d7">cultural significance, seasonal events&lt;SEP&gt;cultural significance, seasonal transition</data>
  <data key="d8">chunk-19ca32b4278ab8247ef043d83b7b65ef&lt;SEP&gt;chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Setsubun" target="February">
  <data key="d5">16.0</data>
  <data key="d6">Setsubun is celebrated in February, marking the transition to spring.</data>
  <data key="d7">cultural event, seasonal celebration</data>
  <data key="d8">chunk-19ca32b4278ab8247ef043d83b7b65ef</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Setsubun" target="2021">
  <data key="d5">9.0</data>
  <data key="d6">Last year's Setsubun was celebrated in 2021 on February 3.</data>
  <data key="d7">historical event, annual celebration</data>
  <data key="d8">chunk-19ca32b4278ab8247ef043d83b7b65ef</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Setsubun" target="2022">
  <data key="d5">9.0</data>
  <data key="d6">This year's Setsubun is celebrated in 2022 on February 4.</data>
  <data key="d7">historical event, annual celebration</data>
  <data key="d8">chunk-19ca32b4278ab8247ef043d83b7b65ef</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Setsubun" target="John">
  <data key="d5">5.0</data>
  <data key="d6">John's problem-solving task involves calculations related to the dimensions of the boxes, which ties into the broader context of Setsubun as a traditional event.</data>
  <data key="d7">cultural context, problem-solving</data>
  <data key="d8">chunk-62a104ca37857a0566f04bc4cd794e8c</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="lm-eval-harness" target="Rinna">
  <data key="d5">9.0</data>
  <data key="d6">The lm-eval-harness is compatible with results from Rinna's leaderboards, allowing for direct comparison of scores.</data>
  <data key="d7">performance comparison, evaluation framework</data>
  <data key="d8">chunk-96aa56aec4e4f18a590f28efaff42c99</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Llama-2-13b-hf" target="13B source models">
  <data key="d5">10.0</data>
  <data key="d6">Llama-2-13b-hf is categorized as a 13B source model, highlighting its performance metrics.".</data>
  <data key="d7">category classification, model type</data>
  <data key="d8">chunk-08f49a80ddeee72c7972766e2571c12b</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Swallow-70b-instruct-hf" target="Swallow-70b-hf">
  <data key="d5">16.0</data>
  <data key="d6">Both models are part of the Swallow family, showcasing high performance in various tasks and evaluations.&lt;SEP&gt;Both models are part of the Swallow family, showcasing high performance in various tasks.</data>
  <data key="d7">model family, performance comparison</data>
  <data key="d8">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Swallow-70b-hf" target="japanese-stablelm-instruct-beta-70b">
  <data key="d5">8.0</data>
  <data key="d6">Both models are high-performance AI models that focus on instruction and task execution, indicating similar applications.</data>
  <data key="d7">task execution, performance metrics</data>
  <data key="d8">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="japanese-stablelm-base-beta-70b" target="nekomata-14b-instruction">
  <data key="d5">18.0</data>
  <data key="d6">Both models are designed for advanced performance in instruction-based tasks, indicating a shared focus on instructional capabilities.</data>
  <data key="d7">instructional focus, model comparison</data>
  <data key="d8">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="nekomata-14b" target="Qwen-14B">
  <data key="d5">8.0</data>
  <data key="d6">Both models are designed to deliver competitive performance metrics across various tasks, highlighting their capability in AI evaluations.</data>
  <data key="d7">performance metrics, AI capabilities</data>
  <data key="d8">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="youri-7b-chat" target="Llama-2-70b-chat-hf">
  <data key="d5">18.0</data>
  <data key="d6">Both models are optimized for chat interactions, suggesting they serve similar purposes in conversational AI applications.&lt;SEP&gt;Both models are optimized for chat interactions, suggesting they serve similar purposes in conversational AI.</data>
  <data key="d7">chat optimization, conversational models</data>
  <data key="d8">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="youri-7b-instruction" target="nekomata-14b-instruction-gguf">
  <data key="d5">9.0</data>
  <data key="d6">Both models are tailored for instruction tasks, indicating their shared focus on enhancing user interaction through effective learning models.</data>
  <data key="d7">instructional focus, model enhancement</data>
  <data key="d8">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Swallow-MX-8x7b-NVE-v0.1" target="youri-7b-chat-gptq">
  <data key="d5">14.0</data>
  <data key="d6">Both models are part of the advanced AI model family, each targeting specific performance metrics in chat and interaction tasks.</data>
  <data key="d7">model family, performance metrics</data>
  <data key="d8">chunk-9644ad737be96b755c1c9bdea916b724</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Llama-2-70b-chat-hf" target="youri-7b-instruction-gptq">
  <data key="d5">16.0</data>
  <data key="d6">Both models are designed for natural language processing tasks and contribute to advancements in AI language understanding.&lt;SEP&gt;Both models are developed for natural language processing tasks, contributing to advancements in AI language understanding and instruction following.</data>
  <data key="d7">language models, AI development</data>
  <data key="d8">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="japanese-stablelm-base-gamma-7b" target="Swallow-13b-instruct-hf">
  <data key="d5">14.0</data>
  <data key="d6">Both models focus on language processing and instruction-following capabilities, enhancing their effectiveness in applications.&lt;SEP&gt;Both models focus on language processing and instruction-following capabilities, enhancing their effectiveness in various applications.</data>
  <data key="d7">language processing, instruction following</data>
  <data key="d8">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Swallow-13b-instruct-hf" target="llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0">
  <data key="d5">8.0</data>
  <data key="d6">Both models are designed for instruction-following tasks, highlighting advancements in AI's ability to understand and generate language based on user instructions.</data>
  <data key="d7">instruction-following, AI advancements</data>
  <data key="d8">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="nekomata-14b-gguf" target="Swallow-MS-7b-v0.1">
  <data key="d5">16.0</data>
  <data key="d6">Both models are part of the growing field of advanced language models, emphasizing performance and multi-task learning capabilities.&lt;SEP&gt;Both models are part of the growing field of advanced language models, emphasizing performance and multi-task learning.</data>
  <data key="d7">model performance, multi-task learning</data>
  <data key="d8">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Swallow-7b-instruct-hf" target="japanese-stablelm-instruct-beta-7b">
  <data key="d5">18.0</data>
  <data key="d6">Both models are designed for instruction-based tasks, indicating a trend towards improving user interaction with AI technologies.&lt;SEP&gt;Both models are designed for instruction-based tasks, indicating a trend towards improving user interaction with AI.</data>
  <data key="d7">instruction-based models, user interaction</data>
  <data key="d8">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Qwen-7B" target="youri-7b-gptq">
  <data key="d5">14.0</data>
  <data key="d6">Both models are examples of 7 billion parameter language models, showcasing the trend of scaling AI capabilities in natural language processing.&lt;SEP&gt;Both models are examples of 7 billion parameter language models, showcasing the trend of scaling AI capabilities.</data>
  <data key="d7">model scaling, AI capabilities</data>
  <data key="d8">chunk-de4fd08ec74cc6d0a33238a38b1bdb52</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="youri-7b-gptq" target="youri-7b">
  <data key="d5">28.0</data>
  <data key="d6">youri-7b-gptq and youri-7b are related as they represent different versions or configurations of the same model family.&lt;SEP&gt;youri-7b-gptq is a variant of youri-7b, indicating a development in the same model lineage for improved performance in language tasks.&lt;SEP&gt;youri-7b-gptq is a variant of youri-7b, indicating a development in the same model lineage for improved performance.</data>
  <data key="d7">model variants&lt;SEP&gt;model variants, performance improvement</data>
  <data key="d8">chunk-de4fd08ec74cc6d0a33238a38b1bdb52&lt;SEP&gt;chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="youri-7b-gptq" target="nekomata-7b-gguf">
  <data key="d5">7.0</data>
  <data key="d6">nekomata-7b-gguf and youri-7b-gptq are related as they are both models with performance metrics reflecting their effectiveness in AI applications.</data>
  <data key="d7">model effectiveness</data>
  <data key="d8">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="youri-7b" target="stockmark-13b">
  <data key="d5">7.0</data>
  <data key="d6">stockmark-13b and youri-7b are both models that have performance metrics indicating their capabilities in AI tasks.</data>
  <data key="d7">model capabilities</data>
  <data key="d8">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="ELYZA-japanese-Llama-2-7b-instruct" target="ELYZA-japanese-Llama-2-7b">
  <data key="d5">18.0</data>
  <data key="d6">Both ELYZA models are related as they belong to the same family of instruction-based models with performance metrics.</data>
  <data key="d7">model family</data>
  <data key="d8">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="japanese-stablelm-instruct-ja_vocab-beta-7b" target="japanese-stablelm-base-ja_vocab-beta-7b">
  <data key="d5">16.0</data>
  <data key="d6">Both Japanese StableLM models are related as they are part of the same series with varying performance metrics.</data>
  <data key="d7">model series</data>
  <data key="d8">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Llama-2-7b-hf" target="calm2-7b">
  <data key="d5">6.0</data>
  <data key="d6">Both calm2-7b and Llama-2-7b-hf are models that focus on performance metrics in various AI tasks.</data>
  <data key="d7">model comparison</data>
  <data key="d8">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="plamo-13b" target="plamo-13b-instruct">
  <data key="d5">14.0</data>
  <data key="d6">plamo-13b and plamo-13b-instruct are related as they represent different configurations of the same model family.</data>
  <data key="d7">model variants</data>
  <data key="d8">chunk-e9d84184a93eb3a47df509c662067d1a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="EvoLLM-JP-v1-7B" target="Koi-nobori">
  <data key="d5">16.0</data>
  <data key="d6">EvoLLM-JP-v1-7B's performance in understanding Koi-nobori reflects its grasp of Japanese cultural traditions.</data>
  <data key="d7">cultural understanding, language model performance</data>
  <data key="d8">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="EvoLLM-JP-v1-7B" target="Japanese Culture">
  <data key="d5">18.0</data>
  <data key="d6">EvoLLM-JP-v1-7B demonstrated knowledge of Japanese culture, affecting its performance on tasks related to cultural context.</data>
  <data key="d7">cultural fluency, language model</data>
  <data key="d8">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="EvoLLM-JP-v1-7B" target="Traffic Lights">
  <data key="d5">14.0</data>
  <data key="d6">EvoLLM-JP-v1-7B's understanding of traffic lights in Japan shows its ability to interpret local terminology and cultural nuances.</data>
  <data key="d7">language understanding, cultural context</data>
  <data key="d8">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="EvoLLM-JP-v1-7B" target="MGSM Test Set">
  <data key="d5">8.0</data>
  <data key="d6">EvoLLM-JP-v1-7B was tested using the MGSM Test Set to evaluate its mathematical reasoning capabilities.</data>
  <data key="d7">evaluation, performance testing</data>
  <data key="d8">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Koi-nobori" target="Japanese-Stable-VLM">
  <data key="d5">9.0</data>
  <data key="d6">Japanese-Stable-VLM successfully identified Koi-nobori, indicating its cultural knowledge.</data>
  <data key="d7">cultural understanding, language model performance</data>
  <data key="d8">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Japanese Culture" target="EvoVLM-JP">
  <data key="d5">9.0</data>
  <data key="d6">EvoVLM-JP's fluency in Japanese expression demonstrates its deep understanding of Japanese culture.</data>
  <data key="d7">cultural fluency, language model</data>
  <data key="d8">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Traffic Lights" target="EvoVLM-JP">
  <data key="d5">10.0</data>
  <data key="d6">EvoVLM-JP accurately identifies the color of traffic lights in Japan, showcasing its cultural knowledge.</data>
  <data key="d7">cultural understanding, language model performance</data>
  <data key="d8">chunk-57491ddb5284500415bc46fc834a6343</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="2021" target="2022">
  <data key="d5">12.0</data>
  <data key="d6">The years 2021 and 2022 are relevant for the calculations related to the dates of Setsubun.</data>
  <data key="d7">temporal relationship, event sequencing</data>
  <data key="d8">chunk-19ca32b4278ab8247ef043d83b7b65ef</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="2021" target="2023">
  <data key="d5">14.0</data>
  <data key="d6">The year 2021 is used to calculate the combined date for Setsubun in 2023.</data>
  <data key="d7">temporal relationship, event calculation</data>
  <data key="d8">chunk-19ca32b4278ab8247ef043d83b7b65ef</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="2021" target="Last Year's Setsubun">
  <data key="d5">17.0</data>
  <data key="d6">Last year's Setsubun corresponds to the year 2021, marking the date of celebration.&lt;SEP&gt;Last year's Setsubun was celebrated in 2021 on February 2.</data>
  <data key="d7">event year, annual tradition&lt;SEP&gt;historical event, annual celebration</data>
  <data key="d8">chunk-19ca32b4278ab8247ef043d83b7b65ef</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="2022" target="2023">
  <data key="d5">14.0</data>
  <data key="d6">The year 2022 is used to calculate the combined date for Setsubun in 2023.</data>
  <data key="d7">temporal relationship, event calculation</data>
  <data key="d8">chunk-19ca32b4278ab8247ef043d83b7b65ef</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="2022" target="This Year's Setsubun">
  <data key="d5">17.0</data>
  <data key="d6">This year's Setsubun corresponds to the year 2022, marking the date of celebration.&lt;SEP&gt;This year's Setsubun is celebrated in 2022 on February 3.</data>
  <data key="d7">event year, annual tradition&lt;SEP&gt;historical event, annual celebration</data>
  <data key="d8">chunk-19ca32b4278ab8247ef043d83b7b65ef</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="John" target="Boxes">
  <data key="d5">16.0</data>
  <data key="d6">John calculates the inner volume of the boxes, indicating his direct involvement with them.&lt;SEP&gt;John owns three boxes, which are the subject of the volume calculation problem he is tasked with solving.</data>
  <data key="d7">measurement, calculation&lt;SEP&gt;ownership, problem-solving</data>
  <data key="d8">chunk-30dbd5c2c163db07de6f5e159458d713&lt;SEP&gt;chunk-62a104ca37857a0566f04bc4cd794e8c</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="John" target="Volume Calculation">
  <data key="d5">9.0</data>
  <data key="d6">John is directly involved in the Volume Calculation event as he needs to determine the total inner volume of the boxes he owns.</data>
  <data key="d7">mathematical problem, involvement</data>
  <data key="d8">chunk-62a104ca37857a0566f04bc4cd794e8c</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="John" target="72 cubic inches">
  <data key="d5">9.0</data>
  <data key="d6">John arrives at the total inner volume of 72 cubic inches as a result of his calculations.</data>
  <data key="d7">result, calculation outcome</data>
  <data key="d8">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Boxes" target="Volume Calculation">
  <data key="d5">10.0</data>
  <data key="d6">The Boxes are the objects for which the Volume Calculation is being performed, making them central to the event.</data>
  <data key="d7">object of calculation, mathematical focus</data>
  <data key="d8">chunk-62a104ca37857a0566f04bc4cd794e8c</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Boxes" target="Dimensions">
  <data key="d5">8.0</data>
  <data key="d6">The boxes have specific dimensions that are crucial for calculating their inner volume.</data>
  <data key="d7">measurement, physical properties</data>
  <data key="d8">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Dimensions" target="Inner Volume">
  <data key="d5">9.0</data>
  <data key="d6">The inner volume is calculated based on the dimensions of the box after accounting for wall thickness.</data>
  <data key="d7">calculation, geometry</data>
  <data key="d8">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Dimensions" target="1 inch">
  <data key="d5">7.0</data>
  <data key="d6">The 1 inch thickness of the walls affects the overall dimensions of the box and subsequently the inner volume.</data>
  <data key="d7">measurement impact, wall thickness</data>
  <data key="d8">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Dimensions" target="5 inches">
  <data key="d5">8.0</data>
  <data key="d6">The original length of 5 inches is one of the dimensions used to determine the inner volume of the box.</data>
  <data key="d7">measurement, physical properties</data>
  <data key="d8">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Dimensions" target="6 inches">
  <data key="d5">8.0</data>
  <data key="d6">The original width of 6 inches is one of the dimensions used to determine the inner volume of the box.</data>
  <data key="d7">measurement, physical properties</data>
  <data key="d8">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Dimensions" target="4 inches">
  <data key="d5">8.0</data>
  <data key="d6">The original height of 4 inches is one of the dimensions used to determine the inner volume of the box.</data>
  <data key="d7">measurement, physical properties</data>
  <data key="d8">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Inner Volume" target="3 boxes">
  <data key="d5">10.0</data>
  <data key="d6">The inner volume of one box is multiplied by three to find the total volume for all boxes.</data>
  <data key="d7">aggregation, total volume</data>
  <data key="d8">chunk-30dbd5c2c163db07de6f5e159458d713</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Peace Tower" target="August 15, 1945">
  <data key="d5">9.0</data>
  <data key="d6">The Peace Tower was destroyed on August 15, 1945, during World War II, marking a significant historical event.</data>
  <data key="d7">historical event, destruction</data>
  <data key="d8">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Peace Tower" target="1964">
  <data key="d5">8.0</data>
  <data key="d6">The reconstruction of the Peace Tower in 1964 represents its rebirth after World War II's destruction.</data>
  <data key="d7">reconstruction, historical significance</data>
  <data key="d8">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="World War II" target="Hiroshima">
  <data key="d5">20.0</data>
  <data key="d6">Hiroshima was significantly impacted during World War II, particularly by the atomic bombing that led to massive destruction.&lt;SEP&gt;Hiroshima's historical significance is tied to World War II, particularly the atomic bombing that occurred there.</data>
  <data key="d7">historical impact, conflict</data>
  <data key="d8">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Hiroshima" target="Atomic Bomb Dome">
  <data key="d5">9.0</data>
  <data key="d6">The Atomic Bomb Dome is located in Hiroshima and serves as a memorial for the events that took place during World War II.</data>
  <data key="d7">location, memorial</data>
  <data key="d8">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Atomic Bomb Dome" target="August 6, 1945">
  <data key="d5">10.0</data>
  <data key="d6">The Atomic Bomb Dome stands as a reminder of the destruction that occurred on August 6, 1945, when the atomic bomb was dropped on Hiroshima.</data>
  <data key="d7">historical significance, memorial</data>
  <data key="d8">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Atomic Bomb Dome" target="Hiroshima Prefectural Industrial Promotion Hall">
  <data key="d5">8.0</data>
  <data key="d6">The Hiroshima Prefectural Industrial Promotion Hall was the original structure that became the Atomic Bomb Dome after the bombing.</data>
  <data key="d7">historical transformation, identity</data>
  <data key="d8">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Atomic Bomb Dome" target="UNESCO World Heritage Site">
  <data key="d5">9.0</data>
  <data key="d6">The Atomic Bomb Dome was designated as a UNESCO World Heritage Site in recognition of its cultural and historical importance.</data>
  <data key="d7">cultural significance, preservation</data>
  <data key="d8">chunk-dcaeb2c5b50f95dc8036fa2cb57cf45a</data>
  <data key="d9">evolutionary_optimization_model_merging.txt</data>
</edge>
<edge source="Yue Zhou" target="Jilin University">
  <data key="d5">18.0</data>
  <data key="d6">Yue Zhou is affiliated with Jilin University, where he conducts research in artificial intelligence and model merging techniques.</data>
  <data key="d7">affiliation, research</data>
  <data key="d8">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Yi Chang" target="Engineering Research Center of Knowledge-Driven Human-Machine Intelligence">
  <data key="d5">16.0</data>
  <data key="d6">Yi Chang works at the Engineering Research Center, contributing to the study of human-machine intelligence and model merging.</data>
  <data key="d7">affiliation, research</data>
  <data key="d8">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Yi Chang" target="Nuo Xu">
  <data key="d5">8.0</data>
  <data key="d6">Nuo Xu and Yi Chang are both researchers working on evaluation datasets, contributing to health-related assessments in language models.</data>
  <data key="d7">research collaboration, dataset focus</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Yi Chang" target="Yuan Wu">
  <data key="d5">7.0</data>
  <data key="d6">Yi Chang and Yuan Wu are involved in similar research areas, focusing on evaluating language models and their health applications.</data>
  <data key="d7">research alignment, evaluation focus</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Yi Chang" target="Rethinking data selection at scale">
  <data key="d5">8.0</data>
  <data key="d6">Yi Chang is a co-author contributing to the research on data selection methods in machine learning.</data>
  <data key="d7">research collaboration, data selection</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Yuan Wu" target="Jilin University">
  <data key="d5">18.0</data>
  <data key="d6">Yuan Wu is associated with Jilin University, contributing to research in model merging and AI technologies.</data>
  <data key="d7">affiliation, research</data>
  <data key="d8">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Yuan Wu" target="Yupeng Chang">
  <data key="d5">6.0</data>
  <data key="d6">Yupeng Chang and Yuan Wu are co-authors of a paper on bias-alleviating low-rank adaptation.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Yuan Wu" target="Rethinking data selection at scale">
  <data key="d5">8.0</data>
  <data key="d6">Yuan Wu is involved in the research on effective data selection methods in machine learning.</data>
  <data key="d7">research collaboration, data selection</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Jilin University" target="International Center of Future Science">
  <data key="d5">14.0</data>
  <data key="d6">Jilin University hosts the International Center of Future Science, which focuses on innovative research in various scientific fields.</data>
  <data key="d7">institutional relationship, research</data>
  <data key="d8">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Mixup Model Merge (M3)" target="Large Language Models (LLMs)">
  <data key="d5">20.0</data>
  <data key="d6">M3 is a method designed to enhance the performance of LLMs by improving their merging capabilities.</data>
  <data key="d7">methodology, AI advancement</data>
  <data key="d8">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Natural Language Processing (NLP)" target="Large Language Models (LLMs)">
  <data key="d5">18.0</data>
  <data key="d6">NLP utilizes LLMs to perform complex language tasks, showcasing their capabilities in the field.</data>
  <data key="d7">field application, technology</data>
  <data key="d8">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Large Language Models (LLMs)" target="Supervised Fine-Tuning (SFT)">
  <data key="d5">8.0</data>
  <data key="d6">SFT is a technique used to refine the performance of LLMs for specific tasks, making them more effective in application.</data>
  <data key="d7">training technique, model performance</data>
  <data key="d8">chunk-3df998a85b2dff618feb2e2ce206b62c</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="WizardLM-13B">
  <data key="d5">16.0</data>
  <data key="d6">WizardLM-13B benefits from the M&lt;sup&gt;3&lt;/sup&gt; method which helps in improving its instruction-following capabilities.&lt;SEP&gt;WizardLM-13B was one of the fine-tuned models evaluated to assess the effectiveness of the M&lt;sup&gt;3&lt;/sup&gt; technique.</data>
  <data key="d7">model evaluation, technique application&lt;SEP&gt;model improvement, instruction-following</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="WizardMath-13B">
  <data key="d5">34.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; is used to improve the performance of the merged model involving WizardMath-13B, enhancing its effectiveness in code generation.&lt;SEP&gt;WizardMath-13B utilizes the M&lt;sup&gt;3&lt;/sup&gt; method to enhance its performance in mathematical reasoning tasks.&lt;SEP&gt;WizardMath-13B was tested in conjunction with M&lt;sup&gt;3&lt;/sup&gt; to evaluate its impact on mathematical reasoning models.</data>
  <data key="d7">model evaluation, technique application&lt;SEP&gt;model improvement, mathematical reasoning&lt;SEP&gt;performance enhancement, model merging</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="llama-2-13b-code-alpaca">
  <data key="d5">16.0</data>
  <data key="d6">llama-2-13b-code-alpaca employs the M&lt;sup&gt;3&lt;/sup&gt; method to optimize its capabilities in code generation tasks.&lt;SEP&gt;llama-2-13b-code-alpaca was included in the experiments to determine the effectiveness of the M&lt;sup&gt;3&lt;/sup&gt; technique in code generation.</data>
  <data key="d7">model evaluation, technique application&lt;SEP&gt;model improvement, code generation</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="LiveBench">
  <data key="d5">18.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; enhances performance in LiveBench evaluations, indicating its effectiveness in various tasks."|&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; was validated through extensive evaluations on LiveBench to check the robustness of merged models.</data>
  <data key="d7">evaluation framework, robustness testing&lt;SEP&gt;performance improvement, evaluation</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="PromptBench">
  <data key="d5">25.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; is validated using PromptBench to assess its effectiveness in improving model robustness.&lt;SEP&gt;The effectiveness of M&lt;sup&gt;3&lt;/sup&gt; was also tested on PromptBench, further assessing its impact on model performance.</data>
  <data key="d7">evaluation framework, robustness testing&lt;SEP&gt;validation, performance assessment</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Yang et al.">
  <data key="d5">9.0</data>
  <data key="d6">Yang et al. contributed to the development of the M&lt;sup&gt;3&lt;/sup&gt; technique, which enhances model merging processes.</data>
  <data key="d7">research contribution, technique development</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Akiba et al.">
  <data key="d5">8.0</data>
  <data key="d6">Akiba et al. are cited regarding advancements in model merging, influencing the M&lt;sup&gt;3&lt;/sup&gt; technique.</data>
  <data key="d7">research contribution, technique development</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Wortsman et al.">
  <data key="d5">8.0</data>
  <data key="d6">Wortsman et al. are referenced for their work on parameter fusion strategies that M&lt;sup&gt;3&lt;/sup&gt; aims to improve upon.</data>
  <data key="d7">research contribution, technique development</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Ilharco et al.">
  <data key="d5">8.0</data>
  <data key="d6">Ilharco et al. are mentioned in the context of limitations that M&lt;sup&gt;3&lt;/sup&gt; seeks to address in model merging.</data>
  <data key="d7">research contribution, technique improvement</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Zhang">
  <data key="d5">9.0</data>
  <data key="d6">Zhang's Mixup technique inspired the M&lt;sup&gt;3&lt;/sup&gt; method, indicating a direct influence on its development.</data>
  <data key="d7">inspiration, technique development</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Zhu et al.">
  <data key="d5">8.0</data>
  <data key="d6">Zhu et al. contributed to the development of M&lt;sup&gt;3&lt;/sup&gt; for enhancing model robustness.</data>
  <data key="d7">research contribution, model robustness</data>
  <data key="d8">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Yu et al.">
  <data key="d5">8.0</data>
  <data key="d6">Yu et al. have investigated the role of LLMs in enhancing model merging capabilities, relating to M&lt;sup&gt;3&lt;/sup&gt;.</data>
  <data key="d7">research exploration, model enhancement</data>
  <data key="d8">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="SFT">
  <data key="d5">8.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; utilizes SFT models to create a merged model by interpolating their parameters.</data>
  <data key="d7">model merging, parameter adjustment</data>
  <data key="d8">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Beta Distribution">
  <data key="d5">9.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; employs the Beta distribution to determine the linear interpolation ratio for merging models.</data>
  <data key="d7">statistical method, interpolation</data>
  <data key="d8">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Vicinal Risk Minimization (VRM)">
  <data key="d5">7.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; extends the VRM principle by applying interpolation in the model parameter space for improved performance.</data>
  <data key="d7">theoretical framework, model enhancement</data>
  <data key="d8">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Task-Specific Fine-Tuned LLMs">
  <data key="d5">9.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; is a method that enhances the performance of task-specific fine-tuned LLMs by merging their parameters through linear interpolation.</data>
  <data key="d7">model merging, performance enhancement</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="_{SFT}^{t_1}">
  <data key="d5">8.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; uses _{SFT}^{t_1} as one of the parameters for linear interpolation in model training.</data>
  <data key="d7">parameter usage, model training</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="_{SFT}^{t_2}">
  <data key="d5">8.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; incorporates _{SFT}^{t_2} in the parameter merging process to enhance model performance.</data>
  <data key="d7">parameter usage, model training</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="_m">
  <data key="d5">9.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; employs _m as the interpolation ratio to effectively combine parameters from different models.</data>
  <data key="d7">parameter combination, model merging</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="P_{}(_M)">
  <data key="d5">9.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; results in the formation of P_{}(_M) as the new distribution representing merged model parameters.</data>
  <data key="d7">distribution formation, model merging</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Dirac Delta Function">
  <data key="d5">8.0</data>
  <data key="d6">The Dirac delta function is used in M&lt;sup&gt;3&lt;/sup&gt; to ensure that the interpolated parameters satisfy the linear interpolation rule.</data>
  <data key="d7">mathematical enforcement, model merging</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Neighborhood Distribution">
  <data key="d5">9.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; creates a neighborhood distribution that integrates knowledge from different tasks, enhancing model performance.</data>
  <data key="d7">knowledge integration, model performance</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Decision Boundaries">
  <data key="d5">9.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; aims to create smoother decision boundaries between different tasks, improving task adaptability.</data>
  <data key="d7">task adaptability, performance improvement</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Occam's Razor">
  <data key="d5">8.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; aligns with Occam's Razor by promoting simpler solutions that generalize better across tasks.</data>
  <data key="d7">principle application, generalization</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Task Conflicts">
  <data key="d5">9.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; addresses task conflicts by balancing conflicting parameter values for improved performance.</data>
  <data key="d7">conflict mitigation, performance enhancement</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Experiments">
  <data key="d5">10.0</data>
  <data key="d6">Experiments are conducted to evaluate the effectiveness of the M&lt;sup&gt;3&lt;/sup&gt; method in various tasks.</data>
  <data key="d7">evaluation, method effectiveness</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Average Merging">
  <data key="d5">25.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; is integrated into Average Merging to enhance the performance of merging fine-tuned LLMs."|&lt;SEP&gt;When Average Merging is combined with M&lt;sup&gt;3&lt;/sup&gt;, it contributes to performance improvements in model evaluations.</data>
  <data key="d7">method integration, performance boost&lt;SEP&gt;model merging, performance improvement</data>
  <data key="d8">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Matthews Correlation Coefficient (MCC)">
  <data key="d5">8.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; is assessed using Matthews Correlation Coefficient (MCC) as a performance metric for models on the CoLA dataset."|</data>
  <data key="d7">performance metric, evaluation</data>
  <data key="d8">chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="HumanEval">
  <data key="d5">9.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; shows improvements in performance on HumanEval, linking the merging technique with code generation evaluation."|</data>
  <data key="d7">performance improvement, evaluation</data>
  <data key="d8">chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="MBPP">
  <data key="d5">29.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; is evaluated against MBPP, demonstrating its impact on coding task performance."|&lt;SEP&gt;The M&lt;sup&gt;3&lt;/sup&gt; approach shows a measurable improvement in the pass@1 score on the MBPP benchmark for the merged model.</data>
  <data key="d7">benchmark improvement, evaluation&lt;SEP&gt;performance improvement, evaluation</data>
  <data key="d8">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="llama-2-13b-codealpaca">
  <data key="d5">16.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; addresses the suboptimal results of merging with llama-2-13b-codealpaca, aiming to improve its performance in code generation.</data>
  <data key="d7">performance enhancement, model optimization</data>
  <data key="d8">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="LiveBench-Instruction">
  <data key="d5">14.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; enhances the win rate of the Math &amp; Code model on the LiveBench-Instruction dataset.</data>
  <data key="d7">dataset evaluation, performance improvement</data>
  <data key="d8">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="LiveBench-Coding">
  <data key="d5">14.0</data>
  <data key="d6">The LM &amp; Math model achieves better results on the LiveBench-Coding dataset through the application of M&lt;sup&gt;3&lt;/sup&gt;.</data>
  <data key="d7">dataset evaluation, performance improvement</data>
  <data key="d8">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="LiveBench-TypoFixing">
  <data key="d5">16.0</data>
  <data key="d6">The LM &amp; Code model shows significant accuracy improvements on the LiveBench-TypoFixing dataset when using M&lt;sup&gt;3&lt;/sup&gt;.</data>
  <data key="d7">dataset evaluation, performance improvement</data>
  <data key="d8">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Math &amp; Code">
  <data key="d5">36.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; enhances the performance of the Math &amp; Code model across various tasks and merging strategies.&lt;SEP&gt;The Math &amp; Code model achieves significant performance improvements when merged with M&lt;sup&gt;3&lt;/sup&gt;, demonstrating the effectiveness of this approach.&lt;SEP&gt;The Math &amp; Code model achieves significant performance improvements when merged with M&lt;sup&gt;3&lt;/sup&gt;.</data>
  <data key="d7">model performance, merging technique&lt;SEP&gt;performance enhancement, model improvement</data>
  <data key="d8">chunk-070652f4f36e86620289fcafaaa29599&lt;SEP&gt;chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="LM &amp; Math">
  <data key="d5">18.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; contributes to the performance improvements seen in the LM &amp; Math model on diverse tasks.</data>
  <data key="d7">performance enhancement, model improvement</data>
  <data key="d8">chunk-070652f4f36e86620289fcafaaa29599</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="LM &amp; Code">
  <data key="d5">36.0</data>
  <data key="d6">LM &amp; Code sees notable accuracy increases when enhanced with M&lt;sup&gt;3&lt;/sup&gt; during merging, indicating a strong relationship between the model and the merging technique.&lt;SEP&gt;LM &amp; Code sees notable accuracy increases when enhanced with M&lt;sup&gt;3&lt;/sup&gt; during merging.&lt;SEP&gt;M&lt;sup&gt;3&lt;/sup&gt; improves the robustness and performance of the LM &amp; Code model on various benchmarks.</data>
  <data key="d7">model performance, merging technique&lt;SEP&gt;performance enhancement, model improvement</data>
  <data key="d8">chunk-070652f4f36e86620289fcafaaa29599&lt;SEP&gt;chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Model Merging Techniques">
  <data key="d5">8.0</data>
  <data key="d6">Various model merging techniques are evaluated alongside M&lt;sup&gt;3&lt;/sup&gt;, indicating that M&lt;sup&gt;3&lt;/sup&gt; offers a novel approach compared to traditional methods.</data>
  <data key="d7">comparison, merging techniques</data>
  <data key="d8">chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Parameter Linear Interpolation Process">
  <data key="d5">9.0</data>
  <data key="d6">The parameter linear interpolation process is a core component of M&lt;sup&gt;3&lt;/sup&gt;, enabling dynamic adjustments that lead to improved model merging outcomes.</data>
  <data key="d7">core technique, model merging</data>
  <data key="d8">chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Performance Metrics">
  <data key="d5">8.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; aims to enhance performance metrics such as accuracy and MCC, demonstrating its impact on model evaluation.</data>
  <data key="d7">evaluation criteria, performance improvement</data>
  <data key="d8">chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="Adversarial Robustness">
  <data key="d5">9.0</data>
  <data key="d6">M&lt;sup&gt;3&lt;/sup&gt; is designed to improve adversarial robustness in merged models, indicating its relevance to model reliability.</data>
  <data key="d7">model reliability, performance under stress</data>
  <data key="d8">chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="M&lt;sup&gt;3&lt;/sup&gt;" target="BERTAttack">
  <data key="d5">14.0</data>
  <data key="d6">BERTAttack is utilized in the context of evaluating models that may be merged using the M&lt;sup&gt;3&lt;/sup&gt; technique, highlighting its relevance in adversarial robustness studies.</data>
  <data key="d7">adversarial evaluation, model merging</data>
  <data key="d8">chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="WizardLM-13B" target="AlpacaEval">
  <data key="d5">7.0</data>
  <data key="d6">AlpacaEval is used to evaluate the performance of the WizardLM-13B model in instruction-following tasks.</data>
  <data key="d7">evaluation benchmark, instruction-following</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="WizardLM-13B" target="Instruction Following">
  <data key="d5">9.0</data>
  <data key="d6">The WizardLM-13B model is specifically designed for instruction following tasks, showcasing its specialized capabilities.</data>
  <data key="d7">task specialization, model design</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="WizardLM-13B" target="LiveBench-Coding">
  <data key="d5">8.0</data>
  <data key="d6">WizardLM-13B was evaluated on the LiveBench-Coding event, showcasing its performance in coding tasks.</data>
  <data key="d7">performance evaluation, coding tasks</data>
  <data key="d8">chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="WizardLM-13B" target="Pass@1">
  <data key="d5">9.0</data>
  <data key="d6">WizardLM-13B's performance on LiveBench-Coding is evaluated using the Pass@1 metric to determine its accuracy.</data>
  <data key="d7">evaluation metric, performance assessment</data>
  <data key="d8">chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="WizardLM-13B" target="Evol-Instruct">
  <data key="d5">9.0</data>
  <data key="d6">Evol-Instruct is a method used in the development of WizardLM-13B to generate effective instruction data.</data>
  <data key="d7">method application, model improvement</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="WizardMath-13B" target="GSM8K">
  <data key="d5">16.0</data>
  <data key="d6">GSM8K is a dataset used to train WizardMath-13B for mathematical reasoning tasks.&lt;SEP&gt;GSM8K serves as a testing dataset for evaluating the performance of the WizardMath-13B model in mathematical reasoning tasks.</data>
  <data key="d7">dataset utilization, model training&lt;SEP&gt;evaluation dataset, mathematical reasoning</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="WizardMath-13B" target="MATH">
  <data key="d5">16.0</data>
  <data key="d6">MATH is a dataset utilized for enhancing the capabilities of WizardMath-13B in mathematical reasoning.&lt;SEP&gt;MATH is utilized to assess the capabilities of the WizardMath-13B model in mathematical reasoning tasks.</data>
  <data key="d7">dataset utilization, model training&lt;SEP&gt;evaluation dataset, mathematical reasoning</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc&lt;SEP&gt;chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="WizardMath-13B" target="Mathematical Reasoning">
  <data key="d5">9.0</data>
  <data key="d6">The WizardMath-13B model is specifically fine-tuned for mathematical reasoning tasks, highlighting its focus area.</data>
  <data key="d7">task specialization, model design</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="WizardMath-13B" target="LiveBench-TypoFixing">
  <data key="d5">9.0</data>
  <data key="d6">WizardMath-13B demonstrated a 4% enhancement in accuracy during the LiveBench-TypoFixing event.</data>
  <data key="d7">performance improvement, typo correction</data>
  <data key="d8">chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="WizardMath-13B" target="Accuracy Improvement">
  <data key="d5">8.0</data>
  <data key="d6">WizardMath-13B achieved an accuracy improvement of 4% in LiveBench-TypoFixing, indicating its enhanced performance.</data>
  <data key="d7">performance enhancement, accuracy</data>
  <data key="d8">chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="llama-2-13b-code-alpaca" target="HumanEval">
  <data key="d5">7.0</data>
  <data key="d6">HumanEval is a benchmark used to evaluate the code generation performance of the llama-2-13b-code-alpaca model.</data>
  <data key="d7">evaluation benchmark, code generation</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="llama-2-13b-code-alpaca" target="MBPP">
  <data key="d5">7.0</data>
  <data key="d6">MBPP is a benchmark that assesses the performance of the llama-2-13b-code-alpaca model in code generation tasks.</data>
  <data key="d7">evaluation benchmark, code generation</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="llama-2-13b-code-alpaca" target="Code Generation">
  <data key="d5">9.0</data>
  <data key="d6">The llama-2-13b-code-alpaca model is tailored for code generation tasks, demonstrating its specialized function.</data>
  <data key="d7">task specialization, model design</data>
  <data key="d8">chunk-58a8a639dd3eddf8bbf49b678e294fd7</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="LiveBench" target="White et al.">
  <data key="d5">9.0</data>
  <data key="d6">White et al. conducted evaluations on LiveBench to test the robustness of models, including those improved by M&lt;sup&gt;3&lt;/sup&gt;.</data>
  <data key="d7">evaluation, model robustness</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="LiveBench" target="Appendix A.1">
  <data key="d5">8.0</data>
  <data key="d6">LiveBench is elaborated upon in Appendix A.1, which provides details on its evaluation methods and frameworks."|</data>
  <data key="d7">evaluation framework, documentation</data>
  <data key="d8">chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="LiveBench" target="Out-of-Distribution Dataset Selection">
  <data key="d5">9.0</data>
  <data key="d6">Out-of-Distribution Dataset Selection is a crucial part of LiveBench's evaluation framework for assessing model robustness.</data>
  <data key="d7">evaluation framework, model testing</data>
  <data key="d8">chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="LiveBench" target="Instruction Following">
  <data key="d5">8.0</data>
  <data key="d6">Instruction Following is a task category featured in LiveBench to evaluate the ability of models to follow instructions.</data>
  <data key="d7">task evaluation, model capability</data>
  <data key="d8">chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="LiveBench" target="Coding">
  <data key="d5">8.0</data>
  <data key="d6">Coding is another task category in LiveBench that tests the coding abilities of language models.</data>
  <data key="d7">task evaluation, model capability</data>
  <data key="d8">chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="LiveBench" target="Language Comprehension">
  <data key="d5">8.0</data>
  <data key="d6">Language Comprehension is a task category in LiveBench focusing on a model's understanding of language intricacies.</data>
  <data key="d7">task evaluation, model capability</data>
  <data key="d8">chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="PromptBench" target="Zhu et al.">
  <data key="d5">17.0</data>
  <data key="d6">Zhu et al. developed the Adversarial Prompt Attacks module within PromptBench, linking their research directly to the evaluation of LLMs."|&lt;SEP&gt;Zhu et al. evaluated the effectiveness of M&lt;sup&gt;3&lt;/sup&gt; using PromptBench, assessing model performance.</data>
  <data key="d7">development, evaluation&lt;SEP&gt;evaluation, model effectiveness</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="PromptBench" target="Adversarial Prompt Attacks">
  <data key="d5">9.0</data>
  <data key="d6">PromptBench includes an Adversarial Prompt Attacks module to evaluate the robustness of LLMs against adversarial prompts.</data>
  <data key="d7">robustness evaluation, testing framework</data>
  <data key="d8">chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="PromptBench" target="Adversarial Robustness Evaluation">
  <data key="d5">9.0</data>
  <data key="d6">Adversarial Robustness Evaluation is conducted using the PromptBench framework to assess how models handle adversarial prompts.</data>
  <data key="d7">evaluation method, model robustness</data>
  <data key="d8">chunk-b650b88d4bebbc45d20b8908b045eff8</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Ding et al." target="Brown et al.">
  <data key="d5">6.0</data>
  <data key="d6">Ding et al. and Brown et al. are both referenced in discussions regarding model merging and computational resources.</data>
  <data key="d7">research collaboration, citation</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Xia et al." target="Chang et al.">
  <data key="d5">6.0</data>
  <data key="d6">Xia et al. and Chang et al. are both cited in the context of advancements in model merging techniques.</data>
  <data key="d7">research collaboration, citation</data>
  <data key="d8">chunk-f114e9b193fa84b93a73655c49035fc1</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="White et al." target="Austin et al.">
  <data key="d5">7.0</data>
  <data key="d6">Austin et al. and White et al. are both involved in the evaluation of LLMs and their datasets, indicating collaboration or related research."|</data>
  <data key="d7">research collaboration, evaluation</data>
  <data key="d8">chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Fisher Merging" target="Matena and Raffel">
  <data key="d5">8.0</data>
  <data key="d6">Matena and Raffel's work focuses on Fisher Merging, which is a specific technique within model merging.</data>
  <data key="d7">specific technique, research focus</data>
  <data key="d8">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Mixup" target="Empirical Risk Minimization (ERM)">
  <data key="d5">18.0</data>
  <data key="d6">Mixup serves as an enhancement over traditional ERM by providing a new way to train models through virtual examples.</data>
  <data key="d7">training enhancement, model generalization</data>
  <data key="d8">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Mixup" target="Vicinal Risk Minimization (VRM)">
  <data key="d5">16.0</data>
  <data key="d6">Mixup is rooted in the principles of VRM, aiming to improve generalization in machine learning models.</data>
  <data key="d7">theoretical foundation, model improvement</data>
  <data key="d8">chunk-c8ff785e8102dfde59dbc9309abb6b6a</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Mixup" target="LLMs">
  <data key="d5">9.0</data>
  <data key="d6">Mixup can be applied to LLMs to improve their robustness and performance by augmenting the training dataset.</data>
  <data key="d7">data augmentation, model training</data>
  <data key="d8">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Mixup Model Merge" target="Fine-Tuned LLMs">
  <data key="d5">8.0</data>
  <data key="d6">Mixup Model Merge (M&lt;sup&gt;3&lt;/sup&gt;) utilizes fine-tuned LLMs as the basis for its parameter merging process.</data>
  <data key="d7">model merging, machine learning</data>
  <data key="d8">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Mixup Model Merge" target="Average Merging">
  <data key="d5">7.0</data>
  <data key="d6">Mixup Model Merge incorporates concepts from Average Merging to enhance its model fusion approach.</data>
  <data key="d7">model merging techniques, innovation</data>
  <data key="d8">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Fine-Tuned LLMs" target="Llama 2">
  <data key="d5">9.0</data>
  <data key="d6">Fine-tuned LLMs often utilize Llama 2 as their underlying architecture to improve task performance.</data>
  <data key="d7">model architecture, performance enhancement</data>
  <data key="d8">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Beta Distribution" target="Hyperparameter">
  <data key="d5">10.0</data>
  <data key="d6">The interpolation ratio in Mixup is determined by a hyperparameter sampled from a Beta distribution, influencing the model merging process.</data>
  <data key="d7">statistical modeling, machine learning</data>
  <data key="d8">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Llama 2" target="Instruction-following llama model">
  <data key="d5">18.0</data>
  <data key="d6">Llama 2 is part of the advancements in instruction-following models, indicating ongoing research in AI.</data>
  <data key="d7">AI research, model development</data>
  <data key="d8">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Interpolation Ratio" target="Synthetic Sample">
  <data key="d5">10.0</data>
  <data key="d6">The Interpolation Ratio directly influences how Synthetic Samples are generated during the Mixup process.</data>
  <data key="d7">data generation, sample creation</data>
  <data key="d8">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Parameters" target="Fine-Tuned Models">
  <data key="d5">9.0</data>
  <data key="d6">The Parameters of Fine-Tuned Models are adjusted to enhance performance on specific tasks after initial training.</data>
  <data key="d7">model training, performance optimization</data>
  <data key="d8">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Fine-Tuned Models" target="Pre-Trained Backbone">
  <data key="d5">10.0</data>
  <data key="d6">Fine-Tuned Models are built upon a Pre-Trained Backbone, leveraging its learned features for improved task performance.</data>
  <data key="d7">model architecture, training efficiency</data>
  <data key="d8">chunk-c0e16cf5d449d6cfe6e80843b2eefa06</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="$\theta^t_{SFT}$" target="$\theta_{PRE}$">
  <data key="d5">9.0</data>
  <data key="d6">$\theta^t_{SFT}$ is derived from $\theta_{PRE}$ through the process of Supervised Fine-Tuning, indicating a progression in model training.</data>
  <data key="d7">model training, parameter evolution</data>
  <data key="d8">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="$\theta^t_{SFT}$" target="$\delta^t$">
  <data key="d5">8.0</data>
  <data key="d6">$\delta^t$ quantifies the change in parameters from $\theta_{PRE}$ to $\theta^t_{SFT}$, illustrating the effect of fine-tuning.</data>
  <data key="d7">parameter change, fine-tuning effect</data>
  <data key="d8">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="$\lambda_m$" target="$\alpha$">
  <data key="d5">7.0</data>
  <data key="d6">$\lambda_m$ is sampled from a Beta distribution where $\alpha$ influences the shape and behavior of the distribution.</data>
  <data key="d7">sampling method, distribution characteristics</data>
  <data key="d8">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="$\alpha$" target="Figure 3">
  <data key="d5">6.0</data>
  <data key="d6">Figure 3 provides a visual representation of how different values of $\alpha$ affect the Beta distribution used for $\lambda_m$.</data>
  <data key="d7">visualization, statistical analysis</data>
  <data key="d8">chunk-6c3c559e2fcc2c84ba368ddf5b7db117</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="AlpacaEval" target="GSM8K">
  <data key="d5">23.0</data>
  <data key="d6">AlpacaEval is used to evaluate models on the GSM8K dataset for instruction-following tasks.&lt;SEP&gt;AlpacaEval is utilized to evaluate models on the GSM8K dataset for instruction-following tasks.".&lt;SEP&gt;GSM8K is used to evaluate zero-shot accuracy in AlpacaEval, indicating a connection between the two benchmarks."|</data>
  <data key="d7">benchmark relationship, evaluation&lt;SEP&gt;evaluation metric, dataset usage</data>
  <data key="d8">chunk-bbe47f2e361d8d9243dc14b2ce92036b&lt;SEP&gt;chunk-9fbd0c0622e4d5fb84be83781e1c4521</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="GSM8K" target="MATH">
  <data key="d5">18.0</data>
  <data key="d6">Both GSM8K and MATH datasets are used to evaluate mathematical reasoning abilities of language models.&lt;SEP&gt;Both GSM8K and MATH datasets are used to evaluate mathematical reasoning abilities of language models.".</data>
  <data key="d7">dataset comparison, evaluation</data>
  <data key="d8">chunk-9fbd0c0622e4d5fb84be83781e1c4521</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="HumanEval" target="MBPP">
  <data key="d5">16.0</data>
  <data key="d6">HumanEval and MBPP are datasets used to assess different aspects of code generation models.&lt;SEP&gt;HumanEval and MBPP are datasets used to assess different aspects of code generation models.".</data>
  <data key="d7">dataset comparison, evaluation</data>
  <data key="d8">chunk-9fbd0c0622e4d5fb84be83781e1c4521</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="MBPP" target="Math &amp; Code">
  <data key="d5">16.0</data>
  <data key="d6">Math &amp; Code achieves specific performance scores on the MBPP dataset when merged with DARE, showcasing the effectiveness of combining different techniques.&lt;SEP&gt;Math &amp; Code achieves specific performance scores on the MBPP dataset when merged with DARE.</data>
  <data key="d7">dataset performance, model evaluation</data>
  <data key="d8">chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="SST2" target="Math &amp; Code">
  <data key="d5">22.0</data>
  <data key="d6">Math &amp; Code was evaluated using the SST2 dataset to measure its performance in sentiment analysis.&lt;SEP&gt;The performance of Math &amp; Code is evaluated using the SST2 dataset, showing improvements with M&lt;sup&gt;3&lt;/sup&gt;, which highlights the practical applications of the merging technique.&lt;SEP&gt;The performance of Math &amp; Code is evaluated using the SST2 dataset, showing improvements with M&lt;sup&gt;3&lt;/sup&gt;.</data>
  <data key="d7">dataset evaluation, performance testing&lt;SEP&gt;dataset evaluation, sentiment analysis</data>
  <data key="d8">chunk-407b385e429f3495cdcf3f23f7e80f73&lt;SEP&gt;chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="SST2" target="LM &amp; Math">
  <data key="d5">8.0</data>
  <data key="d6">LM &amp; Math was tested on the SST2 dataset to analyze its effectiveness in sentiment classification.</data>
  <data key="d7">dataset evaluation, performance testing</data>
  <data key="d8">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="SST2" target="LM &amp; Code">
  <data key="d5">8.0</data>
  <data key="d6">LM &amp; Code was also evaluated on the SST2 dataset to determine its adversarial robustness.</data>
  <data key="d7">dataset evaluation, performance testing</data>
  <data key="d8">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="SST2" target="Sentiment Analysis Dataset">
  <data key="d5">9.0</data>
  <data key="d6">SST2 is a specific sentiment analysis dataset used to evaluate models' abilities to classify sentiments in text.</data>
  <data key="d7">dataset classification, evaluation</data>
  <data key="d8">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="SST2" target="CoLA">
  <data key="d5">16.0</data>
  <data key="d6">CoLA and SST2 are both datasets used in natural language processing for different purposes, with CoLA focusing on linguistic acceptability and SST2 on sentiment analysis.</data>
  <data key="d7">dataset comparison, NLP evaluation</data>
  <data key="d8">chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="CoLA" target="Math &amp; Code">
  <data key="d5">22.0</data>
  <data key="d6">Math &amp; Code was tested on the CoLA dataset to evaluate its grammar correctness capability.&lt;SEP&gt;Math &amp; Code's performance metrics are assessed using the CoLA dataset, emphasizing the model's capabilities in grammar correctness when enhanced with M&lt;sup&gt;3&lt;/sup&gt;.&lt;SEP&gt;Math &amp; Code's performance metrics are assessed using the CoLA dataset, highlighting improvements with M&lt;sup&gt;3&lt;/sup&gt;.</data>
  <data key="d7">dataset evaluation, grammar correctness&lt;SEP&gt;dataset evaluation, performance testing</data>
  <data key="d8">chunk-407b385e429f3495cdcf3f23f7e80f73&lt;SEP&gt;chunk-47449b4eaab59a26168fa9d821973678</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="CoLA" target="LM &amp; Math">
  <data key="d5">8.0</data>
  <data key="d6">LM &amp; Math was assessed on the CoLA dataset to determine its performance in grammar correctness.</data>
  <data key="d7">dataset evaluation, performance testing</data>
  <data key="d8">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="CoLA" target="LM &amp; Code">
  <data key="d5">8.0</data>
  <data key="d6">LM &amp; Code was evaluated on the CoLA dataset to assess its grammar correctness performance.</data>
  <data key="d7">dataset evaluation, performance testing</data>
  <data key="d8">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="CoLA" target="Grammar Correctness Dataset">
  <data key="d5">9.0</data>
  <data key="d6">CoLA serves as a grammar correctness dataset that assesses the grammaticality of sentences, used in model evaluations.</data>
  <data key="d7">dataset classification, evaluation</data>
  <data key="d8">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Adversarial Prompt Attacks" target="DeepWordBug">
  <data key="d5">16.0</data>
  <data key="d6">DeepWordBug is one of the attack methods used within the Adversarial Prompt Attacks module to assess LLM robustness."|&lt;SEP&gt;DeepWordBug is one of the methods employed in Adversarial Prompt Attacks to test language model robustness.</data>
  <data key="d7">attack method, evaluation&lt;SEP&gt;evaluation method, adversarial testing</data>
  <data key="d8">chunk-b650b88d4bebbc45d20b8908b045eff8&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Adversarial Prompt Attacks" target="BERTAttack">
  <data key="d5">16.0</data>
  <data key="d6">BERTAttack is another attack method featured in the Adversarial Prompt Attacks module for evaluating language model resilience."|&lt;SEP&gt;BERTAttack is another method used to evaluate language models within the Adversarial Prompt Attacks framework.</data>
  <data key="d7">attack method, evaluation&lt;SEP&gt;evaluation method, adversarial testing</data>
  <data key="d8">chunk-b650b88d4bebbc45d20b8908b045eff8&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Adversarial Prompt Attacks" target="StressTest">
  <data key="d5">16.0</data>
  <data key="d6">StressTest is included in the Adversarial Prompt Attacks module as a method to evaluate LLMs against adversarial prompts."|&lt;SEP&gt;StressTest is included in the Adversarial Prompt Attacks to assess how models handle distractions and maintain performance.</data>
  <data key="d7">attack method, evaluation&lt;SEP&gt;evaluation method, distraction testing</data>
  <data key="d8">chunk-b650b88d4bebbc45d20b8908b045eff8&lt;SEP&gt;chunk-bbe47f2e361d8d9243dc14b2ce92036b</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="DeepWordBug" target="Math &amp; Code">
  <data key="d5">23.0</data>
  <data key="d6">DeepWordBug is used to evaluate the adversarial robustness of the Math &amp; Code model, assessing its vulnerability to attacks.&lt;SEP&gt;DeepWordBug was applied to evaluate the adversarial robustness of the Math &amp; Code model on the SST2 and CoLA datasets.</data>
  <data key="d7">adversarial evaluation, model robustness&lt;SEP&gt;adversarial testing, robustness evaluation</data>
  <data key="d8">chunk-070652f4f36e86620289fcafaaa29599&lt;SEP&gt;chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="DeepWordBug" target="LM &amp; Math">
  <data key="d5">9.0</data>
  <data key="d6">DeepWordBug was used to assess the robustness of the LM &amp; Math model against adversarial attacks on the datasets.</data>
  <data key="d7">adversarial testing, robustness evaluation</data>
  <data key="d8">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="DeepWordBug" target="LM &amp; Code">
  <data key="d5">9.0</data>
  <data key="d6">DeepWordBug was utilized to evaluate the LM &amp; Code model's performance under adversarial conditions on the datasets.</data>
  <data key="d7">adversarial testing, robustness evaluation</data>
  <data key="d8">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="BERTAttack" target="LM &amp; Math">
  <data key="d5">14.0</data>
  <data key="d6">BERTAttack assesses the robustness of the LM &amp; Math model, helping to identify weaknesses against adversarial inputs.</data>
  <data key="d7">adversarial evaluation, model robustness</data>
  <data key="d8">chunk-070652f4f36e86620289fcafaaa29599</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="BERTAttack" target="Performance Drop Rate (PDR)">
  <data key="d5">18.0</data>
  <data key="d6">Performance Drop Rate (PDR) is a metric evaluated through the BERTAttack method to assess model robustness against adversarial attacks.</data>
  <data key="d7">evaluation metric, adversarial robustness</data>
  <data key="d8">chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="StressTest" target="LM &amp; Code">
  <data key="d5">14.0</data>
  <data key="d6">StressTest is employed to evaluate the adversarial performance of the LM &amp; Code model, measuring its robustness under attack.</data>
  <data key="d7">adversarial evaluation, model robustness</data>
  <data key="d8">chunk-070652f4f36e86620289fcafaaa29599</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="llama-2-13b-codealpaca" target="LiveBench-Instruction">
  <data key="d5">7.0</data>
  <data key="d6">llama-2-13b-codealpaca showed improved performance in the LiveBench-Instruction event through the application of TIES-Merging.</data>
  <data key="d7">performance enhancement, instruction following</data>
  <data key="d8">chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="llama-2-13b-codealpaca" target="Win Rate">
  <data key="d5">7.0</data>
  <data key="d6">The application of TIES-Merging led to a 1.1% improvement in the win rate of llama-2-13b-codealpaca on LiveBench-Instruction.</data>
  <data key="d7">performance improvement, competitive success</data>
  <data key="d8">chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="LiveBench-Instruction" target="Win Rate">
  <data key="d5">9.0</data>
  <data key="d6">The win rate of models is determined based on their performance during the LiveBench-Instruction event, reflecting their ability to follow instructions.</data>
  <data key="d7">event evaluation, performance metrics</data>
  <data key="d8">chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="LiveBench-Instruction" target="LM &amp; Math">
  <data key="d5">16.0</data>
  <data key="d6">The LM &amp; Math model achieves better performance metrics on the LiveBench-Instruction benchmark, showcasing its capabilities in instruction tasks.</data>
  <data key="d7">benchmark performance, model effectiveness</data>
  <data key="d8">chunk-070652f4f36e86620289fcafaaa29599</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="LiveBench-Coding" target="Accuracy Improvement">
  <data key="d5">9.0</data>
  <data key="d6">The accuracy improvement of models is measured during the LiveBench-Coding event, highlighting their effectiveness in coding tasks.</data>
  <data key="d7">event evaluation, performance metrics</data>
  <data key="d8">chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="LiveBench-Coding" target="LM &amp; Code">
  <data key="d5">16.0</data>
  <data key="d6">The LM &amp; Code model demonstrates enhanced performance on the LiveBench-Coding benchmark, reflecting its coding capabilities.</data>
  <data key="d7">benchmark performance, model effectiveness</data>
  <data key="d8">chunk-070652f4f36e86620289fcafaaa29599</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="LiveBench-TypoFixing" target="Accuracy Improvement">
  <data key="d5">8.0</data>
  <data key="d6">The results from LiveBench-TypoFixing provide insights into the accuracy improvements of models in typo correction tasks.</data>
  <data key="d7">event evaluation, performance metrics</data>
  <data key="d8">chunk-17c651f0ad506eb37b0f1742513e2650</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="LiveBench-TypoFixing" target="Math &amp; Code">
  <data key="d5">16.0</data>
  <data key="d6">The Math &amp; Code model shows improved accuracy on the LiveBench-TypoFixing benchmark, indicating its effectiveness in typo fixing tasks.</data>
  <data key="d7">benchmark performance, model effectiveness</data>
  <data key="d8">chunk-070652f4f36e86620289fcafaaa29599</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="OOD Datasets" target="Model Robustness">
  <data key="d5">9.0</data>
  <data key="d6">Evaluating models on OOD datasets is essential for assessing their robustness and generalization capabilities.</data>
  <data key="d7">robustness evaluation, generalization</data>
  <data key="d8">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Model Robustness" target="Merging Ratio">
  <data key="d5">8.0</data>
  <data key="d6">The merging ratio impacts the robustness of the model, influencing how well it performs on out-of-distribution data.</data>
  <data key="d7">performance factors, model evaluation</data>
  <data key="d8">chunk-a1a3eb87bde3ce7ebd6a6c7404f447ff</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Math &amp; Code" target="Performance Drop Rate (PDR)">
  <data key="d5">8.0</data>
  <data key="d6">PDR is a metric used to evaluate the robustness of the Math &amp; Code model under adversarial conditions.</data>
  <data key="d7">metric evaluation, model robustness</data>
  <data key="d8">chunk-070652f4f36e86620289fcafaaa29599</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="LM &amp; Math" target="Performance Drop Rate (PDR)">
  <data key="d5">8.0</data>
  <data key="d6">PDR is a metric used to assess the robustness of the LM &amp; Math model when subjected to adversarial attacks.</data>
  <data key="d7">metric evaluation, model robustness</data>
  <data key="d8">chunk-070652f4f36e86620289fcafaaa29599</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="LM &amp; Code" target="Performance Drop Rate (PDR)">
  <data key="d5">8.0</data>
  <data key="d6">PDR is a critical measure for evaluating the adversarial robustness of the LM &amp; Code model.</data>
  <data key="d7">metric evaluation, model robustness</data>
  <data key="d8">chunk-070652f4f36e86620289fcafaaa29599</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Performance Drop Rate (PDR)" target="Metric_no attack">
  <data key="d5">8.0</data>
  <data key="d6">Metric_no attack is used in the calculation of Performance Drop Rate (PDR) to evaluate model performance without attacks.</data>
  <data key="d7">performance evaluation, metric calculation</data>
  <data key="d8">chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Performance Drop Rate (PDR)" target="Metric_attack">
  <data key="d5">8.0</data>
  <data key="d6">Metric_attack is considered in the calculation of Performance Drop Rate (PDR) to assess the impact of attacks on model performance.</data>
  <data key="d7">performance evaluation, metric calculation</data>
  <data key="d8">chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Performance Drop Rate (PDR)" target="adversarial robustness">
  <data key="d5">9.0</data>
  <data key="d6">Adversarial robustness is assessed through the Performance Drop Rate (PDR), indicating how well a model withstands attacks.</data>
  <data key="d7">robustness assessment, adversarial evaluation</data>
  <data key="d8">chunk-568ea9e09f85a7b95cfe61d48b0ba893</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Adversarial Robustness" target="Evaluation">
  <data key="d5">9.0</data>
  <data key="d6">Adversarial Robustness is evaluated through systematic assessments that determine how well a model performs under adversarial conditions.</data>
  <data key="d7">model evaluation, robustness</data>
  <data key="d8">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Jacob Austin" target="Augustus Odena">
  <data key="d5">6.0</data>
  <data key="d6">Jacob Austin and Augustus Odena are co-authors of a paper on program synthesis with large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Augustus Odena" target="Maxwell Nye">
  <data key="d5">6.0</data>
  <data key="d6">Augustus Odena and Maxwell Nye are co-authors of a paper on program synthesis with large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Maxwell Nye" target="Maarten Bosma">
  <data key="d5">6.0</data>
  <data key="d6">Maxwell Nye and Maarten Bosma are co-authors of a paper on program synthesis with large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Maarten Bosma" target="Henryk Michalewski">
  <data key="d5">6.0</data>
  <data key="d6">Maarten Bosma and Henryk Michalewski are co-authors of a paper on program synthesis with large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Henryk Michalewski" target="David Dohan">
  <data key="d5">6.0</data>
  <data key="d6">Henryk Michalewski and David Dohan are co-authors of a paper on program synthesis with large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="David Dohan" target="Ellen Jiang">
  <data key="d5">6.0</data>
  <data key="d6">David Dohan and Ellen Jiang are co-authors of a paper on program synthesis with large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Ellen Jiang" target="Carrie Cai">
  <data key="d5">6.0</data>
  <data key="d6">Ellen Jiang and Carrie Cai are co-authors of a paper on program synthesis with large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Carrie Cai" target="Michael Terry">
  <data key="d5">6.0</data>
  <data key="d6">Carrie Cai and Michael Terry are co-authors of a paper on program synthesis with large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Michael Terry" target="Quoc Le">
  <data key="d5">6.0</data>
  <data key="d6">Michael Terry and Quoc Le are co-authors of a paper on program synthesis with large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Yupeng Chang" target="Xu Wang">
  <data key="d5">6.0</data>
  <data key="d6">Yupeng Chang and Xu Wang are co-authors of a paper on evaluation of large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Edward Beeching" target="Clmentine Fourrier">
  <data key="d5">6.0</data>
  <data key="d6">Edward Beeching and Clmentine Fourrier are co-authors of a paper on the Open LLM leaderboard.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Clmentine Fourrier" target="Nathan Habib">
  <data key="d5">6.0</data>
  <data key="d6">Clmentine Fourrier and Nathan Habib are co-authors of a paper on the Open LLM leaderboard.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Nathan Habib" target="Sheon Han">
  <data key="d5">6.0</data>
  <data key="d6">Nathan Habib and Sheon Han are co-authors of a paper on the Open LLM leaderboard.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Sheon Han" target="Nathan Lambert">
  <data key="d5">6.0</data>
  <data key="d6">Sheon Han and Nathan Lambert are co-authors of a paper on the Open LLM leaderboard.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Nathan Lambert" target="Nazneen Rajani">
  <data key="d5">6.0</data>
  <data key="d6">Nathan Lambert and Nazneen Rajani are co-authors of a paper on the Open LLM leaderboard.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Nazneen Rajani" target="Omar Sanseviero">
  <data key="d5">6.0</data>
  <data key="d6">Nazneen Rajani and Omar Sanseviero are co-authors of a paper on the Open LLM leaderboard.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Omar Sanseviero" target="Lewis Tunstall">
  <data key="d5">6.0</data>
  <data key="d6">Omar Sanseviero and Lewis Tunstall are co-authors of a paper on the Open LLM leaderboard.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Lewis Tunstall" target="Thomas Wolf">
  <data key="d5">6.0</data>
  <data key="d6">Lewis Tunstall and Thomas Wolf are co-authors of a paper on the Open LLM leaderboard.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Tom Brown" target="Benjamin Mann&quot;&lt;||&gt;&quot;Tom Brown and Benjamin Mann are co-authors of a paper discussing language models as few-shot learners.">
  <data key="d5">6.0</data>
  <data key="d6">co-authorship, research</data>
  <data key="d7">6</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Benjamin Mann" target="Nick Ryder">
  <data key="d5">6.0</data>
  <data key="d6">Benjamin Mann and Nick Ryder are co-authors of a paper discussing language models as few-shot learners.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Nick Ryder" target="Melanie Subbiah">
  <data key="d5">6.0</data>
  <data key="d6">Nick Ryder and Melanie Subbiah are co-authors of a paper discussing language models as few-shot learners.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Melanie Subbiah" target="Jared D Kaplan&quot;&lt;||&quot;Melanie Subbiah and Jared D Kaplan are co-authors of a paper discussing language models as few-shot learners.">
  <data key="d5">6.0</data>
  <data key="d6">co-authorship, research</data>
  <data key="d7">6</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Jared D Kaplan" target="Prafulla Dhariwal">
  <data key="d5">6.0</data>
  <data key="d6">Jared D Kaplan and Prafulla Dhariwal are co-authors of a paper discussing language models as few-shot learners.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Prafulla Dhariwal" target="Arvind Neelakantan">
  <data key="d5">6.0</data>
  <data key="d6">Prafulla Dhariwal and Arvind Neelakantan are co-authors of a paper discussing language models as few-shot learners.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Arvind Neelakantan" target="Pranav Shyam">
  <data key="d5">6.0</data>
  <data key="d6">Arvind Neelakantan and Pranav Shyam are co-authors of a paper discussing language models as few-shot learners.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Pranav Shyam" target="Girish Sastry">
  <data key="d5">6.0</data>
  <data key="d6">Pranav Shyam and Girish Sastry are co-authors of a paper discussing language models as few-shot learners.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Girish Sastry" target="Amanda Askell">
  <data key="d5">6.0</data>
  <data key="d6">Girish Sastry and Amanda Askell are co-authors of a paper discussing language models as few-shot learners.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Xu Wang" target="Jindong Wang">
  <data key="d5">6.0</data>
  <data key="d6">Xu Wang and Jindong Wang are co-authors of a paper on evaluation of large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Jindong Wang" target="Linyi Yang">
  <data key="d5">6.0</data>
  <data key="d6">Jindong Wang and Linyi Yang are co-authors of a paper on evaluation of large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Linyi Yang" target="Kaijie Zhu">
  <data key="d5">6.0</data>
  <data key="d6">Linyi Yang and Kaijie Zhu are co-authors of a paper on evaluation of large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Kaijie Zhu" target="Hao Chen">
  <data key="d5">6.0</data>
  <data key="d6">Kaijie Zhu and Hao Chen are co-authors of a paper on evaluation of large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Hao Chen" target="Xiaoyuan Yi">
  <data key="d5">6.0</data>
  <data key="d6">Hao Chen and Xiaoyuan Yi are co-authors of a paper on evaluation of large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Xiaoyuan Yi" target="Cunxiang Wang">
  <data key="d5">6.0</data>
  <data key="d6">Xiaoyuan Yi and Cunxiang Wang are co-authors of a paper on evaluation of large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Cunxiang Wang" target="Yidong Wang">
  <data key="d5">6.0</data>
  <data key="d6">Cunxiang Wang and Yidong Wang are co-authors of a paper on evaluation of large language models.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Karl Cobbe" target="Vineet Kosaraju">
  <data key="d5">6.0</data>
  <data key="d6">Karl Cobbe and Vineet Kosaraju are co-authors of a paper on training verifiers for math word problems.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Vineet Kosaraju" target="Mohammad Bavarian">
  <data key="d5">6.0</data>
  <data key="d6">Vineet Kosaraju and Mohammad Bavarian are co-authors of a paper on training verifiers for math word problems.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Mohammad Bavarian" target="Mark Chen">
  <data key="d5">6.0</data>
  <data key="d6">Mohammad Bavarian and Mark Chen are co-authors of a paper on training verifiers for math word problems.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Mark Chen" target="Heewoo Jun">
  <data key="d5">6.0</data>
  <data key="d6">Mark Chen and Heewoo Jun are co-authors of a paper on training verifiers for math word problems.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Heewoo Jun" target="Lukasz Kaiser">
  <data key="d5">6.0</data>
  <data key="d6">Heewoo Jun and Lukasz Kaiser are co-authors of a paper on training verifiers for math word problems.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Lukasz Kaiser" target="Matthias Plappert">
  <data key="d5">6.0</data>
  <data key="d6">Lukasz Kaiser and Matthias Plappert are co-authors of a paper on training verifiers for math word problems.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Matthias Plappert" target="Jerry Tworek">
  <data key="d5">6.0</data>
  <data key="d6">Matthias Plappert and Jerry Tworek are co-authors of a paper on training verifiers for math word problems.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Jerry Tworek" target="Jacob Hilton">
  <data key="d5">6.0</data>
  <data key="d6">Jerry Tworek and Jacob Hilton are co-authors of a paper on training verifiers for math word problems.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Jacob Hilton" target="Reiichiro Nakano">
  <data key="d5">6.0</data>
  <data key="d6">Jacob Hilton and Reiichiro Nakano are co-authors of a paper on training verifiers for math word problems.</data>
  <data key="d7">co-authorship, research</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Ronald A Fisher" target="Philosophical Transactions of the Royal">
  <data key="d5">17.0</data>
  <data key="d6">Ronald A Fisher's work on theoretical statistics was published in Philosophical Transactions of the Royal, establishing foundational concepts in statistics.&lt;SEP&gt;Ronald A Fisher's work on theoretical statistics was published in Philosophical Transactions of the Royal.</data>
  <data key="d7">publication, historical significance</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Ronald A Fisher" target="Philosophical Transactions of the Royal Society of London">
  <data key="d5">8.0</data>
  <data key="d6">Ronald A Fisher's work was published in the Philosophical Transactions, showcasing his contributions to statistics.</data>
  <data key="d7">publication, academic contribution</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Reinforcement Learning with Human Feedback" target="arXiv preprint arXiv:2403.13187">
  <data key="d5">8.0</data>
  <data key="d6">The paper discusses the application of Reinforcement Learning with Human Feedback in optimizing model merging recipes.</data>
  <data key="d7">research methodology, model optimization</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="arXiv preprint arXiv:2403.13187" target="UCI Machine Learning Repository">
  <data key="d5">7.0</data>
  <data key="d6">The paper utilizes datasets from the UCI Machine Learning Repository for its experiments and evaluations.</data>
  <data key="d7">data source, research application</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="arXiv preprint arXiv:2403.13187" target="ACM Transactions on Intelligent Systems and Technology">
  <data key="d5">6.0</data>
  <data key="d6">The research on model merging recipes contributes to the body of knowledge published in ACM Transactions on Intelligent Systems and Technology.</data>
  <data key="d7">publication, research contribution</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="UCI Machine Learning Repository" target="arXiv preprint arXiv:2108.07732">
  <data key="d5">7.0</data>
  <data key="d6">The program synthesis research utilizes datasets from the UCI Machine Learning Repository for its evaluations.</data>
  <data key="d7">data source, research application</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="arXiv preprint arXiv:2408.04556" target="Nature Machine Intelligence">
  <data key="d5">8.0</data>
  <data key="d6">The research on bias-alleviating low-rank adaptation is published in Nature Machine Intelligence, contributing to the field of AI ethics and performance.</data>
  <data key="d7">publication, AI ethics</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="arXiv preprint arXiv:2408.04556" target="Code Alpaca">
  <data key="d5">7.0</data>
  <data key="d6">The instruction-following LLaMA model, Code Alpaca, is related to advancements in language models discussed in the paper on bias-alleviating techniques.</data>
  <data key="d7">model development, research application</data>
  <data key="d8">chunk-11849aa770ab48fe9e288c6cec166b28</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="arXiv preprint arXiv:2404.04475" target="IEEE Security and Privacy Workshops (SPW)">
  <data key="d5">7.0</data>
  <data key="d6">The arXiv preprint discusses topics relevant to security and privacy, which are central themes at the IEEE SPW.</data>
  <data key="d7">research relevance, thematic connection</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="arXiv preprint arXiv:2404.04475" target="Lora">
  <data key="d5">9.0</data>
  <data key="d6">Lora's methodology for adapting language models is discussed as part of advancements in LLM fine-tuning in the arXiv preprint.</data>
  <data key="d7">methodology, model adaptation</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="arXiv preprint arXiv:2404.04475" target="Math Dataset">
  <data key="d5">8.0</data>
  <data key="d6">The Math Dataset is referenced in the context of evaluating mathematical problem-solving capabilities of language models.</data>
  <data key="d7">evaluation, problem-solving</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="arXiv preprint arXiv:2404.04475" target="Wizardmath">
  <data key="d5">9.0</data>
  <data key="d6">Wizardmath's focus on enhancing mathematical reasoning aligns with the advancements discussed in the arXiv preprint.</data>
  <data key="d7">research alignment, reasoning enhancement</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Ji Gao" target="Jack Lanchantin">
  <data key="d5">8.0</data>
  <data key="d6">Ji Gao and Jack Lanchantin collaborated on research involving adversarial text generation techniques for deep learning classifiers.</data>
  <data key="d7">collaboration, research focus</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Jack Lanchantin" target="Mary Lou Soffa">
  <data key="d5">8.0</data>
  <data key="d6">Jack Lanchantin and Mary Lou Soffa have worked together on adversarial techniques in machine learning, focusing on text generation.</data>
  <data key="d7">collaboration, adversarial techniques</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Mary Lou Soffa" target="Yanjun Qi">
  <data key="d5">7.0</data>
  <data key="d6">Mary Lou Soffa and Yanjun Qi are both involved in research related to adversarial machine learning, contributing to similar projects.</data>
  <data key="d7">research alignment, adversarial learning</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Chenlu Guo" target="Nuo Xu">
  <data key="d5">9.0</data>
  <data key="d6">Chenlu Guo and Nuo Xu collaborated on the development of a dataset for evaluating health in large language models.</data>
  <data key="d7">dataset development, collaboration</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Dan Hendrycks" target="Collin Burns">
  <data key="d5">9.0</data>
  <data key="d6">Dan Hendrycks and Collin Burns collaborated on the Math Dataset, which evaluates mathematical problem-solving abilities in machine learning models.</data>
  <data key="d7">dataset collaboration, evaluation</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Collin Burns" target="Saurav Kadavath">
  <data key="d5">8.0</data>
  <data key="d6">Collin Burns and Saurav Kadavath worked together on projects related to evaluating mathematical capabilities in machine learning models.</data>
  <data key="d7">collaboration, evaluation focus</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Saurav Kadavath" target="Akul Arora">
  <data key="d5">8.0</data>
  <data key="d6">Saurav Kadavath and Akul Arora collaborated on evaluating machine learning models, particularly in mathematical contexts.</data>
  <data key="d7">research collaboration, evaluation focus</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Akul Arora" target="Steven Basart">
  <data key="d5">7.0</data>
  <data key="d6">Akul Arora and Steven Basart are involved in similar research projects focused on evaluating mathematical problem-solving in machine learning.</data>
  <data key="d7">collaboration, evaluation focus</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Steven Basart" target="Eric Tang&quot;&lt;||&quot;Steven Basart and Eric Tang have worked together on projects evaluating machine learning models, particularly in mathematics.">
  <data key="d5">7.0</data>
  <data key="d6">research collaboration, evaluation focus</data>
  <data key="d7">7</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Dawn Song" target="Jacob Steinhardt&quot;&lt;||&quot;Dawn Song and Jacob Steinhardt are both prominent researchers in machine learning, collaborating on evaluation methodologies.">
  <data key="d5">8.0</data>
  <data key="d6">collaboration, research focus</data>
  <data key="d7">8</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Edward J Hu" target="Yelong Shen">
  <data key="d5">9.0</data>
  <data key="d6">Edward J Hu and Yelong Shen are involved in research focused on low-rank adaptation techniques for language models.</data>
  <data key="d7">research collaboration, model adaptation</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Phillip Wallis" target="Zeyuan Allen-Zhu">
  <data key="d5">8.0</data>
  <data key="d6">Phillip Wallis and Zeyuan Allen-Zhu have collaborated on developing techniques for adapting large language models.</data>
  <data key="d7">collaboration, model adaptation</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Yuanzhi Li" target="Shean Wang">
  <data key="d5">7.0</data>
  <data key="d6">Yuanzhi Li and Shean Wang are both focused on enhancing the performance of language models through adaptation techniques.</data>
  <data key="d7">research alignment, performance enhancement</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Lu Wang" target="Weizhu Chen">
  <data key="d5">8.0</data>
  <data key="d6">Lu Wang and Weizhu Chen are involved in similar research areas, focusing on adapting large language models for improved efficiency.</data>
  <data key="d7">research collaboration, model adaptation</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Wenxuan Wang" target="Jen-tse Huang">
  <data key="d5">8.0</data>
  <data key="d6">Wenxuan Wang and Jen-tse Huang have collaborated on evaluating language models, particularly in translation contexts.</data>
  <data key="d7">collaboration, evaluation focus</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Xing Wang" target="Zhaopeng Tu">
  <data key="d5">7.0</data>
  <data key="d6">Xing Wang and Zhaopeng Tu are both focused on evaluating machine learning models, particularly in translation tasks.</data>
  <data key="d7">research collaboration, evaluation focus</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Xisen Jin" target="Xiang Ren">
  <data key="d5">8.0</data>
  <data key="d6">Xisen Jin and Xiang Ren are researchers collaborating on knowledge fusion techniques in language models.</data>
  <data key="d7">research collaboration, knowledge integration</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Daniel Preotiuc-Pietro" target="Pengxiang Cheng">
  <data key="d5">9.0</data>
  <data key="d6">Daniel Preotiuc-Pietro and Pengxiang Cheng are involved in developing techniques for knowledge integration in language models.</data>
  <data key="d7">research collaboration, knowledge integration</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Alex Krizhevsky" target="Geoffrey Hinton">
  <data key="d5">9.0</data>
  <data key="d6">Alex Krizhevsky and Geoffrey Hinton are both prominent figures in deep learning, collaborating on various research projects.</data>
  <data key="d7">collaboration, deep learning</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Xuechen Li" target="Tianyi Zhang">
  <data key="d5">8.0</data>
  <data key="d6">Xuechen Li and Tianyi Zhang are involved in evaluating instruction-following models, contributing to similar research areas.</data>
  <data key="d7">research collaboration, evaluation focus</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Yann Dubois" target="Rohan Taori">
  <data key="d5">7.0</data>
  <data key="d6">Yann Dubois and Rohan Taori are both focused on evaluating machine learning models, particularly in instruction-following contexts.</data>
  <data key="d7">research collaboration, evaluation focus</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Ishaan Gulrajani" target="Carlos Guestrin">
  <data key="d5">8.0</data>
  <data key="d6">Ishaan Gulrajani and Carlos Guestrin are involved in research on machine learning evaluation techniques, contributing to similar projects.</data>
  <data key="d7">research collaboration, evaluation focus</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Percy Liang" target="Tatsunori B Hashimoto">
  <data key="d5">9.0</data>
  <data key="d6">Percy Liang and Tatsunori B Hashimoto are both researchers focused on evaluating instruction-following models and their applications.</data>
  <data key="d7">research collaboration, evaluation focus</data>
  <data key="d8">chunk-f4bbff6c3c7fd885c34be5b4f68f6a36</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Norman Sadeh" target="Stress test evaluation for natural language inference">
  <data key="d5">16.0</data>
  <data key="d6">Norman Sadeh contributed to the research on stress test evaluation in natural language inference, indicating his involvement in the study.</data>
  <data key="d7">research contribution, academic collaboration</data>
  <data key="d8">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Carolyn Rose" target="Stress test evaluation for natural language inference">
  <data key="d5">16.0</data>
  <data key="d6">Carolyn Rose co-authored the research on stress test evaluation for natural language inference, showcasing her collaboration with Norman Sadeh.</data>
  <data key="d7">research contribution, academic collaboration</data>
  <data key="d8">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Graham Neubig" target="Stress test evaluation for natural language inference">
  <data key="d5">16.0</data>
  <data key="d6">Graham Neubig is a co-author of the study on stress test evaluation for natural language inference, indicating his expertise in the field.</data>
  <data key="d7">research contribution, academic collaboration</data>
  <data key="d8">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Daye Nam" target="Using an llm to help with code understanding">
  <data key="d5">18.0</data>
  <data key="d6">Daye Nam is a contributor to research that explores the use of large language models for code understanding, indicating his role in software engineering research.</data>
  <data key="d7">research contribution, software engineering</data>
  <data key="d8">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Andrew Macvean" target="Using an llm to help with code understanding">
  <data key="d5">18.0</data>
  <data key="d6">Andrew Macvean co-authored the research on using language models for code understanding, reflecting his involvement in the project.</data>
  <data key="d7">research contribution, software engineering</data>
  <data key="d8">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Vincent Hellendoorn" target="Using an llm to help with code understanding">
  <data key="d5">18.0</data>
  <data key="d6">Vincent Hellendoorn's contribution to the research indicates his focus on enhancing code comprehension using AI tools.</data>
  <data key="d7">research contribution, software engineering</data>
  <data key="d8">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Bogdan Vasilescu" target="Using an llm to help with code understanding">
  <data key="d5">18.0</data>
  <data key="d6">Bogdan Vasilescu's involvement in the research highlights his expertise in software engineering and AI applications.</data>
  <data key="d7">research contribution, software engineering</data>
  <data key="d8">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Brad Myers" target="Using an llm to help with code understanding">
  <data key="d5">18.0</data>
  <data key="d6">Brad Myers' contribution to the research indicates his commitment to advancing software engineering methodologies using AI.</data>
  <data key="d7">research contribution, software engineering</data>
  <data key="d8">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="R OpenAI" target="GPT-4 Technical Report">
  <data key="d5">20.0</data>
  <data key="d6">R OpenAI published the GPT-4 Technical Report, detailing their advancements in AI language models.</data>
  <data key="d7">organization output, AI development</data>
  <data key="d8">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Stanford Alpaca" target="Instruction-following llama model">
  <data key="d5">18.0</data>
  <data key="d6">Stanford Alpaca is a project that focuses on instruction-following models, showcasing advancements in AI technology.</data>
  <data key="d7">AI research, model development</data>
  <data key="d8">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Stanford Alpaca" target="Llama-2-13b-code-alpaca">
  <data key="d5">14.0</data>
  <data key="d6">Llama-2-13b-code-alpaca is fine-tuned from Stanford Alpaca, focusing on code-related tasks.&lt;SEP&gt;Llama-2-13b-code-alpaca is fine-tuned from Stanford Alpaca, focusing on code-related tasks.".</data>
  <data key="d7">model development, fine-tuning</data>
  <data key="d8">chunk-9fbd0c0622e4d5fb84be83781e1c4521</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Imagenet Challenge" target="Large scale visual recognition challenge">
  <data key="d5">18.0</data>
  <data key="d6">The Imagenet Challenge assesses large-scale visual recognition capabilities, indicating its significance in computer vision research.</data>
  <data key="d7">competition, visual recognition</data>
  <data key="d8">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Speech Commands Dataset" target="Launching the speech commands dataset">
  <data key="d5">18.0</data>
  <data key="d6">The Speech Commands Dataset was launched by R OpenAI, indicating their role in advancing speech recognition technology.</data>
  <data key="d7">dataset launch, AI research</data>
  <data key="d8">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Chain-of-Thought Prompting" target="Chain-of-thought prompting elicits reasoning in large language models">
  <data key="d5">20.0</data>
  <data key="d6">Chain-of-Thought Prompting is a technique used to enhance reasoning in AI, indicating its significance in model training.</data>
  <data key="d7">AI training technique, reasoning enhancement</data>
  <data key="d8">chunk-514284ab5149fbee36b23182dfc87ed2</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="arXiv preprint arXiv:2410.09335" target="Rethinking data selection at scale">
  <data key="d5">8.0</data>
  <data key="d6">The preprint discusses data selection, a critical aspect of machine learning, reflecting ongoing research in the field.</data>
  <data key="d7">research dissemination, data selection</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="ACM Transactions on Management Information Systems" target="Designing heterogeneous llm agents for financial sentiment analysis">
  <data key="d5">9.0</data>
  <data key="d6">The research published in this journal includes advancements in designing agents for financial analysis.</data>
  <data key="d7">research publication, financial analysis</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="The Twelfth International Conference on Learning Representations" target="Wizardlm: Empowering large pre-trained language models">
  <data key="d5">9.0</data>
  <data key="d6">This conference features research on empowering language models, including the Wizardlm project.</data>
  <data key="d7">conference presentation, language models</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="arXiv preprint arXiv:2306.01708" target="Resolving interference when merging models">
  <data key="d5">8.0</data>
  <data key="d6">The preprint addresses challenges in model merging, relevant to the ongoing research in the field.</data>
  <data key="d7">research dissemination, model merging</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="arXiv preprint arXiv:2408.07666" target="Model merging in llms, mllms, and beyond">
  <data key="d5">8.0</data>
  <data key="d6">This preprint discusses model merging techniques, contributing to the understanding of large language models.</data>
  <data key="d7">research dissemination, model merging</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Forty-first International Conference on Machine Learning" target="Language models are super mario">
  <data key="d5">9.0</data>
  <data key="d6">The conference presents research on innovative techniques for language models, including the Super Mario analogy.</data>
  <data key="d7">conference presentation, language models</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Tingyu Xia" target="Rethinking data selection at scale">
  <data key="d5">9.0</data>
  <data key="d6">Tingyu Xia is a co-author of the paper discussing effective data selection methods in machine learning.</data>
  <data key="d7">research contribution, data selection</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Bowen Yu" target="Rethinking data selection at scale">
  <data key="d5">8.0</data>
  <data key="d6">Bowen Yu collaborates on the research paper focusing on data selection techniques in machine learning.</data>
  <data key="d7">research collaboration, data selection</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Kai Dang" target="Rethinking data selection at scale">
  <data key="d5">8.0</data>
  <data key="d6">Kai Dang is a co-author contributing to the study of data selection methods in machine learning.</data>
  <data key="d7">research collaboration, data selection</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="An Yang" target="Rethinking data selection at scale">
  <data key="d5">8.0</data>
  <data key="d6">An Yang contributes to the research discussing random selection in machine learning.</data>
  <data key="d7">research collaboration, data selection</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Yuan Tian" target="Rethinking data selection at scale">
  <data key="d5">8.0</data>
  <data key="d6">Yuan Tian is a co-author of the paper focusing on data selection techniques in machine learning.</data>
  <data key="d7">research collaboration, data selection</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Junyang Lin" target="Rethinking data selection at scale">
  <data key="d5">8.0</data>
  <data key="d6">Junyang Lin is a co-author of the research discussing effective data selection in machine learning.</data>
  <data key="d7">research collaboration, data selection</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Frank Xing" target="Designing heterogeneous llm agents for financial sentiment analysis">
  <data key="d5">9.0</data>
  <data key="d6">Frank Xing authored a paper discussing the design of language model agents for financial analysis.</data>
  <data key="d7">research contribution, financial sentiment analysis</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Can Xu" target="Wizardlm: Empowering large pre-trained language models">
  <data key="d5">9.0</data>
  <data key="d6">Can Xu is a co-author of the research on enhancing large language models for complex tasks.</data>
  <data key="d7">research collaboration, language models</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Qingfeng Sun" target="Wizardlm: Empowering large pre-trained language models">
  <data key="d5">9.0</data>
  <data key="d6">Qingfeng Sun collaborates on the research discussing large pre-trained language models.</data>
  <data key="d7">research collaboration, language models</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Kai Zheng" target="Wizardlm: Empowering large pre-trained language models">
  <data key="d5">9.0</data>
  <data key="d6">Kai Zheng is a co-author of the paper on empowering large language models for complex instructions.</data>
  <data key="d7">research collaboration, language models</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Xiubo Geng" target="Wizardlm: Empowering large pre-trained language models">
  <data key="d5">9.0</data>
  <data key="d6">Xiubo Geng contributes to the research on enhancing language models for instruction following tasks.</data>
  <data key="d7">research collaboration, language models</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Pu Zhao" target="Wizardlm: Empowering large pre-trained language models">
  <data key="d5">9.0</data>
  <data key="d6">Pu Zhao is involved in the research discussing advancements in large language models.</data>
  <data key="d7">research collaboration, language models</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Jiazhan Feng" target="Wizardlm: Empowering large pre-trained language models">
  <data key="d5">9.0</data>
  <data key="d6">Jiazhan Feng co-authors the research on empowering language models for complex tasks.</data>
  <data key="d7">research collaboration, language models</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Chongyang Tao" target="Wizardlm: Empowering large pre-trained language models">
  <data key="d5">9.0</data>
  <data key="d6">Chongyang Tao is a contributor to the research on enhancing language models for instruction following tasks.</data>
  <data key="d7">research collaboration, language models</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Qingwei Lin" target="Wizardlm: Empowering large pre-trained language models">
  <data key="d5">9.0</data>
  <data key="d6">Qingwei Lin collaborates on the research discussing large language models and their capabilities.</data>
  <data key="d7">research collaboration, language models</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Daxin Jiang" target="Wizardlm: Empowering large pre-trained language models">
  <data key="d5">9.0</data>
  <data key="d6">Daxin Jiang is involved in the study of empowering language models for better performance.</data>
  <data key="d7">research collaboration, language models</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Proceedings of the 1st ACM Workshop on Large AI Systems and Models with Privacy and Safety Analysis" target="large AI systems">
  <data key="d5">8.0</data>
  <data key="d6">This event focuses on the implications of large AI systems, discussing privacy and safety analysis.</data>
  <data key="d7">event focus, AI systems</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Journal of Machine Learning Research" target="Designing heterogeneous llm agents for financial sentiment analysis">
  <data key="d5">9.0</data>
  <data key="d6">The journal publishes significant research findings, including those on financial sentiment analysis agents.</data>
  <data key="d7">research publication, financial analysis</data>
  <data key="d8">chunk-6d1eea6873850c90c145f9d396d917bc</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="WizardMath-Mistral 7B" target="WizardMath 70B">
  <data key="d5">16.0</data>
  <data key="d6">Both WizardMath-Mistral 7B and WizardMath 70B are mathematical models evaluated for their performance in solving math problems.&lt;SEP&gt;Both WizardMath-Mistral 7B and WizardMath 70B are mathematical models evaluated for their performance in solving math problems.".</data>
  <data key="d7">model comparison, performance evaluation</data>
  <data key="d8">chunk-9fbd0c0622e4d5fb84be83781e1c4521</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="WizardMath 70B" target="GPT-3.5-Turbo">
  <data key="d5">18.0</data>
  <data key="d6">WizardMath 70B is compared to GPT-3.5-Turbo in terms of mathematical reasoning capabilities.&lt;SEP&gt;WizardMath 70B is compared to GPT-3.5-Turbo in terms of mathematical reasoning capabilities.".</data>
  <data key="d7">model comparison, reasoning tasks</data>
  <data key="d8">chunk-9fbd0c0622e4d5fb84be83781e1c4521</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="WizardMath 70B" target="Claude 2">
  <data key="d5">16.0</data>
  <data key="d6">WizardMath 70B is evaluated alongside Claude 2 for mathematical reasoning tasks.&lt;SEP&gt;WizardMath 70B is evaluated alongside Claude 2 for mathematical reasoning tasks.".</data>
  <data key="d7">model comparison, evaluation</data>
  <data key="d8">chunk-9fbd0c0622e4d5fb84be83781e1c4521</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Task Vectors" target="Merged Task Vector">
  <data key="d5">8.0</data>
  <data key="d6">Merged Task Vector is derived from the combination of two or more Task Vectors, enhancing the model's ability to learn from data.</data>
  <data key="d7">model enhancement, data representation</data>
  <data key="d8">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Parameter Value" target="Model Performance Metrics">
  <data key="d5">8.0</data>
  <data key="d6">Parameter Value settings influence Model Performance Metrics, affecting how well a model performs on various tasks.</data>
  <data key="d7">model configuration, performance</data>
  <data key="d8">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Extraneous Information" target="Model Performance Metrics">
  <data key="d5">7.0</data>
  <data key="d6">Extraneous Information can negatively impact Model Performance Metrics by introducing noise and confusion during evaluation.</data>
  <data key="d7">data quality, evaluation impact</data>
  <data key="d8">chunk-407b385e429f3495cdcf3f23f7e80f73</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Marthe Ballon" target="Andres Algaba">
  <data key="d5">12.0</data>
  <data key="d6">Marthe Ballon and Andres Algaba are both researchers contributing to the study of language models, affiliated with the same institutions.</data>
  <data key="d7">collaboration, research affiliation</data>
  <data key="d8">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Marthe Ballon" target="Vincent Ginis">
  <data key="d5">12.0</data>
  <data key="d6">Marthe Ballon and Vincent Ginis work together in the Data Analytics Lab, focusing on language model research.</data>
  <data key="d7">collaboration, research affiliation</data>
  <data key="d8">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Marthe Ballon" target="Data Analytics Lab">
  <data key="d5">8.0</data>
  <data key="d6">Marthe Ballon conducts research at the Data Analytics Lab, contributing to advancements in language models.</data>
  <data key="d7">research, institutional affiliation</data>
  <data key="d8">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Marthe Ballon" target="Vrije Universiteit Brussel">
  <data key="d5">9.0</data>
  <data key="d6">Marthe Ballon is affiliated with Vrije Universiteit Brussel, where she conducts her research.</data>
  <data key="d7">affiliation, academic research</data>
  <data key="d8">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Andres Algaba" target="Vincent Ginis">
  <data key="d5">12.0</data>
  <data key="d6">Andres Algaba and Vincent Ginis are researchers at different institutions but collaborate on language model studies.</data>
  <data key="d7">collaboration, research affiliation</data>
  <data key="d8">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Andres Algaba" target="Data Analytics Lab">
  <data key="d5">8.0</data>
  <data key="d6">Andres Algaba is associated with the Data Analytics Lab, focusing on machine learning and language models.</data>
  <data key="d7">research, institutional affiliation</data>
  <data key="d8">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Andres Algaba" target="Vrije Universiteit Brussel">
  <data key="d5">9.0</data>
  <data key="d6">Andres Algaba is affiliated with Vrije Universiteit Brussel, contributing to research initiatives.</data>
  <data key="d7">affiliation, academic research</data>
  <data key="d8">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Vincent Ginis" target="Data Analytics Lab">
  <data key="d5">8.0</data>
  <data key="d6">Vincent Ginis is part of the Data Analytics Lab, contributing to research on large language models.</data>
  <data key="d7">research, institutional affiliation</data>
  <data key="d8">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
<edge source="Vincent Ginis" target="Harvard University">
  <data key="d5">9.0</data>
  <data key="d6">Vincent Ginis is affiliated with Harvard University, where he collaborates on research related to language models.</data>
  <data key="d7">affiliation, academic research</data>
  <data key="d8">chunk-f21dc353d188ca08cba26298157ff09d</data>
  <data key="d9">Mixup_model_merging.txt</data>
</edge>
</graph></graphml>