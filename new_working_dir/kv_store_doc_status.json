{
  "doc-c84d49bffb4c347d3cb113c469e68d4b": {
    "status": "processed",
    "chunks_count": 33,
    "content": "# Evolutionary Optimization of Model Merging Recipes\n\nTakuya Akiba, Makoto Shing, Yujin Tang, Qi Sun, David Ha Sakana AI Tokyo, Japan {takiba,mkshing,yujintang,qisun,hadavid}@sakana.ai\n\n## Abstract\n\nLarge language models (LLMs) have become increasingly capable, but their development often requires substantial computational resources. While model merging has emerged as a cost-effective promising approach for creating new models by combining existing ones, it currently relies on human intuition and domain knowledge, limiting its potential. Here, we propose an evolutionary approach that overcomes this limitation by automatically discovering effective combinations of diverse open-source models, harnessing their collective intelligence without requiring extensive additional training data or compute. Our approach operates in both parameter space and data flow space, allowing for optimization beyond just the weights of the individual models. This approach even facilitates crossdomain merging, generating models like a Japanese LLM with Math reasoning capabilities. Surprisingly, our Japanese Math LLM achieved state-of-the-art performance on a variety of established Japanese LLM benchmarks, even surpassing models with significantly more parameters, despite not being explicitly trained for such tasks. Furthermore, a culturally-aware Japanese VLM generated through our approach demonstrates its effectiveness in describing Japanese culture-specific content, outperforming previous Japanese VLMs. This work not only contributes new state-of-the-art models back to the open-source community, but also introduces a new paradigm for automated model composition, paving the way for exploring alternative, efficient approaches to foundation model development.\n\n## 1 Introduction\n\nModel merging [\\[21,](#page-18-0) [34\\]](#page-19-0), a recent development in the large language model (LLM) community, presents a novel paradigm shift. By strategically combining multiple LLMs into a single architecture, this exciting development has captured the attention of researchers due to its key advantage: it requires no additional training, making it an incredibly cost-effective approach for developing new models. This accessibility has fueled a surge in interest and experimentation with model merging. The Open LLM Leaderboard [\\[26\\]](#page-19-1) is now dominated by merged models, showcasing its potential for democratizing foundation model development.\n\nHowever, model merging is considered by many to be a form of black art or alchemy, relying on the model maker's intuition and instincts about model selection and merging recipes to create and refine a new model that performs well for a particular task. Furthermore, the model maker is often required to have some domain knowledge for the various different benchmark tasks. Given the large diversity of open models and benchmarks in the community, human intuition can only go so far, and we believe a more systematic approach for discovering new model combinations will take things much further.\n\nWe believe evolutionary algorithms will be able to discover more effective model merging solutions, and thus provide a path for automating the creation of more capable models. As a step towards this direction, in this work, we show that evolution can be employed to discover novel and unintuitive ways to merge multiple models to produce new models with a new combined ability. In this work, we present a methodology that leverages evolutionary algorithms to facilitate the merging of foundation models. Our approach is distinguished by its ability to navigate both parameter space (weights) and the data flow space (inference path), proposing a framework that integrates these two dimensions.\n\nThis work makes several key contributions to the field of foundation model development:\n\n- 1. Automated Model Composition: We introduce Evolutionary Model Merge, a general evolutionary method to automatically discover effective combinations of selected models for creating new foundation models with user-specified capabilities. This approach harnesses the collective intelligence of existing open models, enabling the creation of powerful models without the need for extensive training data or compute.\n- 2. Cross-Domain Merging: We demonstrate that our method can discover novel ways to merge models from disparate domains (e.g., non-English language and Math, non-English language and Vision), potentially exceeding the capabilities achievable through conventional human design strategies.\n- 3. State-of-the-Art Performance: We showcase the effectiveness of our method by automatically generating a Japanese LLM with Math reasoning capability and a Japanese Vision-Language Model (VLM). Notably, both models achieve state-of-the-art performance on various benchmarks, even without explicit optimization for those tasks.\n- 4. High Efficiency and Surprising Generalizability: We observe that our 7B parameter LLM surpasses the performance of some previous 70B parameter Japanese LLMs on benchmark datasets, highlighting the high efficiency and surprising generalization capability of our approach. We believe this model can serve as a strong general-purpose Japanese LLM.\n- 5. Culturally-Aware VLM: The generated Japanese VLM achieves top results when tested on a domestically-sourced dataset of Japanese image-description pairs, demonstrating its ability to handle Japanese culture-specific content.\n\nWe are committed to open science and are excited to open-source our EvoLLM-JP and EvoVLM-JP, two state-of-the-art Japanese foundation models, to the community, enabling further research and development in the field. Our work challenges the conventional paradigm of expensive model development by demonstrating that our evolutionary-based method can produce competitive models without relying on gradient-based training. This paves the way for exploring alternative, potentially more efficient, approaches to foundation model development.\n\n### 1.1 Background and Related Work\n\n### 1.1.1 Overview of Model Merging.\n\nModel merging offers a novel approach to leverage the strengths of multiple pre-trained models. It allows us to combine task-specific models, each potentially fine-tuned for a particular downstream task, into a single unified model. This technique stands in contrast to traditional transfer learning, where a pre-trained model is further fine-tuned for a new task. While transfer learning offers advantages like improved performance and faster convergence, the resulting models are typically limited to single tasks. Model merging, on the other hand, strives to create a versatile and comprehensive model by combining the knowledge from multiple pre-trained models, potentially yielding a model capable of handling various tasks simultaneously.\n\nWhile fine-tuning an existing model involves training an existing model further on a specialized dataset, model merging methods work by combining existing models without any additional gradientbased training, making it very cost-effective in terms of compute requirements. Many in the machine learning community, including researchers, hackers, and hobbyists alike, are using both fine-tuning and model-merging to create the best models for their purposes. In fact, this collaborative way of developing continually improved models has parallels with the traditional open-source software development approach [\\[47\\]](#page-19-2) where the trend is for the community to develop foundational machine learning models like open source software [\\[45,](#page-19-3) [46\\]](#page-19-4).\n\nA simple method of merging multiple models is to average the weights of multiple models finetuned from the same base initial model. This *model soup* approach [\\[61\\]](#page-20-0) demonstrated significant improvements on relatively large image processing and image classification models. The work pointed to similarity between weighted model averaging leading to a flat local minima. Theoretical [\\[17,](#page-18-1) [31,](#page-19-5) [32,](#page-19-6) [44\\]](#page-19-7) and empirical studies dating back from the 1990s [\\[24,](#page-18-2) [25\\]](#page-19-8) demonstrated that flatter local optima generalize better to out-of-distribution shifts. More recent work [\\[14,](#page-18-3) [41\\]](#page-19-9) examined the theoretical properties and practical implications of weight averaging applied to larger generative models.\n\nLinear weight averaging is performed as follows. Let  $\\theta\\_1, \\theta\\_2 \\in \\mathbb{R}^d$  represent the weight vectors of two distinct models, where d is the dimension of the weight space. The merged model's weights, denoted as  $\\theta\\_{new}$ , are computed using the following equation:  $\\theta\\_{new} = \\lambda \\theta\\_1 + (1 - \\lambda)\\theta\\_2$ . Here,  $\\lambda \\in [0, 1]$  is a weighting parameter that determines the relative contribution of each model to the final merged model. By adjusting  $\\lambda$ , practitioners can control the balance between the two original models' characteristics in the resulting merged model.\n\nLinear weight averaging not only works well for image processing and classification models, but is also effective for image generation models such as latent diffusion models. The release of Stable Diffusion [\\[50\\]](#page-20-1) led to a variety of specialist fine-tunes of the model for a variety of different styles, from photorealism, to anime, to realistic painting styles. Popular open-source WebUIs [\\[10\\]](#page-18-4) enabled practitioners to experiment with merging different models using linear or spherical linear interpolation (SLERP) [\\[60\\]](#page-20-2) of weight parameters of distinct fine-tuned Stable Diffusion models. This has led to the discovery of merged models that combine strengths of specialized models into a single model. For some time, the most popular Stable Diffusion models were neither the original base models nor the fine-tuned versions, but rather the merged models created by enthusiasts.\n\n#### 1.1.2 Merging Language Models\n\nSimple weight interpolation works well for merging image generation models that operate in a real valued latent and output space, but to our surprise, they also still work for merging language models under the right conditions, but have performance issues [\\[63\\]](#page-20-3). Recently, new methods have been proposed to address merging language models specifically. One method, Task Arithmetic [\\[27\\]](#page-19-10), involves building *task vectors* by subtracting pre-trained from fine-tuned model weights, enabling manipulation through arithmetic operations to steer the merged model's behavior.\n\nTask Arithmetic operates as follows. Let  $\\theta\\_{base} \\in \\mathbb{R}^d$  denote the weights of a pre-trained base model. For each task k, let  $\\theta\\_k \\in \\mathbb{R}^d$  represent the weights of a model fine-tuned for that specific task. The task vector  $\\tau\\_k$  for each task k is defined as the difference between the fine-tuned model weights and the base model weights:  $\\tau\\_k = \\theta\\_k - \\theta\\_{base}$ . The weights of the new, merged model  $\\theta\\_{new}$  are then computed by adding a weighted sum of these task vectors to the base model weights:  $\\theta\\_{new} = \\theta\\_{base} + \\sum\\_k \\lambda\\_k\\tau\\_k$  Here,  $\\lambda\\_k$  is a scaling parameter that determines the contribution of each task vector to the final merged model.\n\nA key problem of weight interpolation-based methods is that they neglect parameter interference, leading to performance degradation. A recent work [\\[63\\]](#page-20-3) identifies two key sources of interference: redundant parameter values and conflicting parameter signs across models, and proposes a method that addresses these issues to achieve improved merging performance. The proposed TIES-Merging method addresses information loss in existing merging methods by incorporating three steps: resetting minimal parameter changes, resolving sign conflicts, and merging only aligned parameters. The trimming of redundant parameter changes in TIES-Merging not only resulted in higher performance of merged models, but also achieved better compression for the fine-tuned parameter updates. This compression of fine-tuned weight parameter updates is further investigated in ComPEFT [\\[62\\]](#page-20-4).\n\nThe three steps of TIES-Merging are implemented as follows. In the first step, minimal parameter changes are reset by retaining only the most influential components of each task vector τ<sup>k</sup> and setting the rest to zero. This effectively removes redundant or insignificant modifications. The second step resolves sign conflicts across models by determining an aggregate sign for each parameter based on the overall direction of significant changes across all tasks. This ensures that conflicting updates do not cancel each other out. In the final step, only the aligned parameters are merged by selectively averaging the values of parameters that agree with the elected sign from the previous step, while excluding those that disagree. This selective merging process ensures that the final model incorporates only consistent and significant parameter changes from each task-specific model, thereby preserving task-relevant information while minimizing interference.\n\nAnother recent work [\\[65\\]](#page-20-5) proposes the DARE method, which goes further by zeroing out small differences between the fine-tuned model and the original base model, while amplifying the differences. In practice, DARE [\\[65\\]](#page-20-5) is often used together with Task Arithmetic [\\[27\\]](#page-19-10) or TIES-Merging [\\[63\\]](#page-20-3). Specifically, DARE operates as follows. It applies a random mask m<sup>k</sup> ∼ Bernoulli(p) to task vector τk, where p is the drop rate, and then rescales the remaining parameters:\n\n$$\n\\hat{\\tau}\\_k = \\frac{1}{1-p} \\left( (1-m\\_k) \\odot \\tau\\_k \\right). \\tag{1}\n$$\n\nThe resulting sparsified task vector τˆ<sup>k</sup> is then used to merge models. This task vector can be utilized in the aforementioned linear task arithmetic (referred to as DARE-Linear) or TIES (referred to as DARE-TIES) methods.\n\nModel merging is gaining momentum in ML research, and has been applied in areas such as multimodal model development [\\[7,](#page-18-5) [58\\]](#page-20-6) and merging experts [\\[7\\]](#page-18-5). It is also becoming popular in both the broader image generation and LLM communities, where due to the low barrier to entry, many researchers and engineers are continually merging existing models to create new customized models.\n\nIn the image generation domain, model merging techniques became widely accessible to practitioners especially after their implementation in open-source toolkits [\\[10\\]](#page-18-4). Similarly, language model merging really gained popularity after the implementation of *mergekit* [\\[21,](#page-18-0) [34\\]](#page-19-0). This toolkit provides all of the popular *recipes* for merging language models. In addition to simple linear and spherical interpolation, advanced recipes such as Task Arithmetic, TIES-Merging, DARE are also implemented, offering users the ability to experiment with combining these methods to work with fine-tunes of popular base models such as Mistral [\\[28\\]](#page-19-11). With this toolkit, a large number of capable merged models are developed by the community, and most of the top models on the Open LLM Leaderboard [\\[26\\]](#page-19-1) are gradually dominated by merged models produced by the community of language model enthusiasts.\n\nMergekit also introduced an additional method, called *Frankenmerging* which is not based on weight merging, but for the user to experiment with trying to stack different layers from multiple models to sequentially create a new model. This method has the advantage of not tying the user to one particular family of models with a fixed architecture for merging (e.g. Mistral-based models), but potentially creating *new* architectures from merging entirely different models. It is noted that discovering new *Frankenmerging* technique remains a challenge for the community, and requires much more trial and error to discover new recipes for this technique. To date, almost everyone uses a similar *Frankenmerging* merge recipe, and there has been very little trial and error to improve upon it. The field remains highly under-explored, and we believe this is where evolution can help.\n\n#### 1.1.3 Connection to Evolutionary Neural Architecture Search\n\nModel merging holds significant potential and democratizes the model-building process, making it accessible to a broad range of participants. However, it heavily relies on intuition and domain knowledge. Human intuition, however, has its limits. With the growing diversity of open models and tasks, we need a more systematic approach. We believe evolutionary algorithms, inspired by natural selection, can unlock more effective merging solutions. These algorithms can explore a vast space of possibilities, discovering novel and counter-intuitive combinations that traditional methods and human intuition might miss.\n\nIn deep learning, techniques such as Neural Architecture Search (NAS) [\\[66\\]](#page-20-7) employed evolutionary techniques to discover new architectures [\\[48,](#page-19-12) [55\\]](#page-20-8) that might be non-intuitive for human designers to discover. However, NAS-methods consumed significant computational resources given how each candidate model architecture needed to be trained. Our approach aims to *save* compute resources by discovering structures that take advantage of existing capabilities of existing Transformer blocks that had already consumed considerable resources to train in the first place.\n\nPre-trained transformer blocks are akin to neural network components one can choose to mix and match using an evolutionary architecture search algorithm. However, unlike NAS, we do not need to train the model, and can evaluate the candidates right away. In a sense, our work resonates with the goals of earlier works of morphology search such as NEAT [\\[56\\]](#page-20-9) and more recent work such as Weight Agnostic Neural Networks [\\[18\\]](#page-18-6) which evolved neural network structures with certain task-specific inductive biases, without the need to train the weight parameters using gradient descent. Similarly, this was how NAS methods such as SMASH [\\[56\\]](#page-20-9) avoided costly inner-loop training through the application of a Hypernetwork [\\[22\\]](#page-18-7) to estimate the weights of architectural candidates.\n\nRecent work in language models tried to duplicate the layers of a pre-trained Transformer many times [\\[58\\]](#page-20-6) as a starting point to pre-train a much larger network, to enable efficient training of much larger language models [\\[64\\]](#page-20-10). Similarly, in our approach, we are interested in applying automated search methods to discover the effective reuse of existing pre-trained layers.\n\nIn this work, we apply evolution to not only automate the optimization of model merging recipes in weight space, confined to a single architecture, but also explore applying evolution to optimize the stacking of layers from different models—a more involved approach that has the potential to create\n\nentirely novel neural architectures from existing building blocks. Unlike weight merging optimization which confine space of models to be merged together to fine-tuned descendents of the same parent base model, layer stacking optimization has no such constraints. Our work systematically explores the application of evolution applied in both parameter and layer space.\n\n## <span id=\"page-4-1\"></span>2 Methods\n\nOur goal is to create a unified framework capable of automatically generating a merged model from a any individual in the collection. Central to our approach is the application of evolutionary algorithms, which we employ to refine the intricacies involved in model merging. To systematically address this challenge, we first dissect the merging process into two distinct, orthogonal configuration spaces, analyzing their individual impacts. Building on this analysis, we then introduce a cohesive framework that seamlessly integrates these spaces. Figure [1](#page-4-0) provides a schematic representation of our approach. ✅ ❎ ✅\n\n✅\n\n✅\n\n✅\n\nQ2: Cynthia eats one serving of ice cream every night. … How much will she have spent on ice cream after 60 days?\n\n![](_page_4_Figure_4.jpeg)\n\n<span id=\"page-4-0\"></span>Figure 1: Overview of Evolutionary Model Merge. Our approach encompasses (1) evolving the weights for mixing parameters at each layer in the parameter space (PS); (2) evolving layer permutations in the data flow space (DFS); and (3) an integrated strategy that combines both methods for merging in both PS and DFS. Notice that merging in the PS is not simple copying and stitching of the layers parameters, but also mixes the weights. This merging is akin to blending colors as illustrated here (e.g., red and blue becomes purple). Note that we translated the questions to English for the reader; the models operate on Japanese text.\n\n### 2.1 Merging in the Parameter Space\n\nModel merging in the parameter space (PS) aims to integrate the weights of multiple foundational models into a unified entity with the same neural network architecture, yet outperforming the individual models. While various strategies for combining model parameters exist [\\[41,](#page-19-9) [61\\]](#page-20-0), our approach leverages task vectors analysis to understand each model's strengths, based on the specific tasks they are optimized for or excel in [\\[27\\]](#page-19-10). Specifically, we enhance TIES-Merging with DARE [\\[63,](#page-20-3) [65\\]](#page-20-5), allowing for more granular, layer-wise merging (In this paper, by \"layer\" we mean the input/output embedding layers or a transformer block). We establish merging configuration parameters for sparsification and weight mixing at each layer, including input and output embeddings. These configurations are then optimized using an evolutionary algorithm, such as CMA-ES [\\[23\\]](#page-18-8), for selected tasks, guided by critical task-specific metrics (e.g., accuracy for MGSM, ROUGE score for VQA).\n\n## 2.2 Merging in the Data Flow Space\n\nRecent analysis and discoveries imply that knowledge is stored distributedly in language models [\\[20,](#page-18-9) [42,](#page-19-13) [43\\]](#page-19-14), suggesting simple yet novel model merging possibilities in the data flow space (DFS). Unlike\n\nmerging in PS, model merging in DFS preserves the original weights of each layer intact. Instead, it optimizes the inference path that tokens follow as they traverse through the neural network. For example, after the i-th layer in model A, a token may be directed to the j-th layer in model B.\n\nIn our initial effort in this domain, we limit ourselves to serial connections and non-adaptive configurations, deferring the investigation of more flexible model merging to future work. Concretely, with a collection of N models and a budget T, our method searches for a sequence of layer indices L (t) i,j that delineates the path all the tokens should follow for a specific task. Here Li,j denotes the j-th layer in the i-th model, with t ∈ [1, T] marking the step in the inference path.\n\nOne can quickly imagine how large the search space is. Assuming the total number of layers across all models is  $M$ , the size of the search space is  $(M + 1)^T$ , here the extra one indicates the inclusion of a pass-through layer. Even with a modest setting of  $M = 64$  (e.g., 2 models of 32 layers each) and  $T = 60$ , this setting translates to an astronomically large search space, a challenge even for a capable evolutionary search algorithm. Luckily, our preliminary studies indicated that certain layer arrangements, particularly repetitive or permuted sequences from earlier in the model, can adversely affect performance. Based on this, we modify our settings to include an indicator array  $\\mathcal{I}$  of size  $T = M \\times r$  in the evolutionary search space, here  $r$  is number of repetitions.\n\nConceptually, we layout all the layers in sequential order (i.e., all layers in the *i*-th model followed by those in the *i* + 1-th model) and repeat them  $r$  times, the indicator array then manages the inclusion/exclusion of layers. If  $\\mathcal{I}\\_i > 0$  we include the layer corresponding to index  $i$  in the slots in the merged model, otherwise we exclude it. Consequently, our search space is reduced to  $2^T$ , which is still large, but tractable for evolutionary search.\n\nIn our search, we only optimize the data inference path inside the merged model and keep parameters in the models intact. In this setting, a layer may face an input whose distribution is different from what it is used to (from its original model), leading to unexpected outputs. For example, our preliminary studies [\\[57\\]](#page-20-11) show that swapping a pair of neighboring layers in a language model makes its performance drop. Although more theoretical studies are needed to model the distribution shift, empirically we find that appropriately scaling an input that wishes to go from layer  $i$  to  $j$  by  $W\\_{ij}$  help alleviate the problem. Here,  $W \\in R^{M \\times M}$  is a matrix that is also optimized by the evolutionary search together with the indicator array  $\\mathcal{I}$ .\n\nThe size of  $W$  grows quadratically with  $M$ , for scenarios involving a large number of layers. An alternative approach to contain the search space size involves parameterizing  $W$  with a neural network [\\[22,](#page-18-7) [51\\]](#page-20-12). We can instead evolve a feed-forward network to output the scaling weights conditioned on the layer and step indices:  $W\\_{ij} = \\pi\\_{\\theta}(i, j, t)$  where  $\\theta$ 's are the parameters to be evolved, whose size does not change when  $M$  grows.\n\n#### 2.3 Merging in Both Spaces\n\nModel merging in the PS and in the DFS are orthogonal approaches, however, it is straightforward to combine these disentangled methods and further boost the performance of a merged model. As we show in the rightmost illustration in Figure [1](#page-4-0) and in Section [3.1,](#page-6-0) it is possible to first apply PS merging to a collection of models, and then put back this merged model in the collection and apply DFS merging from this enlarged collection.\n\nThis can be extremely helpful when one considers model merging with multiple objectives, wherein PS merging can be applied first to produce several merged models each of which targets one of the muliple objectives of interest, and then DFS merging is applied with multi-objective genetic algorithms such as NSGA-II [\\[16\\]](#page-18-10) to further expand the final model's performance in relevant metrics.\n\n## 3 Results\n\nMost merged models in the community optimize for a narrow set of tasks defined in The Open LLM Leaderboard [\\[26\\]](#page-19-1). Our motivation is for evolutionary search to discover novel ways to merge different models from vastly different domains (e.g., non-English language and Math, or non-English language and Vision) which might be difficult for human experts to discover effective merging solutions themselves. Furthermore, effectively merging models from very different domains can lead to models of wider real-world applicability and enable us to develop models beyond the large population of models that are optimized for the narrow range of tasks defined by a leaderboard.\n\nWe demonstrate our Evolutionary Model Merge approach described in Section [2](#page-4-1) by evolving a Japanese LLM capable of Math reasoning, and a Japanese VLM proficient in handling culturallyspecific content. Specifically, in Section [3.1,](#page-6-0) we apply evolution to merge a Japanese LLM with an English Math LLM to build a Japanese Math LLM, and in Section [3.3,](#page-13-0) we apply evolution to merge a Japanese LLM with an English VLM to create a Japanese VLM.\n\n#### <span id=\"page-6-0\"></span>3.1 Evolving Japanese Math LLM\n\n#### 3.1.1 Experimental Setup\n\nSource Models To develop a model capable of solving math problems in Japanese, we apply evolutionary model merge on a set of source models containing a Japanese LLM and Math LLMs: shisa-gamma-7b-v1 [\\[9\\]](#page-18-11) (Japanese LLM), WizardMath-7B-V1.1 [\\[40\\]](#page-19-15) and Abel-7B-002 [\\[12\\]](#page-18-12). All these models are fine-tuned from Mistral-7B-v0.1 [\\[28\\]](#page-19-11).\n\nDataset For testing, we used the MGSM dataset [\\[52\\]](#page-20-13), a multilingual translation of a subset of the GSM8k dataset [\\[13\\]](#page-18-13). The Japanese test set of MGSM, consisting of 250 samples, was used for the final evaluation. Specifically, MGSM contains translations of the first 250 samples (ID 0-249) from the GSM8k test set. We used a different dataset for evolutionary search to avoid overfitting the test set. Specifically, we translated the remaining 1069 samples (out of 1319 examples) of the GSM8k test set that were not included in the MGSM test set into Japanese. These correspond to samples with IDs 250-1318 in the original GSM8k test set, ensuring no overlap with the MGSM Japanese test set. One may think that it is more natural to translate the GSM8k training set. However, in our preliminary efforts, this approach did not work well. Since open-source math models were trained on the GSM8k training set, we were unable to perform accurate evaluations.\n\nEvaluation We evaluated the ability to generate Japanese answers to Japanese math problems. Therefore, we considered an answer correct if it met the following criteria: (1) the concluding numerical value must be correct, and (2) the reasoning text should be written in Japanese.\n\nWe treated the last numerical value appearing in the output as the answer. We needed to adopt this heuristic because we are merging multiple models that were trained in different formats, which made it difficult to correct the output format. This method appeared to extract the answers correctly in almost all cases. Additionally, to determine the language of the output, we utilized fasttext [\\[29,](#page-19-16) [30\\]](#page-19-17). We used greedy sampling for generation and calculated the zero-shot pass@1 accuracy.\n\nOptimization For optimization in PS, we used the CMA-ES[\\[23\\]](#page-18-8) algorithm implemented in Optuna [\\[8\\]](#page-18-14) with default hyper-parameters. Specifically, we set all initial parameter values to 0.5, sigma to 1/6, and the population size to  $4 + \\lfloor 3 \\ln(n\\_{\\text{params}}) \\rfloor$ , where  $n\\_{\\text{params}}$  is the number of parameters to optimize. The fitness value is defined as the accuracy for all 1069 training samples. Please note that this set is disjoint from MGSM's test set. The optimization was conducted for 1000 trials, and the best trial with respect to the training accuracy was chosen as the final model. We decided to employ TIES-Merging [\\[63\\]](#page-20-3) with DARE [\\[65\\]](#page-20-5) through preliminary experiments, and optimized its parameters.\n\nIn our DFS merging experiments,  $M = 64$ ,  $r = 3$ , and consequently,  $T = M \\times r = 192$ . We kept the last 200 examples in the training data as our validation set and optimize on the rest of the data with a batch size of 200. We report the performance of the snapshot that achieved the highest accuracy in the validation set, and the test set is strictly isolated from the optimization process. We adopted CMA-ES in EvoJAX [\\[59\\]](#page-20-14), it optimized  $I$  and  $W$  for a total of 100 generations with a popluation size of 128, and we used the default hyper-parameters. We limited our DFS merging to two models A and B to ensure that the final model remains modest in size and can be run on a single GPU, but in principle, the methodology can scale to merging multiple models. During the merging, model A's tokenizer, and input/output embeddings are utilized. Furthermore, to maintain compatibility with the embedding layers, we mandate that the initial and final transformer layers of model A define the start and the end of the inference path. We initialized the indicator array  $I$  so that all layers in model A are more likely to be included as initial hops in the inference path to shorten the search time.\n\n#### 3.1.2 Experimental Results\n\nTable [1](#page-7-0) summarizes the performance of the LLMs on Japanese Math and overall Japanese benchmark tasks. The MGSM-JA column reports the results from the MGSM test set, employing the previously described metrics. The Japanese language model (Model 1) demonstrates limited mathematical proficiency, while the Math models (Models 2 and 3), though mathematically adept, show insufficient command of the Japanese language. Consequently, all three models score low scores on the MGSM-JA, with accuracy at or below 30.0.\n\nTable 1: Performance Comparison of the LLMs. Models 1–3 are source models, Models 4–6 are our optimized merge models, and Models 7–11 are provided for reference. MGSM-JA measures math ability in Japanese, and JP-LMEH evaluates general Japanese language capabilities, with a detailed breakdown provided in Table [2.](#page-8-0)\n\n<span id=\"page-7-0\"></span>\n\n| #  | Model                 | Type       | Size | MGSM-JA (acc ↑) | JP-LMEH (avg ↑) |\n|----|-----------------------|------------|------|-----------------|-----------------|\n| 0  | Mistral-7B-v0.1       | EN general | 7B   | 2.0             | 55.7            |\n| 1  | Shisa Gamma 7B v1     | JA general | 7B   | 9.6             | 66.1            |\n| 2  | WizardMath 7B v1.1    | EN math    | 7B   | 18.4            | 60.1            |\n| 3  | Abel 7B 002           | EN math    | 7B   | 30.0            | 56.5            |\n| 4  | Ours (PS)             | 1 + 2 + 3  | 7B   | 52.0            | 70.5            |\n| 5  | Ours (DFS)            | 3 + 1      | 10B  | 36.4            | 53.2            |\n| 6  | Ours (PS+DFS)         | 4 + 1      | 10B  | 55.2            | 66.2            |\n| 7  | Llama 2 70B           | EN general | 70B  | 18.0            | 64.5            |\n| 8  | Japanese StableLM 70B | JA general | 70B  | 17.2            | 68.3            |\n| 9  | Swallow 70B           | JA general | 70B  | 13.6            | 71.5            |\n| 10 | GPT-3.5               | commercial | -    | 50.4            | -               |\n| 11 | GPT-4                 | commercial | -    | 78.8            | -               |\n\nIn contrast, our merged models (Models 4–6) manifest a substantial elevation in performance. Notably, the model merged in PS (Model 4) achieves an impressive score of 52.0, highlighting the remarkable potential in combining models with distinct expertises. The DFS-merged model (Model 5) also shows a performance enhancement, with an over 6 percent increase in accuracy compared to the source models. While the leap in performance is not as pronounced as with PS merging, it still proves to be a valid and orthogonal approach. Finally, our hybrid model (Model 6), which integrates both merging strategies, shows further enhancements on the task. The order of the source models in the indicator array  $I$  affects the performance of the DFS merging method. We conduct experiments with all possible combinations and report the best scores in Table [1,](#page-7-0) we defer detailed analysis to Sec [3.2.5.](#page-12-0)\n\nFigure [2](#page-7-1) gives an overview of the five models' \"answer sheet\" on the math problems. Our merged models retain the foundational knowledge in the source models, as evidenced by the similar score patterns on problems 1–15. Moreover, they exhibit emergent capabilities, successfully tackling problems that stumped the source models (e.g., problems 20–30). Evidently, by effectively integrating a Japanese LLM and mathematical models, we have succeeded in producing models that are proficient in both Japanese language understanding and mathematical problem-solving.\n\n![](_page_7_Figure_6.jpeg)\n\n<span id=\"page-7-1\"></span>Figure 2: Performance Overview. The figure depicts the success of various models on the MGSM-JA task, with each of the 250 test problems represented along the x-axis by problem ID. Correct answers are indicated by colored markers at the corresponding positions.\n\nFurthermore, Table [2](#page-8-0) presents the results of evaluating the general Japanese language ability using the Japanese Language Model Evaluation Harness (JP-LMEH) benchmark suite. This benchmark suite consists of nine tasks, and the average score across these tasks is widely used as an indicator of overall Japanese language proficiency. Our models achieve remarkably high scores of 70.5 and 66.2, surpassing the source models and even the previous state-of-the-art 70B parameter Japanese LLM (specifically, Japanese StableLM 70B), despite having only 7B to 10B parameters.\n\nCompared to the source Japanese model (shisa-gamma-7b-v1), it is interesting to note that there is not only a significant improvement in the MGSM scores but also improvements in other tasks such as JSQuAD and JAQKET. However, we also acknowledge that there are certain tasks, such as JNLI and MARC (for DFS), where performance has dropped. It should be noted that the MGSM scores here do not match those in Table [1,](#page-7-0) due to the differences in evaluation protocols (few-shot, prompting, etc.). Additional details are provided in Appendix [A,](#page-21-0) including extensive comparisons with other models in Table [6.](#page-22-0)\n\nMoreover, Appendix [C](#page-21-1) showcases intriguing examples that demonstrate the utility of our models merged using evolution. The merged models correctly answered questions that require both knowledge about Japanese-culture *and* Math ability. In contrast, even if such Japanese questions were translated into English and answered in English, English math models would likely fail to provide the correct answers as they may not be aware of Japanese culture-specific context in the questions.\n\nTable 2: Breakdown of JP-LMEH Scores for Japanese Language Proficiency. JP-LMEH (Japanese Language Model Evaluation Harness) is a benchmark suite consisting of 9 tasks, and the average score (Avg column) is used as an indicator of overall Japanese language proficiency. Full results are in Table [6](#page-22-0)\n\n<span id=\"page-8-0\"></span>\n\n| Model                  | Size | JComQA | JNLI | MARC | JSQuAD | JAQKET | XLSum | XWino | MGSM | JCoLA | Avg  |\n|------------------------|------|--------|------|------|--------|--------|-------|-------|------|-------|------|\n| Shisa Gamma 7b v1      | 7B   | 91.2   | 72.1 | 94.6 | 73.9   | 68.0   | 25.9  | 80.5  | 29.6 | 58.7  | 66.1 |\n| WizardMath 7B V1.1     | 7B   | 74.7   | 42.7 | 90.4 | 84.6   | 68.5   | 22.3  | 69.8  | 38.8 | 48.9  | 60.1 |\n| Abel 7B 002            | 7B   | 70.3   | 51.8 | 62.3 | 83.8   | 69.0   | 22.5  | 68.2  | 28.0 | 52.7  | 56.5 |\n| Ours (PS)              | 7B   | 89.1   | 65.7 | 95.4 | 89.5   | 77.7   | 25.5  | 81.2  | 50.0 | 60.5  | 70.5 |\n| Ours (DFS)             | 10B  | 67.7   | 58.2 | 53.5 | 66.8   | 54.3   | 17.3  | 65.6  | 30.0 | 65.6  | 53.2 |\n| Ours (PS+DFS)          | 10B  | 88.2   | 50.3 | 91.5 | 78.6   | 77.8   | 23.2  | 73.0  | 40.0 | 73.0  | 66.2 |\n| Llama 2 70B            | 70B  | 80.2   | 53.4 | 94.4 | 91.6   | 80.1   | 21.8  | 73.6  | 30.4 | 54.6  | 64.5 |\n| Japanese Stable LM 70B | 70B  | 91.2   | 50.4 | 92.9 | 87.1   | 88.4   | 24.3  | 82.0  | 37.2 | 61.7  | 68.3 |\n| Swallow 70B            | 70B  | 95.3   | 57.2 | 91.7 | 94.1   | 93.9   | 23.1  | 83.3  | 45.2 | 59.5  | 71.5 |\n\n#### 3.1.3 Analysis\n\nIn our exploration of model merging in the parameter space, we experimented with diverse configurations, such as varying the assignment of merging parameters across different layer groups. However, due to a constrained dataset, we did not witness notable improvements in performance correlating with increase in configuration complexity. Consequently, we focused our reporting on a PS merged model (Model 4 in Table [1\\)](#page-7-0) that adopts the simplest setting: considering each source model as a singular layer and allocating two DARE-TIES associated parameters to each for evolutionary merging. Figure [3](#page-9-0) illustrates the evolved parameter configuration post PS merging.\n\nThe CMA-ES optimization results reveals that all three models are important, as suggested by the uniformity of the optimized weighting values. The fact that the sum of the weights exceeds 1 and is approaching 2, is noteworthy. This suggests that a combination method which amplifies the contributions of the models, rather than a simple interpolation, proved to be more effective.\n\nThe dominant density from the Japanese LLM suggests its critical contribution to solving the task. We conjecture that this may also be partially attributed to the Japanese LM's larger amount of fine-tunings from the Mistral base model. Japanese LMs based on English models, such as Shisa-Gamma-7B-v1, are typically created through a two-step process: continued pretraining and instruction fine-tuning. The continued pretraining phase involves learning from a significantly larger dataset compared to standard fine-tuning. For instance, Shisa-Gamma-7B-v1 is based on a Japanese base model that underwent continued pretraining on 100B tokens of Japanese text. The resulting differences in weights between Shisa-Gamma-7B-v1 and the original Mistral-7B-v1 likely encapsulate more information than standard fine-tuning, making them more challenging to sparsify. In line with the discussion in [\\[65,](#page-20-5) Section 4.6], the sparsification of DARE tends to degrade performance when applied to such extensively fine-tuned models. Our evolutionary search has seemingly managed to address this issue by increasing the density for the Japanese LM.\n\nThe parameter settings derived from our PS merging experiments align well with outcomes from our DFS merging efforts. By incorporating the PS-merged model into our pool of source models and applying DFS merging across all potential pairings, we observed optimal performance with the combination of the PS-merged model and the Japanese language mode (Model 6 in Table [1\\)](#page-7-0). This finding echoes the significant influence of the Japanese language model, as indicated by its notable presence in Figure [3,](#page-9-0) and reaffirms the substantial promise of evolutionary model merging.\n\n![](_page_9_Figure_1.jpeg)\n\n<span id=\"page-9-0\"></span>Figure 3: Evolved Configurations for PS merging. Although the weights are similar across the 3 source models, the pronounced density from the Japanese LLM underscores its pivotal role.\n\nFigure [4](#page-9-1) displays the evolution of the inference path, where our approach consistently recognized the value of initial steps, incorporating every layer from the first model (our PS-merged model), except for the last decoding layer and the embedding layer. As the process advanced, the method refined the selection to a smaller, more effective set of layers and strategically alternated between layers from both contributing models. Significantly, the scaling parameters  $W\\_{ij}$  emerged as crucial elements, our ablation studies revealed that eliminating them in the evolved model (e.g., by setting  $W\\_{ij} = 1$ ) led to a performance decline exceeding 20 percent, highlighting their importance in the model's efficacy.\n\n![](_page_9_Figure_4.jpeg)\n\n<span id=\"page-9-1\"></span>Figure 4: Evolved Configurations for DFS Merging of models A and B. The 3 figures depict the evolution of the inference path on the MGSM-JA task. The y-axis represents the layer index  $l \\in [1, M]$ , and the x-axis corresponds to the path index  $t \\in [1, T]$ . Blue markers indicate path steps utilizing layers from model A, red markers denotes those from B. Marker color intensity reflects the magnitude of the scaling factor Wij . The evolutionary search result includes most layers in A at an early stage and then alternates between layers from both models. Result from our 10B model (PS+DFS).\n\n#### 3.2 Method Behavior Analysis\n\nIn this section, we conduct a more in-depth analysis of our method's performance. We compare it with multiple types of baselines, experimentally analyze the effect of manual model selection, verify the scalability of our proposed approach to larger models, and show results from ablation studies.\n\n#### 3.2.1 Comparison with Unoptimized Model Merging\n\nTable [3](#page-10-0) presents the performance of models created through unoptimized model merging. We utilized three methods: TIES-Merge [\\[63\\]](#page-20-3), DARE-TIES [\\[65\\]](#page-20-5), and Frankenmerging [\\[21\\]](#page-18-0). We used mergekit to establish the baselines. Since merkit does not include default hyperparamters, we adopted those from a widely recognized resource for learning about model merging and mergekit [\\[34\\]](#page-19-0).\n\n| Method                     | Size | MGSM-JA (acc ↑) | JP-LMEH (avg ↑) |\n|----------------------------|------|-----------------|-----------------|\n| Ours (PS)                  | 7B   | 52.0            | 70.5            |\n| Ours (DFS)                 | 10B  | 36.4            | 53.2            |\n| Ours (PS+DFS)              | 10B  | 55.2            | 66.2            |\n| Unoptimized model merging: |      |                 |                 |\n| TIES-Merge [63]            | 7B   | 4.4             | 63.7            |\n| DARE-TIES [65]             | 7B   | 35.2            | 66.3            |\n| Frankenmerging [21]        | 9B   | 0.0             | 16.1            |\n| Fine-tuning:               |      |                 |                 |\n| LoRA, Shisa Gamma 7B v1    | 7B   | 20.8            | 66.4            |\n| LoRA, WizardMath 7B v1.1   | 7B   | 43.2            | 55.9            |\n| LoRA, Abel 7B 002          | 7B   | 40.0            | 52.8            |\n| Full, Shisa Gamma 7B v1    | 7B   | 19.2            | 54.5            |\n| Full, WizardMath 7B v1.1   | 7B   | 33.6            | 43.4            |\n| Full, Abel 7B 002          | 7B   | 36.4            | 37.5            |\n| Distraction:               |      |                 |                 |\n| +0 irrelevant model        | 7B   | 50.0            | 65.9            |\n| +1 irrelevant model        | 7B   | 46.8            | 64.2            |\n| +2 irrelevant model        | 7B   | 46.8            | 64.1            |\n| +4 irrelevant model        | 7B   | 48.4            | 64.0            |\n| +8 irrelevant model        | 7B   | 40.8            | 65.8            |\n\n<span id=\"page-10-0\"></span>Table 3: Performance Comparison of Proposed Methods and Baselines. Results show MGSM-JA accuracy and JP-LMEH average scores for our proposed methods (PS, DFS, PS+DFS), unoptimized model merging techniques, fine-tuning approaches, and experiments with irrelevant model distraction.\n\nFor the PS baseline equivalents (TIES-Merge and DARE-TIES), we used shisa-gamma-7b-v1, WizardMath-7B-V1.1, and Abel-7B-002 as the source models, allowing direct comparison with our PS experiments. Similarly, for the DFS baseline equivalent (Frankenmerging), we used the same combination of source models as in our DFS experiments, enabling direct comparisons.\n\nThe results indicate that TIES-Merge and Frankenmerging led to decreased performance in both MGSM-JA and JP-LMEH, suggesting unsuccessful merging. Notably, Frankenmerging resulted in a MGSM-JA score of 0, which is consistent with its general tendency to often produce models that fail to generate meaningful outputs. DARE-TIES performed relatively better among these baselines, with a slight improvement in MGSM-JA compared to the source models. However, our PS model significantly outperformed all these baselines, and the hybrid merging method (PS+DFS) gives the best test performance on the target task.\n\n### 3.2.2 Comparison with Fine-tuning\n\nTable [3](#page-10-0) also compares our evolutionary model merging approach with the fine-tuning method. To ensure a fair comparison, we used the identical dataset. Specifically, we utilized the 1069 samples of Japanese-translated GSM8k that were employed in our PS and DFS optimization for fine-tuning the models. We tested learning rates of 1e-5, 5e-5, and 1e-4, choosing the optimal one. We set the number of epochs to 3 and tried both LoRA and full parameter fine-tuning. We experimented with all three source models: shisa-gamma-7b-v1, WizardMath-7B-V1.1, and Abel-7B-002.\n\nThe MGSM-JA scores show that while fine-tuning generally improved performance over the original models, the highest score achieved was 43.2. This is notably lower than our proposed method, which exceeded a score of 50. Additionally, unlike our approach, fine-tuning often led to significant decreases in JP-LMEH scores. This suggests that fine-tuning may be more data-hungry and prone to catastrophic forgetting issues.\n\nOther potential fine-tuning baselines. Another potential fine-tuning baseline would be combining all data used by all source models to fine-tune a base model. However, this approach is often impractical in general settings, and in our specific case, it is indeed impossible to implement for comparison. We elaborate on these points below. Our proposed approach's strength lies in its ability to freely combine openly published models from platforms like HuggingFace. However, model publishers often only release the models themselves, not the training datasets or detailed\n\ntraining methods, as these are considered proprietary knowledge. For instance, the training data for WizardMath-7B-V1.1 and Abel-7B-002 is not publicly available. shisa-gamma-7b-v1 is a further fine-tuned version of japanese-stablelm-base-gamma-7b (which is Mistral-7B-v0.1 fine-tuned for Japanese), and the data used for training japanese-stablelm-base-gamma-7b is also not public. Moreover, the Japanese fine-tuning for japanese-stablelm-base-gamma-7b involved 100B tokens, indicating a substantial computational cost (such as more than 10,000 GPU hours). Therefore, a comparison with this approach is not feasible. It is also worth noting that such fine-tuning would require careful tuning of dataset mixing ratios, which is non-trivial and costly.\n\n#### 3.2.3 Impact of Manual Model Selection\n\nIn our main experiments, we manually selected three source models. To assess the broader applicability of our approach, it is useful to investigate how sensitive the method is to the selection of source models. To this end, we conducted experiments to measure the distraction effect of including irrelevant models, which simulate the scenario where suboptimal models are inadvertently selected.\n\nTo investigate the sensitivity of our method to source model selection, we conducted experiments to measure the distraction effect of including irrelevant models. In addition to our original seed models (shisa-gamma-7b-v1, WizardMath-7B-V1.1, Abel-7B-002), we experimented with one, two, four, and eight irrelevant models, and performed PS experiments to examine the impact on performance. The irrelevant models, chosen for their lack of relation to Japanese or mathematics, were: leo-mistral-hessianai-7b (German), lince-mistral-7b-it-es (Spanish), Mistral-7B-v0.3-Chinese-Chat (Chinese), BioMistral-7B (biomedical domain), Claire-Mistral-7B-0.1 (French), komt-mistral-7b-v1 (Korean), Mistral-7B-v0.1-arabic (Arabic), and Loquace-7B-Mistral (Italy). All these models are openly available on HuggingFace.\n\nWe slightly modified the PS optimization by adding weight normalization, which became necessary due to the increased number of models (up to 11 in total). This modification explains why the \"+0 irrelevant model\" result does not exactly match our original PS result.\n\nThe results, shown in the \"Distraction\" part of Table [3,](#page-10-0) are generally favorable. As expected, using only the initial three models yielded the best results. However, the performance degradation when adding irrelevant models was limited, especially up to four additional models, and even with 8 additional models, the performance did not collapse. This demonstrates that our optimization method is relatively robust to manual source model selection. Due to the huge GPU memory requirement at inference time, the distraction experiments were not conducted for the DFS merging method. We instead present behavior analysis and ablation studies on the topic in the following sections.\n\n### 3.2.4 Scaling to Larger Models\n\nFinally, we demonstrate our proposed methods also work with larger models. Concretely, we used fine-tuned versions of Llama-2-13b: ELYZA-japanese-Llama-2-13b-instruct as the Japanese general model and MetaMath-13B-V1.0 as the English math model. All other settings remained identical to the 7B experiments. Due to the limited availability of suitable 13B models, we could only select two source models.\n\nThe results are shown in Table [4.](#page-12-1) Similar to the 7B case, the MGSM-JA score significantly improved over both source models. This consistent behavior at the 13B size demonstrates the scalability of our evolutionary model merging approach.\n\nThe 7B models achieved higher MGSM-JA scores (52.0, 36.4, 55.2) than the 13B models (31.2, 23.2, 34.0) for (PS, DFS, PS+DFS). This difference is attributed to the source models rather than our proposed method. Mistral-7B-v0.1 is known to outperform Llama-2-13b in performance, especially in basic mathematical abilities, despite its smaller size. This is reflected in the performance of math-specialized models: WizardMath-7B-V1.1 scores 83.2 on GSM8k (an English math benchmark), while MetaMath-13B-V1.0 scores 72.3. This difference explains the performance gap in Japanese math abilities between our merged 7B and 13B models.\n\nOn the other hand, our DFS and PS+DFS models (models #4 and #7) achieved higher JP-LMEH average scores than ELYZA-japanese-Llama-2-13b-instruct, the Japanese general model. In general, the qualitative behavior on JP-LMEH is consistent between 7B and 13B models. Both sets\n\n<span id=\"page-12-1\"></span>\n\n| # | Model                               | Type       | Size | MGSM-JA (acc) | JP-LMEH (acc) |\n|---|-------------------------------------|------------|------|---------------|---------------|\n| 0 | Llama-2-13b                         | EN general | 13B  | 2.8           | 54.1          |\n| 1 | ELYZA-japanese-Llama-2-13b-instruct | JA general | 13B  | 13.2          | 60.2          |\n| 2 | MetaMath-13B-V1.0                   | EN math    | 13B  | 8.0           | 48.7          |\n| 3 | Ours (PS)                           | 1 + 2      | 13B  | 31.2          | 59.7          |\n| 4 | Ours (DFS)                          | 1 + 2      | 19B  | 12.8          | 61.1          |\n| 5 | Ours (DFS)                          | 2 + 1      | 21B  | 23.2          | 46.6          |\n| 6 | Ours (PS+DFS w/o W)                 | 3 + 1      | 19B  | 26.4          | 58.1          |\n| 7 | Ours (PS+DFS)                       | 3 + 1      | 21B  | 34.0          | 60.4          |\n\nTable 4: Scaling to 13B Models with ablation studies on DFS merging. Performance comparison and ablation study results on MGSM-JA and JP-LMEH tasks. Rows in gray are ablation studies.\n\nof results show improvements in JSQuAD, JAQKET, and MGSM scores, with slight decreases in JNLI and MARC compared to the original Japanese model, indicating similar trends in task-specific score changes (see Table [6\\)](#page-22-0).\n\n#### <span id=\"page-12-0\"></span>3.2.5 Analysis on DFS Merging\n\nWe included the results from two ablation studies for DFS merging in Table [4,](#page-12-1) and we wish to get insights as to how and why DFS works.\n\nComparing models #4 and #5, we show that the order of the source models in DFS merging plays a critical role. Specifically, if we put the layers from the English math model in front of those from the Japanese general model in the indicator array  $\\mathcal{I}$  (model #4), the accuracy drops by more than 10 points than the case where the layers orders are reversed (model #5). This \"sub-optimal\" behavior is a trade-off between flexibility and search efficiency. In our experiments, we initialize the indicator array  $\\mathcal{I}$  with zeros, but set the values of those corresponding to layers in model A in the first repetition at  $2\\sigma$  where  $\\sigma$  is the initial standard deviation of CMA-ES exploration (recall that in DFS, layers from model A come before those from model B in each repetition). In this way, the merged model behaves just like model A at the beginning of the DFS process, and is allowed to gradually add/remove layers if necessary. Comparing with the other scenario where we initialize I with only zeros, this setting effectively avoids the cases where poor stacking of the layers causes the LLM to output complete nonsense and waste explorations, at the cost of losing the flexibility to automatically swap the order of models in  $\\mathcal{I}$ .\n\nIn our second ablation study, the difference between models #6 and #7 highlights the importance of the inclusion of the scaling matrix  $W$ . Without scaling, the performance of model #6 is even worse than the PS-merged model, one of its source models. Sufficient as it is in this setting, we wish to point out that this simple scaling treatment does not work if any of the source models has been fine-tuned for a significantly longer time. We suspect that prolonged fine-tuning might lead to increased incompatibility for data traversing through layers from different models. This could be due to permutations in the weight matrices. For example, assuming we are organizing the 4096 dimensions of the hidden states by their absolute values, data from layer 1 in model A might look like (1, 2, ... 4096), but in model B, after extended fine-tuning, it might appear as (4001, 133, 3, ... 989) vs (1.1, 1.9, ..., 4097) when it was only slightly fine-tuned.\n\nWhile it is hard to understand thoroughly why DFS merging helped improve performance, we conducted analysis of the merged model (model #5 in Table [4\\)](#page-12-1) to get insights. Figure [5](#page-13-1) shows the final hops and scales configuration from the merged model. It is important to notice that we forced the scaling factor  $W\\_{i,j} = 1$  if  $j = i + 1$  and layers  $i, j$  are from the same model. This is to prevent the possibility of introducing extra modeling power via  $W$ , in this manner we are certain that all the performance boost comes from layer stacking and permutation. It is interesting and surprising to notice that the DFS-merged model decides to skip layer #30 from MetaMath-13B-V1.0 at the beginning of the inference path. Based on this finding, we removed layer #30 from MetaMath-13B-V1.0, and without any further modifications we found the performance increased to 10%. This implies that DFS merging works because it is able to identify the redundant/harmful operations (with respect to a specific task) embedded in certain layers, and performance increases once these layers are removed.\n\n![](_page_13_Figure_0.jpeg)\n\n<span id=\"page-13-1\"></span>Figure 5: Evolved Configurations from A DFS-merged 13B model. The style and settings of this plot is identical to that of Figure [4.](#page-9-1) Surprisingly, DFS decides to skip layer #30 from the original model (model #2 in Table [4\\)](#page-12-1). If we simply excluded that layer from model #2 and without any further modifications, its performance on MGSM-JA would have increased to 10%. This configuration is from model #5 in Table [4.](#page-12-1)\n\nFurthermore, we analyzed the problems where our DFS-merged model answered correctly while the math source model didn't, and we identify mainly two scenarios where DFS helped improve the performance, see Figure [6](#page-13-2) for illustrations from these scenarios. In the first scenario, we found that the English math model was able to reason correctly. However, it outputs the reasoning steps or answers in English, violating the request that the response must be written in Japanese. In this scenario, DFS-merging managed to shift the output tokens' distribution toward Japanese vocabularies, fixing the problem. In the second scenario, the English math model was actually reasoning in Japanese. But due to the error in understanding the Japanese problem or in the reasoning, it was not able to give correct answers. DFS-merging, due to its appending of the layers from the Japanese general model, was able to largely avoid the problem. Unlike the \"subtraction\" it did previously, these scenarios suggest DFS also works because it is capable of \"addition\" – finding what is missing in one of the source models and compensate that by adding layers in the inference path from other models.\n\n| Scenario 1                                                                                                                                                                                                                                                                                  | Scenario 2                                                                                                                                                                       | Scenario 2 (EN Translation)                                                                                                                                                                                                                                                                                                                                                   |\n|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 以下に、あるタスクを説明する指示があります。リクエストを適切に完了するための<br>回答を日本語で記述してください。一歩一歩考えましょう。                                                                                                                                                                                                                       | 以下に、あるタスクを説明する指示があります。リクエストを適切に完了するための<br>回答を日本語で記述してください。一歩一歩考えましょう。                                                                                                            | Below are instructions describing a certain task. Please write your response<br>in Japanese to complete the request. Let's think step by step.                                                                                                                                                                                                                                |\n| ### 指示:<br>ビリーはDVDを売ります。彼は火曜日に8人のお客様がいます。彼の最初の3人のお客<br>様はそれぞれ1枚のDVDを購入します。彼の翌日の2人の顧客は2枚のDVDをそれぞれ<br>購入しました。の最後の3人のお客様はDVDを購入しません。ビリーは火曜日に何枚<br>のDVDを売りましたか?                                                                                                                                 | ### 指示:<br>ジョーンズおばあちゃんは消防士の昼食会のためにアップルパイを5枚焼きました。<br>それぞれのパイを8個に切り分け、ゲストが自分たちで取れるよう、その5枚のパイを<br>ブッフェテーブルに置きました。ゲストがそれぞれのパイを取り、食べた後、夜更け<br>には14個のパイが残っていました。ゲストは何個のパイを食べましたか?     | ### Instruction:<br>Grandam Jones baked 5 apple pies for the firefighters' luncheon. She cut<br>each pie into 8 pieces and placed the 5 pies on the buffet table. After each<br>guest took and ate some pie, there were 14 pieces of pie left at the end of<br>the night. How many pieces of pie did the guests eat?                                                          |\n| ### 応答:                                                                                                                                                                                                                                                                                     | ### 応答:<br>Question                                                                                                                                                              | ### Response:                                                                                                                                                                                                                                                                                                                                                                 |\n| ビリーの最初の3人のお客様は1枚のDVDを購入しました。<br>So, 3 people x 1 DVD = 3 DVDs were sold.<br>His next day's 2 people of customers bought 2 DVDs each.<br>So, 2 people x 2 DVDs = 4 DVDs were sold.<br>In total, 3 DVDs + 4 DVDs = 7 DVDs were sold.<br>Therefore, Billy sold 7 DVDs on Tuesday.              | ジョーンズおばあちゃんは5枚のパイを8個に切り分け、ブッフェテーブルに置いた。<br>ゲストがそれぞれのパイを取り、食べた後、14個のパイが残っていました.<br>この14個のパイは、5枚のパイを8個に切り分けた8個のパイ + 14個 - 8個 = 14個のパ<br>イです.<br>ゲストは、8個 + 14個 - 8個 = 14個のパイを食べました. | Grandma Jones cut 5 pies into 8 pieces each and placed them on the buffet<br>table. After each guest took and ate some pie, 14 pieces of pie were left.<br>These 14 pieces are from the 5 pies cut into 8 pieces each, so the<br>calculations goes: 8 pieces per pie + 14 pieces - 8 pieces = 14 pieces.<br>Thus, the guests ate 8 pieces + 14 pieces - 8 pieces = 14 pieces. |\n| #### 7<br>The answer is: 7                                                                                                                                                                                                                                                                  | #### 14<br>The answer is: 14<br>MetaMath-13B-V1.0                                                                                                                                | #### 14<br>The answer is: 14                                                                                                                                                                                                                                                                                                                                                  |\n| ビリーの最初の3人のお客様はそれぞれ1枚のDVDを購入しました。<br>そのため, ビリーは最初の3人のお客様に対して, 1枚 x 3 = 3枚のDVDを売りました.<br>次の2人の顧客は2枚のDVDをそれぞれ購入しました.\\n\\nそのため, ビリーは次の2人<br>の顧客に対して, 2枚 x 2 = 4枚のDVDを売りました.<br>最後の3人のお客様はDVDを購入しません.<br>そのため, ビリーは最後の3人のお客様に対して, 0枚のDVDを売りません.<br>また, ビリーはその日のお客様に対して, 3 + 4 + 0 = 7枚のDVDを売りました. | ジョーンズおばあちゃんは5枚のパイを8個に切り分けた。<br>このため、5枚のパイは8個 x 5 = 40個になりました。<br>ゲストがそれぞれのパイを取り、食べた後、14個のパイが残っていました。<br>このため, ゲストは40個 - 14個 = 26個のパイを食べました。<br>#### 26                           | Grandma Jones cut 5 pies into 8 pieces each.<br>Therefore, there were 8 pieces x 5 = 40 pieces in total.<br>After the guests had their share, 14 pieces were left.<br>Consequently, the guests ate 40 pieces - 14 pieces = 26 pieces.<br>#### 26                                                                                                                              |\n| #### 7<br>The answer is: 7                                                                                                                                                                                                                                                                  | The answer is: 26                                                                                                                                                                | The answer is: 26<br>Ours (DFS)                                                                                                                                                                                                                                                                                                                                               |\n\n<span id=\"page-13-2\"></span>Figure 6: Why DFS Improves Performance. DFS merged model improves performance on MGSM-JA mainly by shifting the output tokens' distribution to favor Japanese (scenario 1) and helping the LLM better understand the problem (scenario 2). We show the raw outputs from models #2 and #5 in Table [4,](#page-12-1) but have added/removed redundant lines for the purpose of visualization.\n\n#### <span id=\"page-13-0\"></span>3.3 Evolving Japanese VLM\n\n#### 3.3.1 Multi-modality Extension\n\nWe now extend our method to multi-modal models, and evolve a culturally-specific content aware Japanese VLM. VLMs have recently shown remarkable progress by applying the powerful instructionfollowing capabilities of pre-trained LLMs. The architecture of a VLM generally consists of three components: (1) A vision encoder to extract image features; (2) An LLM to generate text (for the purpose of describing an image); and (3) A projection network to map image features into the LLM's\n\nembedding space [\\[11,](#page-18-15) [15,](#page-18-16) [35,](#page-19-18) [37,](#page-19-19) [39\\]](#page-19-20). Crucially, the LLM component is initialized with powerful pre-trained LLMs for their text generation capabilities. During training, the projection network and optionally the LLM are trained on various vision-language datasets, while the vision encoder is fixed.\n\n#### 3.3.2 Experimental Setup\n\nSource Models The LLM component inside a VLM can be regarded as a standalone LLM, with the extra capability of understanding visual *soft prompts*. From this perspective, by fixing the vision encoder and the projection network and only focusing on the LLM component, it is straightforward to apply the methodologies detailed in Section [2](#page-4-1) to produce a new LLM with expanded capabilities.\n\nIn this experiment, we merge a Japanese LLM and the LLM component in a VLM in the parameter space. We select shisa-gamma-7b-v1 [\\[9\\]](#page-18-11) as the Japanese LLM and LLaVA-1.6-Mistral-7B [\\[38\\]](#page-19-21) as the VLM. Both models are fine-tunes of the Mistral-7B-v0.1 [\\[28\\]](#page-19-11) base model.\n\nDataset To the best of our knowledge, publically accessible Japanese VLM datasets are scarce. In response, we created a new open Japanese VLM benchmark and assessed our VLM on a widely recognized Japanese VQA dataset. Our new benchmark dataset consists of:\n\n- JA-VG-VQA-500: A 500-sample test set extracted from the Japanese Visual Genome VQA dataset [\\[53\\]](#page-20-15).\n- JA-VLM-Bench-In-the-Wild: A Japanese version of LLaVA-Bench-In-the-Wild [\\[39\\]](#page-19-20) . We compiled a rich collection of 42 images, accompanied by a total of 50 questions, featuring a variety of Japanese cultural elements and objects found in Japan. The QAs were crafted with the assistance of GPT-4V [\\[1\\]](#page-18-17) and underwent a human-in-the-loop filtering process to eliminate nonsensical outcomes. Compared to the JA-VG-VQA-500 dataset, our set poses more complex challenges, demanding more nuanced and detailed responses.\n\nWe used another subset of the Japanese Visual Genome VQA dataset during the evolutionary search. This subset is not overlapped with examples in the JA-VG-VQA-500 dataset, to avoid leakage in the optimization process.\n\nThe images in the JA-VLM-Bench-In-the-Wild dataset, which predominantly represent Japanese content, were carefully selected by native Japanese speakers to minimize the risk of insensitive or biased representations. All images were sourced from Unsplash and are published under the Unsplash license, which ensures that there are no ethical or legal issues with their use. Unsplash contributors agree to obtain necessary permissions from related individuals in their photos before uploading, which addresses potential concerns regarding the use of images containing recognizable humans. Additionally, the captions were generated using GPT-4-V and were meticulously reviewed by human annotators to ensure accuracy and cultural sensitivity. This process aims to create a dataset that is both ethically sourced and culturally appropriate.\n\nEvaluation We consider two baselines in our experiments: LLaVA-1.6-Mistral-7B [\\[38\\]](#page-19-21), one of our source models, and Japanese Stable VLM [\\[54\\]](#page-20-16) a Japanese VLM trained from scratch on Japanese datasets.\n\nAll models adopt the same generation configurations, with deterministic decoding. We compute ROUGE-L with a Japanese language detector to replace non-Japanese responses with empty texts, resulting in a score of zero for non-Japanese responses. To be consistent with our LLM experiments in Section [3.1,](#page-6-0) we also employed fasttext [\\[29,](#page-19-16) [30\\]](#page-19-17) for this language detection task. However, we made an exception for cases where the ground-truth answer itself contains non-Japanese but commonly seen words in Japanese texts (e.g., a widely recognized acronym such as \"UFO\"). In these instances, non-Japanese responses from models are not converted to empty texts.\n\nOptimization We use the identical settings as the earlier LLM merging experiments in Section [3.1.](#page-6-0) Concretely, we use TIES-Merging with DARE for merging the source models in the parameter space. For merging in the data flow space, we treat LlaVa 1.6 Mistral 7B as our model A and shisa-gamma-7b-v1 as model B. For PS+DFS, our PS-merged model is model A and shisa-gamma-7b-v1 is model B.\n\n### 3.3.3 Experimental Results\n\nTable [5](#page-15-0) compares the performance of our VLM with the baselines. Please note that the Japanese Stable VLM cannot be evaluated on JA-VG-VQA-500 because it was trained on this dataset.\n\nOur merged VLMs' enhanced performance on the JA-VG-VQA-500 benchmark indicates their proficiency in Japanese, highlighting the successful integration of the source Japanese LLM with the LLM component of the original VLM through evolutionary merging. Consistent with the findings we show in the previous discussions, simple merging without evolutionary search do not give strong performance as ours, see the last 3 rows in Table [5.](#page-15-0) Furthermore, our models' superior results on the JA-VLM-Bench-In-the-Wild compared to both baselines exhibits its adeptness at navigating culturally-specific content.\n\nBesides the quantitative results in Table [5,](#page-15-0) we qualitatively compare our VLM with the baseline models in Appendix [C.](#page-21-1) Our evolved model is able to handle Japanese culture-specific content remarkably well, generally producing more detailed responses with correct information.\n\nTable 5: Performance Comparison of the VLMs. LLaVA 1.6 Mistral 7B is the source VLM and Japanese Stable VLM is an open-sourced Japanese VLM. While JA-VG-VQA-500 measures general VQA abilities in Japanese, JA-VLM-Bench-In-the-Wild evaluates the model's handling of complex VQA tasks within Japanese cultural contexts.\n\n<span id=\"page-15-0\"></span>\n\n| Model                | Size | JA-VG-VQA-500<br>(ROUGE-L ↑) | JA-VLM-Bench-In-the-Wild<br>(ROUGE-L ↑) |\n|----------------------|------|------------------------------|-----------------------------------------|\n| LLaVA 1.6 Mistral 7B | 8B   | 14.3                         | 41.1                                    |\n| Japanese Stable VLM  | 8B   | -                            | 40.5                                    |\n| Ours (PS)            | 8B   | 19.7                         | 51.2                                    |\n| Ours (DFS)           | 12B  | 16.8                         | 46.5                                    |\n| Ours (PS+DFS)        | 11B  | 20.4                         | 47.6                                    |\n| TIES [63]            | 8B   | 16.0                         | 46.5                                    |\n| DARE-TIES [65]       | 8B   | 9.4                          | 36.0                                    |\n| Passthrough [21]     | 9B   | 7.3                          | 26.7                                    |\n\n## 4 Discussion\n\nIn this report, we propose a general method that uses evolutionary techniques to efficiently discover the best ways to combine different models from the vast ocean of different open-source models with diverse capabilities. By working with the vast collective intelligence of existing open models, our method is able to automatically create new foundation models with desired capabilities specified by the user. We find that our approach is able to automatically discover novel ways to merge different models from vastly different domains (e.g., non-English language and Math, or non-English language and Vision), in non-trivial ways that might be difficult for human experts to discover themselves.\n\nTo test our approach, we apply our method to automatically create a Japanese LLM capable of Math reasoning, and a culturally-specific content aware Japanese VLM. Surprisingly, we find that both models achieve state-of-the-art results on several LLM and Vision benchmarks, while not being explicitly optimized to be good at these benchmarks, attaining the top performance on a vast array of other Japanese LLM benchmarks, even exceeding the performance of some previous SOTA 70B parameter Japanese LLMs.\n\nWith these promising initial results, we believe we are just scratching the surface of unlocking the full capabilities of evolutionary model merging, and this is the inception of a long-term development of applying evolutionary principles to foundation model development.\n\nCurrently, we are already achieving promising results in applying evolutionary model merging to image diffusion models, enabling the creation of high performance cross-domain image generation models by merging existing building blocks in novel ways discovered by evolution.\n\nThe method currently requires the user to select a set of source models to use as ingredients for evolutionary search. We believe it is also possible to leverage evolution to search for candidate source models from a vast population of existing models as well. In addition to model selection, we are also exploring using evolution to produce swarms of diverse foundation models each with\n\nits own niche and behaviors. This holds the potential of enabling the emergence of a collective intelligence consisting of a swarm of models capable of self-improvement by continuously producing new complementary internal models of the world through interaction.\n\nRelated to our work is an experiment, called Automerger [\\[33\\]](#page-19-22), released at around the same time as this work. This interesting experiment works by selecting two random models from the top 20 models on the Open LLM Leaderboard [\\[26\\]](#page-19-1) and randomly apply SLERP [\\[60\\]](#page-20-2) or DARE-TIES [\\[63,](#page-20-3) [65\\]](#page-20-5) to create new models. Over time, some of these models will do well, or even better on the benchmark tasks that define this leaderboard, becoming part of the leaderboard. We predict this approach will lead to combinations of the merged models that overfit to the benchmark tasks defined on the leaderboard. The author acknowledged that the idea behind this project was less about creating better models, but more about getting more metrics to help derive a more principled approach to model merging.\n\nOur work takes an orthogonal approach of optimizing for tasks outside of the domain specified by the original leaderboard [\\[26\\]](#page-19-1), rather than being confined by it. As we have shown, surprisingly, stepping away from optimizing for a particular benchmark occasionally results in even greater generalization to numerous other benchmark tasks that we had not intended to optimize for, and such emergent generalization might be the key to unlocking the next great advancements in AI.\n\nThe ability to evolve new models with new emergent capabilities, from a large variety of existing, diverse models with various capabilities have important implications. With the rising costs and resource requirement for training foundation models, by leveraging the rich variety of foundation models in the rich open-source ecosystem, large institutions or governments may consider the cheaper evolutionary approach for developing proof-of-concept prototype models quickly, before committing substantial capital or tapping into the nation's resources to develop entirely custom models from scratch, if that is even needed at all.\n\nFurther Applications and Impact. After the release of the preprint version of this paper, researchers have explored evolutionary model merging in different domains, highlighting the method's versatility and effectiveness. A notable example is EvoSDXL [\\[2\\]](#page-18-18), which applied evolutionary model merging to diffusion image generation models. This proves that our method works well not just for LLMs and VLMs, but for other types of models as well. Moreover, what makes EvoSDXL particularly interesting is its success in merging SDXL-Lightning [\\[36\\]](#page-19-23) with other standard SDXL fine-tunes. SDXL-Lightning is a specialized variant of SDXL that employs an adversarial loss during training, enabling rapid image generation in just a few steps, compared to the 50 or 100 steps typically required by standard diffusion models. The evolutionary model merging technique effectively combined this unique model with conventional SDXL fine-tunes, despite the different protocols used in their development. This success illustrates that our method is capable of integrating models created through varying protocols, combining their strengths to create more robust and powerful models. Additionally, other unique models such as EvoVLM-JP-v2 [\\[4\\]](#page-18-19) and EvoUkiyoe [\\[3\\]](#page-18-20) have also been developed using evolutionary model merging, further demonstrating the method's potential and adaptability. Moreover, after the publication of the preprint, evolutionary model merging was implemented in two famous open-source software packages, MergeKit [\\[21\\]](#page-18-0) and Optuna Hub [\\[8\\]](#page-18-14). It has become widely available to many people, is being used practically, and further possibilities are being explored.\n\nLimitations. We acknowledge that although our evolutionary model merging effectively integrates diverse expertise from the source models, it also inherits their limitations. For instance, we encountered instances where the merged models produced responses that lacked logical coherence. Additionally, this study does not encompass instruction fine-tuning or alignment, raising the potential for the models to yield outputs that may be factually flawed.\n\nAuthor Contributions. Takuya Akiba initiated the \"Evolutionary Optimization of Model Merging Recipes\" project, wrote the project design document, and initiated the parameter space model merging experiments, laying the groundwork for the methodology. Makoto Shing expanded the parameter space model merging to encompass vision-language models and diffusion models. Yujin Tang directed the efforts in data flow space model merging by incorporating ideas and methods inspired from the neural architecture search and morphology evolution literature, establishing foundational methods in this domain and in hybrid merging strategies. Qi Sun contributed to the implementation of our parameter space model merging framework and assisted in model evaluation. David Ha provided overarching guidance for the research project, offering technical insight, advice, feedback and writing. Ethical and Societal Impact. Evolutionary model merging offers significant positive societal impacts by enabling the creation of small yet highly capable models at lower costs. This approach democratizes access to advanced AI capabilities, potentially reducing the environmental footprint of AI development and deployment. By efficiently combining existing models, it can lead to more accessible and versatile AI solutions, particularly benefiting regions and languages with limited resources. However, as with other model development techniques, this approach may present certain considerations. The combination of diverse models could potentially lead to unexpected behaviors or biases, and the complexity of merged models might affect their interpretability. While these challenges are common in AI development, they underscore the importance of continued research and evaluation. It's worth noting that the models and techniques presented in this work serve primarily as a proof of concept. For applications in mission-critical scenarios or models intended for wide public use, further verification and refinement of the methodology may be necessary. This ongoing improvement process is crucial for realizing the full potential of evolutionary model merging while ensuring responsible and ethical deployment.\n\n## Data Availability Statement\n\nAll datasets used in this paper are publicly available. Detailed sources are provided below:\n\n- LLM Experiments:\n\t- Data for Optimization: Available on Hugging Face at [https://huggingface.co/](https://huggingface.co/datasets/SakanaAI/gsm8k-ja-test_250-1319) [datasets/SakanaAI/gsm8k-ja-test\\\\_250-1319](https://huggingface.co/datasets/SakanaAI/gsm8k-ja-test_250-1319).\n\t- Data for Tests: Available on GitHub at [https://github.com/openai/](https://github.com/openai/grade-school-math) [grade-school-math](https://github.com/openai/grade-school-math).\n- VLM Experiments:\n\t- VLM Benchmark Datasets:\n\t\t- \\* JA-VG-VQA-500: Available on Hugging Face at [https://huggingface.co/](https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500) [datasets/SakanaAI/JA-VG-VQA-500](https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500).\n\t\t- \\* JA-VLM-Bench-In-the-Wild: Available on Hugging Face at [https://](https://huggingface.co/datasets/SakanaAI/JA-VLM-Bench-In-the-Wild) [huggingface.co/datasets/SakanaAI/JA-VLM-Bench-In-the-Wild](https://huggingface.co/datasets/SakanaAI/JA-VLM-Bench-In-the-Wild).\n\t- VLM Training Data for Optimization: This subset of the Japanese Visual Genome VQA dataset is also accessible on the JA-VG-VQA-500 page under the train split.\n\nFor detailed references to these datasets within our paper, see Section [3.1](#page-6-0) for LLM experiments and Section [3.3](#page-13-0) for VLM experiments.\n\n## References\n\n- <span id=\"page-18-17\"></span>[1] Open AI. 2023. GPT-4V(ision) System Card. [https://cdn.openai.com/papers/GPTV\\\\_System\\\\_](https://cdn.openai.com/papers/GPTV_System_Card.pdf) [Card.pdf](https://cdn.openai.com/papers/GPTV_System_Card.pdf)\n- <span id=\"page-18-18\"></span>[2] Sakana AI. 2024. EvoSDXL-JP-v1. <https://sakana.ai/evosdxl-jp/>.\n- <span id=\"page-18-20\"></span>[3] Sakana AI. 2024. EvoUkiyoe. <https://sakana.ai/evo-ukiyoe/>.\n- <span id=\"page-18-19\"></span>[4] Sakana AI. 2024. EvoVLM-JP-v2. <https://sakana.ai/evovlm-jp/>.\n- <span id=\"page-18-23\"></span>[5] Stability AI. 2024. Japanese Stable LM Beta. [https://ja.stability.ai/blog/](https://ja.stability.ai/blog/japanese-stable-lm-beta) [japanese-stable-lm-beta](https://ja.stability.ai/blog/japanese-stable-lm-beta).\n- <span id=\"page-18-21\"></span>[6] Stability AI. 2024. JP Language Model Evaluation Harness. [https://github.com/Stability-AI/](https://github.com/Stability-AI/lm-evaluation-harness/tree/jp-stable) [lm-evaluation-harness/tree/jp-stable](https://github.com/Stability-AI/lm-evaluation-harness/tree/jp-stable).\n- <span id=\"page-18-5\"></span>[7] Emanuele Aiello, Lili Yu, Yixin Nie, Armen Aghajanyan, and Barlas Oguz. 2023. Jointly training large autoregressive multimodal models. *arXiv preprint arXiv:2309.15564* (2023).\n- <span id=\"page-18-14\"></span>[8] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. 2019. Optuna: A Next-generation Hyperparameter Optimization Framework. In *Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining* (Anchorage, AK, USA) *(KDD '19)*. Association for Computing Machinery, New York, NY, USA, 2623–2631. [https://doi.org/10.1145/](https://doi.org/10.1145/3292500.3330701) [3292500.3330701](https://doi.org/10.1145/3292500.3330701)\n- <span id=\"page-18-11\"></span>[9] augmxnt. 2023. *shisa-gamma-7b*. HuggingFace. <https://hf.co/augmxnt/shisa-gamma-7b-v1>\n- <span id=\"page-18-4\"></span>[10] AUTOMATIC1111. 2022. Stable Diffusion WebUI. [https://github.com/AUTOMATIC1111/](https://github.com/AUTOMATIC1111/stable-diffusion-webui) [stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui).\n- <span id=\"page-18-15\"></span>[11] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. arXiv:2308.12966 [cs.CV]\n- <span id=\"page-18-12\"></span>[12] Ethan Chern, Haoyang Zou, Xuefeng Li, Jiewen Hu, Kehua Feng, Junlong Li, and Pengfei Liu. 2023. Generative AI for Math: Abel. <https://github.com/GAIR-NLP/abel>.\n- <span id=\"page-18-13\"></span>[13] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training Verifiers to Solve Math Word Problems. *CoRR* abs/2110.14168 (2021). arXiv:2110.14168 <https://arxiv.org/abs/2110.14168>\n- <span id=\"page-18-3\"></span>[14] Nico Daheim, Thomas Möllenhoff, Edoardo Ponti, Iryna Gurevych, and Mohammad Emtiyaz Khan. 2024. Model Merging by Uncertainty-Based Gradient Matching. In *The Twelfth International Conference on Learning Representations*. <https://openreview.net/forum?id=D7KJmfEDQP>\n- <span id=\"page-18-16\"></span>[15] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. arXiv:2305.06500 [cs.CV]\n- <span id=\"page-18-10\"></span>[16] Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan. 2002. A fast and elitist multiobjective genetic algorithm: NSGA-II. *IEEE transactions on evolutionary computation* 6, 2 (2002), 182–197.\n- <span id=\"page-18-1\"></span>[17] Gintare Karolina Dziugaite and Daniel M Roy. 2017. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. *arXiv preprint arXiv:1703.11008* (2017).\n- <span id=\"page-18-6\"></span>[18] Adam Gaier and David Ha. 2019. Weight agnostic neural networks. *Advances in neural information processing systems* 32 (2019).\n- <span id=\"page-18-22\"></span>[19] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation. <https://doi.org/10.5281/zenodo.10256836>\n- <span id=\"page-18-9\"></span>[20] Mor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav Goldberg. 2022. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. *arXiv preprint arXiv:2203.14680* (2022).\n- <span id=\"page-18-0\"></span>[21] Charles O. Goddard. 2024. mergekit. <https://github.com/arcee-ai/mergekit>\n- <span id=\"page-18-7\"></span>[22] David Ha, Andrew Dai, and Quoc V Le. 2016. Hypernetworks. *arXiv preprint arXiv:1609.09106* (2016).\n- <span id=\"page-18-8\"></span>[23] Nikolaus Hansen. 2006. The CMA evolution strategy: a comparing review. *Towards a new evolutionary computation: Advances in the estimation of distribution algorithms* (2006), 75–102.\n- <span id=\"page-18-2\"></span>[24] Sepp Hochreiter and Jürgen Schmidhuber. 1994. Simplifying neural nets by discovering flat minima. *Advances in neural information processing systems* 7 (1994).\n- <span id=\"page-19-8\"></span>[25] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Flat minima. *Neural computation* 9, 1 (1997), 1–42.\n- <span id=\"page-19-1\"></span>[26] HuggingFace. 2023. *Open LLM Leaderboard*. HuggingFace. [https://huggingface.co/spaces/](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) [HuggingFaceH4/open\\\\_llm\\\\_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n- <span id=\"page-19-10\"></span>[27] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. 2022. Editing models with task arithmetic. *arXiv preprint arXiv:2212.04089* (2022).\n- <span id=\"page-19-11\"></span>[28] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7B. arXiv:2310.06825 [cs.CL]\n- <span id=\"page-19-16\"></span>[29] Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hérve Jégou, and Tomas Mikolov. 2016. FastText.zip: Compressing text classification models. *arXiv preprint arXiv:1612.03651* (2016).\n- <span id=\"page-19-17\"></span>[30] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016. Bag of Tricks for Efficient Text Classification. *arXiv preprint arXiv:1607.01759* (2016).\n- <span id=\"page-19-5\"></span>[31] Jean Kaddour, Linqing Liu, Ricardo Silva, and Matt J Kusner. 2022. When do flat minima optimizers work? *Advances in Neural Information Processing Systems* 35 (2022), 16577–16595.\n- <span id=\"page-19-6\"></span>[32] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. 2017. On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima. In *International Conference on Learning Representations*. <https://openreview.net/forum?id=H1oyRlYgg>\n- <span id=\"page-19-22\"></span>[33] Maxime Labonne. 2024. Automerger Experiment. *Tweet Thread* (2024). [https://twitter.com/](https://twitter.com/maximelabonne/status/1767124527551549860) [maximelabonne/status/1767124527551549860](https://twitter.com/maximelabonne/status/1767124527551549860)\n- <span id=\"page-19-0\"></span>[34] Maxime Labonne. 2024. Merge Large Language Models with mergekit. *Hugging Face Blog* (2024). <https://huggingface.co/blog/mlabonne/merge-models>\n- <span id=\"page-19-18\"></span>[35] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. arXiv:2301.12597 [cs.CV]\n- <span id=\"page-19-23\"></span>[36] Shanchuan Lin, Anran Wang, and Xiao Yang. 2024. SDXL-Lightning: Progressive Adversarial Diffusion Distillation. *CoRR* abs/2402.13929 (2024). <https://doi.org/10.48550/ARXIV.2402.13929> arXiv:2402.13929\n- <span id=\"page-19-19\"></span>[37] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023. Improved Baselines with Visual Instruction Tuning. arXiv:2310.03744 [cs.CV]\n- <span id=\"page-19-21\"></span>[38] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. 2024. LLaVA-NeXT: Improved reasoning, OCR, and world knowledge. [https://llava-vl.github.io/](https://llava-vl.github.io/blog/2024-01-30-llava-next/) [blog/2024-01-30-llava-next/](https://llava-vl.github.io/blog/2024-01-30-llava-next/)\n- <span id=\"page-19-20\"></span>[39] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual Instruction Tuning. arXiv:2304.08485 [cs.CV]\n- <span id=\"page-19-15\"></span>[40] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023. WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct. *CoRR* abs/2308.09583 (2023). [https:](https://doi.org/10.48550/ARXIV.2308.09583) [//doi.org/10.48550/ARXIV.2308.09583](https://doi.org/10.48550/ARXIV.2308.09583) arXiv:2308.09583\n- <span id=\"page-19-9\"></span>[41] Michael S Matena and Colin A Raffel. 2022. Merging models with fisher-weighted averaging. *Advances in Neural Information Processing Systems* 35 (2022), 17703–17716.\n- <span id=\"page-19-13\"></span>[42] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in GPT. *Advances in Neural Information Processing Systems* 35 (2022), 17359–17372.\n- <span id=\"page-19-14\"></span>[43] nostalgebraist. 2021. Interpreting GPT: The Logit Lens. [https://www.lesswrong.com/posts/](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens) [AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens). Accessed: 2024-03-08.\n- <span id=\"page-19-7\"></span>[44] Henning Petzka, Michael Kamp, Linara Adilova, Cristian Sminchisescu, and Mario Boley. 2021. Relative Flatness and Generalization. In *Advances in Neural Information Processing Systems*, A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (Eds.). [https://openreview.net/forum?id=](https://openreview.net/forum?id=sygvo7ctb_) [sygvo7ctb\\\\_](https://openreview.net/forum?id=sygvo7ctb_)\n- <span id=\"page-19-3\"></span>[45] Colin Raffel. 2021. A call to build models like we build open-source software. [https://colinraffel.](https://colinraffel.com/blog/a-call-to-build-models-like-we-build-open-source-software.html) [com/blog/a-call-to-build-models-like-we-build-open-source-software.html](https://colinraffel.com/blog/a-call-to-build-models-like-we-build-open-source-software.html).\n- <span id=\"page-19-4\"></span>[46] Colin Raffel. 2023. Building machine learning models like open source software. *Commun. ACM* 66, 2 (2023), 38–40.\n- <span id=\"page-19-2\"></span>[47] Eric Raymond. 1999. The cathedral and the bazaar. *Knowledge, Technology & Policy* 12, 3 (1999), 23–49.\n- <span id=\"page-19-12\"></span>[48] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. 2019. Regularized evolution for image classifier architecture search. In *Proceedings of the aaai conference on artificial intelligence*, Vol. 33. 4780–4789.\n- <span id=\"page-20-17\"></span>[49] rinna. 2024. LM Benchmark. [https://rinnakk.github.io/research/benchmarks/lm/index.](https://rinnakk.github.io/research/benchmarks/lm/index.html) [html](https://rinnakk.github.io/research/benchmarks/lm/index.html).\n- <span id=\"page-20-1\"></span>[50] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. Highresolution image synthesis with latent diffusion models. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*. 10684–10695.\n- <span id=\"page-20-12\"></span>[51] Jürgen Schmidhuber. 1992. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. *Neural Computation* 4, 1 (1992), 131–139.\n- <span id=\"page-20-13\"></span>[52] Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. 2023. Language models are multilingual chain-of-thought reasoners. In *The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023*. OpenReview.net. [https://openreview.net/](https://openreview.net/pdf?id=fR3wGCk-IXp) [pdf?id=fR3wGCk-IXp](https://openreview.net/pdf?id=fR3wGCk-IXp)\n- <span id=\"page-20-15\"></span>[53] Nobuyuki Shimizu, Na Rong, and Takashi Miyazaki. 2018. Visual Question Answering Dataset for Bilingual Image Understanding: A Study of Cross-Lingual Transfer Using Attention Maps. In *Proceedings of the 27th International Conference on Computational Linguistics* (Santa Fe, New Mexico, USA). Association for Computational Linguistics, 1918–1928. <http://aclweb.org/anthology/C18-1163>\n- <span id=\"page-20-16\"></span>[54] Makoto Shing and Takuya Akiba. 2023. Japanese Stable VLM. [https://huggingface.co/](https://huggingface.co/stabilityai/japanese-stable-vlm) [stabilityai/japanese-stable-vlm](https://huggingface.co/stabilityai/japanese-stable-vlm)\n- <span id=\"page-20-8\"></span>[55] David So, Quoc Le, and Chen Liang. 2019. The evolved transformer. In *International conference on machine learning*. PMLR, 5877–5886.\n- <span id=\"page-20-9\"></span>[56] Kenneth O Stanley and Risto Miikkulainen. 2002. Evolving neural networks through augmenting topologies. *Evolutionary computation* 10, 2 (2002), 99–127.\n- <span id=\"page-20-11\"></span>[57] Qi Sun, Marc Pickett, Aakash Kumar Nain, and Llion Jones. 2024. Transformer Layers as Painters. *arXiv preprint arXiv:2407.09298* (2024).\n- <span id=\"page-20-6\"></span>[58] Yi-Lin Sung, Linjie Li, Kevin Lin, Zhe Gan, Mohit Bansal, and Lijuan Wang. 2023. An empirical study of multimodal model merging. *arXiv preprint arXiv:2304.14933* (2023).\n- <span id=\"page-20-14\"></span>[59] Yujin Tang, Yingtao Tian, and David Ha. 2022. EvoJAX: Hardware-Accelerated Neuroevolution. *arXiv preprint arXiv:2202.05008* (2022).\n- <span id=\"page-20-2\"></span>[60] Tom White. 2016. Sampling generative networks. *arXiv preprint arXiv:1609.04468* (2016).\n- <span id=\"page-20-0\"></span>[61] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. 2022. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In *International Conference on Machine Learning*. PMLR, 23965–23998.\n- <span id=\"page-20-4\"></span>[62] Prateek Yadav, Leshem Choshen, Colin Raffel, and Mohit Bansal. 2023. Compeft: Compression for communicating parameter efficient updates via sparsification and quantization. *arXiv preprint arXiv:2311.13171* (2023).\n- <span id=\"page-20-3\"></span>[63] Prateek Yadav, Derek Tam, Leshem Choshen, Colin A. Raffel, and Mohit Bansal. 2023. TIES-Merging: Resolving Interference When Merging Models. In *Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023*, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (Eds.). [http://papers.nips.cc/paper\\\\_files/paper/2023/](http://papers.nips.cc/paper_files/paper/2023/hash/1644c9af28ab7916874f6fd6228a9bcf-Abstract-Conference.html) [hash/1644c9af28ab7916874f6fd6228a9bcf-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/1644c9af28ab7916874f6fd6228a9bcf-Abstract-Conference.html)\n- <span id=\"page-20-10\"></span>[64] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. 2024. Yi: Open foundation models by 01. ai. *arXiv preprint arXiv:2403.04652* (2024).\n- <span id=\"page-20-5\"></span>[65] Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. 2024. Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch. arXiv:2311.03099 [cs.CL]\n- <span id=\"page-20-7\"></span>[66] Barret Zoph and Quoc V Le. 2016. Neural architecture search with reinforcement learning. *arXiv preprint arXiv:1611.01578* (2016).\n\n## SUPPLEMENTARY INFORMATION\n\n## <span id=\"page-21-0\"></span>A Evaluation Details\n\nIn Table [1,](#page-7-0) GPT-3.5 is gpt-3.5-turbo-0613, GPT-4 is gpt-4-0125-preview, Llama 2 70B is Llama-2-70b-chat, Japanese Stable LM 70B is japanese-stablelm-instruct-beta-70b and Swallow 70B is Swallow-70b-instruct-hf.\n\nFor the evaluation using the Japanese LM Evaluation Harness [\\[6\\]](#page-18-21), we utilized Stability AI Japan's fork of lm-eval-harness [\\[19\\]](#page-18-22) and configured it according to their convention. This configuration is widely used and compatible with the results on their report [\\[5\\]](#page-18-23) and Rinna leaderboards [\\[49\\]](#page-20-17), thus allowing direct comparison of scores with a large number of Japanese LLMs.\n\nTable [6](#page-22-0) is the full version of Table [2,](#page-8-0) allowing for comparisons between our models and a wider range of models. Some of the numbers are from the Stability AI Japan report and the Rinna leaderboard. Despite having only 7B to 10B parameters, our models achieve higher scores than all existing <70B parameters models and exhibit scores on par with the SOTA 70B models.\n\n## <span id=\"page-21-2\"></span>B Evolving for License Specific Open-Source Models\n\nIn the main section, our EvoLLM-JP results were evolved using models found on HuggingFace. However, some of the models uses, in particular, WizardMath-7B-V1.1 [\\[40\\]](#page-19-15) has been released under a Non-Commercial, Research-only, Microsoft License, which is not *truly* open-source. Therefore, our release of EvoLLM-JP is also released under a Non-Commercial, Research-only License to be consistent with the WizardMath-7B-V1.1 model.\n\nAs researchers who benefited from the open-source community, we would like for models that we release to also be under an open-source license. In the spirit of open-source, and to showcase the applicability of our method to tackle even challenging issues like model licenses. We have ran a similar experiment where we incorporated *only* models that have been released under a true opensource license, such as MIT or Apache 2.0, and have produced a similar performing model called EvoLLM-JP-A, which we will release under Apache 2.0. This model will also be released on our GitHub: <https://github.com/SakanaAI/>.\n\nSpecifically, our EvoLLM-JP-A is a merge of shisa-gamma-7b-v1, Arithmo2-Mistral-7B, and Abel-7B-002, all of which are under MIT or Apache 2.0 License. The MGSM-JA score measured using the protocol described in Section [3.1](#page-6-0) is 52.4, and the Japanese Language Model Evaluation Harness score is 69.0. We have included results of this Apache 2.0-licensed model for comparison in Table [6,](#page-22-0) which provides a more comprehensive comparison than Table [2](#page-8-0) in the main text.\n\n## <span id=\"page-21-1\"></span>C Case Study\n\nTable [7](#page-24-0) provides an example of responses to a mathematical question by existing models and our model. By merging a Japanese language model, we not only improve our capability to understand and use Japanese in reading and writing but also expand our knowledge about Japan. This example requires both mathematical reasoning and Japanese-specific knowledge, specifically that Setsubun is the day before the beginning of spring (Risshun). Notably, only our merged model provides the correct answer. Even when the question is translated into English, the English math model WizardMath-7B-V1.1 fails to answer correctly, demonstrating that our merged model is superior to a combination of machine translation and an English math model.\n\nIt is worth noting that in Table [7,](#page-24-0) we also provide the answer from the general-purpose math model WizardMath-7B-V1.1 with the question translated into English. However, even with the translated question, WizardMath-7B-V1.1 fails to provide the correct answer. This highlights the fact that simply translating the question is not sufficient to solve the problem, as it also requires an understanding of the Japanese cultural context. Even if one were to consider a combination of machine translation and an English math model as an alternative approach, it would still fall short in capturing the nuances and context-specific knowledge required to answer the question correctly. This demonstrates the unique value of our merged model, which effectively combines mathematical reasoning capabilities with Japanese language understanding and cultural knowledge.\n\nTable 6: Breakdown of JP-LMEH Scores for Japanese Language Proficiency (Full Version of Table [2\\)](#page-8-0). JP-LMEH (Japanese Language Model Evaluation Harness) is a benchmark suite consisting of 9 tasks, and the average score (Avg column) is used as an indicator of overall Japanese language proficiency.\n\n<span id=\"page-22-0\"></span>\n\n| Model                                                                          |            | Size JComQA JNLI MARC JSQuAD JAQKET XLSum XWino MGSM JCoLA Avg |              |              |              | JP Language Model Evaluation Harness |              |              |              |                        |\n|--------------------------------------------------------------------------------|------------|----------------------------------------------------------------|--------------|--------------|--------------|--------------------------------------|--------------|--------------|--------------|------------------------|\n| 7B source models:                                                              |            |                                                                |              |              |              |                                      |              |              |              |                        |\n| Mistral-7B-v0.1                                                                | 7B         | 74.0                                                           | 44.2         | 63.3         | 85.7         | 69.8                                 | 22.8         | 69.9         | 20.4         | 51.1 55.7              |\n| shisa-gamma-7b-v1                                                              | 7B         | 91.2                                                           | 72.1         | 94.6         | 73.9         | 68.0                                 | 25.9         | 80.5         | 29.6         | 58.7 66.1              |\n| WizardMath-7B-V1.1                                                             | 7B         | 74.7                                                           | 42.7         | 90.4         | 84.6         | 68.5                                 | 22.3         | 69.8         | 38.8         | 48.9 60.1              |\n| Abel-7B-002                                                                    | 7B         | 70.3                                                           | 51.8         | 62.3         | 83.8         | 69.0                                 | 22.5         | 68.2         | 28.0         | 52.7 56.5              |\n| 7B merged models:                                                              |            |                                                                |              |              |              |                                      |              |              |              |                        |\n| Ours (PS)<br>Ours (DFS)                                                        | 7B<br>10B  | 89.1<br>67.7                                                   | 65.7<br>58.2 | 95.4<br>53.5 | 89.5<br>66.8 | 77.7<br>54.3                         | 25.5<br>17.3 | 81.2<br>65.6 | 50.0<br>30.0 | 60.5 70.5<br>65.6 53.2 |\n| Ours (PS+DFS)                                                                  | 10B        | 88.2                                                           | 50.3         | 91.5         | 78.6         | 77.8                                 | 23.2         | 73.0         | 40.0         | 73.0 66.2              |\n| Ours (PS-A; see Appendix B)                                                    | 7B         | 87.0                                                           | 62.3         | 91.9         | 90.4         | 77.4                                 | 23.0         | 78.9         | 46.4         | 63.6 69.0              |\n| 13B source models:                                                             |            |                                                                |              |              |              |                                      |              |              |              |                        |\n| Llama-2-13b-hf                                                                 | 13B        | 78.3                                                           | 43.2         | 50.3         | 89.0         | 75.8                                 | 25.7         | 63.4         | 10.4         | 50.6 54.1              |\n| MetaMath-13B-V1.0<br>ELYZA-japanese-Llama-2-13b-instruct                       | 13B<br>13B | 65.6<br>84.6                                                   | 35.5<br>58.3 | 51.3<br>83.5 | 78.5<br>83.8 | 56.7<br>63.3                         | 22.4<br>25.0 | 61.8<br>74.3 | 17.2<br>18.0 | 49.3 48.7<br>50.5 60.2 |\n|                                                                                |            |                                                                |              |              |              |                                      |              |              |              |                        |\n| 13B merged models:<br>Ours (PS)                                                | 13B        | 82.7                                                           | 49.8         | 83.1         | 88.3         | 68.0                                 | 24.4         | 62.3         | 24.8         | 54.3 59.7              |\n| Ours (DFS)                                                                     | 21B        | 62.1                                                           | 36.3         | 55.6         | 76.4         | 56.7                                 | 15.9         | 60.8         | 6.4          | 49.6 46.6              |\n| Ours (PS+DFS)                                                                  | 21B        | 82.2                                                           | 48.9         | 88.7         | 86.7         | 68.9                                 | 18.1         | 62.0         | 30.8         | 57.1 60.4              |\n| Other models:                                                                  |            |                                                                |              |              |              |                                      |              |              |              |                        |\n| Swallow-70b-instruct-hf                                                        | 70B        | 95.3                                                           | 57.2         | 91.7         | 94.1         | 93.9                                 | 23.1         | 83.3         | 45.2         | 59.5 71.5              |\n| Swallow-70b-hf                                                                 | 70B        | 94.8                                                           | 55.8         | 87.8         | 93.5         | 93.8                                 | 23.2         | 83.7         | 47.2         | 60.7 71.2              |\n| japanese-stablelm-base-beta-70b<br>nekomata-14b-instruction                    | 70B<br>14B | 93.7<br>93.7                                                   | 45.2<br>57.5 | 91.0<br>90.0 | 94.2<br>93.6 | 92.8<br>89.0                         | 25.4<br>25.2 | 83.6<br>77.4 | 41.2<br>35.2 | 63.8 70.1<br>67.3 69.9 |\n| japanese-stablelm-instruct-beta-70b                                            | 70B        | 91.2                                                           | 50.4         | 92.9         | 87.1         | 88.4                                 | 24.3         | 82.0         | 37.2         | 61.7 68.3              |\n| nekomata-14b                                                                   | 14B        | 93.1                                                           | 41.7         | 84.1         | 94.2         | 90.8                                 | 21.3         | 78.6         | 37.6         | 65.0 67.4              |\n| youri-7b-chat                                                                  | 7B         | 91.8                                                           | 70.3         | 96.7         | 79.6         | 83.7                                 | 24.2         | 80.9         | 25.2         | 53.8 67.4              |\n| Llama-2-70b-hf<br>nekomata-7b-instruction                                      | 70B<br>7B  | 89.3<br>88.0                                                   | 53.0<br>74.7 | 70.4<br>92.3 | 93.2<br>91.3 | 89.5<br>82.5                         | 23.8<br>21.1 | 79.8<br>76.8 | 40.0<br>19.2 | 65.4 67.2<br>53.8 66.6 |\n| youri-7b-instruction                                                           | 7B         | 88.8                                                           | 63.6         | 93.8         | 92.2         | 83.9                                 | 24.7         | 78.9         | 17.2         | 54.0 66.3              |\n| Qwen-14B                                                                       | 14B        | 89.5                                                           | 65.2         | 76.7         | 93.0         | 79.5                                 | 18.4         | 71.6         | 37.2         | 61.0 65.8              |\n| Swallow-MX-8x7b-NVE-v0.1                                                       | 47B        | 92.0                                                           | 52.4         | 59.8         | 93.1         | 91.9                                 | 26.1         | 72.9         | 44.8         | 59.2 65.8              |\n| youri-7b-chat-gptq<br>nekomata-7b-instruction-gguf                             | 7B<br>7B   | 89.5<br>87.9                                                   | 68.0<br>68.4 | 96.0<br>89.6 | 88.6<br>89.9 | 79.7<br>79.8                         | 15.6<br>20.8 | 79.1<br>77.3 | 21.6<br>19.2 | 53.8 65.8<br>55.3 65.3 |\n| nekomata-14b-instruction-gguf                                                  | 14B        | 93.3                                                           | 57.8         | 51.7         | 91.3         | 86.2                                 | 23.6         | 77.4         | 35.2         | 68.7 65.0              |\n| Llama-2-70b-chat-hf                                                            | 70B        | 80.2                                                           | 53.4         | 94.4         | 91.6         | 80.1                                 | 21.8         | 73.6         | 30.4         | 54.6 64.5              |\n| youri-7b-instruction-gptq                                                      | 7B         | 88.1                                                           | 62.0         | 93.5         | 90.6         | 74.4                                 | 15.3         | 78.2         | 15.2         | 53.3 63.4              |\n| japanese-stablelm-base-gamma-7b                                                | 7B         | 89.5                                                           | 37.6         | 79.2         | 90.6         | 89.5                                 | 22.9         | 82.9         | 20.8         | 55.6 63.2              |\n| Swallow-13b-instruct-hf<br>japanese-stablelm-instruct-gamma-7b                 | 13B<br>7B  | 89.0<br>91.4                                                   | 43.1<br>40.7 | 74.1<br>72.9 | 92.8<br>89.9 | 92.5<br>88.2                         | 20.6<br>22.8 | 70.4<br>82.2 | 24.0<br>17.2 | 61.9 63.2<br>56.4 62.4 |\n| Swallow-13b-hf                                                                 | 13B        | 86.0                                                           | 44.8         | 74.5         | 92.2         | 92.2                                 | 21.6         | 71.4         | 19.2         | 59.5 62.4              |\n| nekomata-14b-gguf                                                              | 14B        | 93.0                                                           | 41.1         | 50.0         | 93.0         | 88.3                                 | 22.2         | 80.5         | 32.4         | 60.2 62.3              |\n| Swallow-MS-7b-v0.1                                                             | 7B         | 88.3                                                           | 35.8         | 83.1         | 90.3         | 89.4                                 | 20.7         | 74.6         | 22.4         | 55.5 62.2              |\n| Swallow-7b-instruct-hf<br>llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0 13B | 7B         | 86.8<br>83.2                                                   | 36.3<br>74.5 | 94.2<br>74.9 | 90.0<br>94.0 | 89.4<br>85.1                         | 17.4<br>9.6  | 76.8<br>71.3 | 10.4<br>3.2  | 50.0 61.2<br>50.4 60.7 |\n| Swallow-7b-hf                                                                  | 7B         | 80.2                                                           | 37.9         | 93.1         | 89.2         | 90.1                                 | 18.3         | 76.6         | 10.4         | 50.0 60.7              |\n| nekomata-7b                                                                    | 7B         | 82.9                                                           | 38.6         | 67.0         | 89.8         | 82.5                                 | 16.9         | 74.5         | 16.0         | 60.0 58.7              |\n| japanese-stablelm-instruct-beta-7b<br>japanese-stablelm-base-beta-7b           | 7B<br>7B   | 81.9<br>76.8                                                   | 39.9<br>38.0 | 88.3<br>86.0 | 84.1<br>87.1 | 78.2<br>79.5                         | 20.8<br>21.9 | 75.7<br>76.6 | 4.8<br>7.6   | 51.7 58.4<br>49.2 58.1 |\n| Qwen-7B                                                                        | 7B         | 76.0                                                           | 46.9         | 83.3         | 89.7         | 69.2                                 | 13.8         | 60.6         | 22.0         | 57.3 57.6              |\n| youri-7b-gptq                                                                  | 7B         | 78.9                                                           | 40.5         | 78.2         | 85.7         | 78.1                                 | 16.3         | 78.3         | 4.8          | 54.0 57.2              |\n| youri-7b                                                                       | 7B         | 82.8                                                           | 35.9         | 90.1         | 57.2         | 82.5                                 | 20.5         | 78.0         | 6.8          | 58.5 56.9              |\n| ELYZA-japanese-Llama-2-7b-instruct<br>weblab-10b-instruction-sft               | 7B<br>10B  | 79.2<br>63.6                                                   | 38.8<br>42.7 | 89.9<br>86.4 | 83.2<br>87.8 | 68.7<br>73.9                         | 16.8<br>23.1 | 71.1<br>72.4 | 11.2<br>2.8  | 50.5 56.6<br>50.5 55.9 |\n| ELYZA-japanese-Llama-2-7b                                                      | 7B         | 75.2                                                           | 36.1         | 87.3         | 86.4         | 68.0                                 | 16.3         | 72.0         | 5.2          | 53.1 55.5              |\n| nekomata-7b-gguf                                                               | 7B         | 82.3                                                           | 33.7         | 50.0         | 86.8         | 74.9                                 | 19.9         | 78.3         | 13.6         | 55.8 55.0              |\n| japanese-stablelm-instruct-ja_vocab-beta-7b                                    | 7B         | 79.9                                                           | 39.2         | 79.5         | 82.0         | 78.9                                 | 9.9          | 68.9         | 6.0          | 49.2 54.8              |\n| japanese-stablelm-base-ja_vocab-beta-7b<br>calm2-7b-chat                       | 7B<br>7B   | 69.9<br>70.3                                                   | 38.8<br>33.7 | 80.0<br>87.7 | 83.8<br>82.6 | 80.1<br>77.8                         | 7.5<br>5.4   | 72.5<br>70.9 | 3.6<br>5.6   | 50.0 54.0<br>51.9 54.0 |\n| stockmark-13b                                                                  | 13B        | 70.7                                                           | 36.2         | 56.5         | 87.2         | 87.3                                 | 7.7          | 73.4         | 9.6          | 50.0 53.2              |\n| Llama-2-7b-hf                                                                  | 7B         | 65.9                                                           | 35.1         | 83.2         | 82.7         | 70.6                                 | 19.0         | 65.2         | 6.8          | 47.1 52.9              |\n| calm2-7b                                                                       | 7B         | 58.2                                                           | 33.3         | 81.2         | 81.8         | 81.9                                 | 2.4          | 73.0         | 5.6          | 53.3 52.3              |\n| plamo-13b<br>weblab-10b                                                        | 13B<br>10B | 54.5<br>44.0                                                   | 33.8<br>30.7 | 85.0<br>64.0 | 78.6<br>83.3 | 74.5<br>74.6                         | 12.9<br>20.3 | 70.5<br>71.4 | 2.4<br>2.0   | 54.5 51.9<br>50.8 49.0 |\n| llm-jp-13b-v1.0                                                                | 13B        | 39.6                                                           | 35.0         | 67.4         | 79.3         | 78.0                                 | 10.9         | 69.5         | 2.0          | 54.4 48.5              |\n| plamo-13b-instruct-nc                                                          | 13B        | 45.8                                                           | 35.3         | 91.6         | 75.2         | 63.5                                 | 5.8          | 65.2         | 1.2          | 50.6 48.3              |\n| plamo-13b-instruct                                                             | 13B        | 42.5                                                           | 34.9         | 90.0         | 77.3         | 67.0                                 | 5.4          | 62.9         | 2.0          | 50.0 48.0              |\n\nOn the other hand, Table [8](#page-25-0) highlights an instance where the LLMs failed to solve a problem correctly, using a question from the MGSM test set. The problem involved calculating the internal volume of boxes. When presented in Japanese, all models encountered difficulties in correctly accounting for the wall thickness. EvoLLM-JP-v1-7B attempted to adjust for it but made an arithmetic error, leading to an incorrect volume of 351 cubic inches. shisa-gamma-7b-v1 ignored the wall thickness entirely, calculating the external dimensions only, and thus arrived at 360 cubic inches. WizardMath-7B-V1.1 recognized the need for an adjustment but applied it incorrectly, resulting in 288 cubic inches. Notably, when the original English problem was presented to WizardMath-7B-V1.1, it produced the correct answer of 72 cubic inches. This suggests that there may still be some loss in transferring the model's high mathematical reasoning abilities from English to Japanese.\n\nAdditionally, Table [9](#page-26-0) provides the case studies for our VLM. We observe that our VLM successfully obtained knowledge, cultural understanding, and fluency in Japanese expression of the Japanese LLM.\n\nThe first example features a photo of Koi-nobori, a cultural tradition specific to Japan. LLaVA-1.6-Mistral-7B, lacking sufficient knowledge of Japanese culture, failed to answer Koinobori correctly. In contrast, both Japanese-Stable-VLM and EvoVLM-JP, possessing knowledge of Japanese culture, successfully identified the Koi-nobori. Notably, EvoVLM-JP's response demonstrates greater fluency in its explanation.\n\nThe third example involves a question about the color of traffic lights. In Japan, the green traffic light is called \"blue\" because historically, the word \"blue\" referred to a range of colors, including blue and green, and also because the initial traffic lights used in Japan had a bluish-green hue, leading to the continuation of this terminology today. EvoVLM-JP is the only model that correctly identifies the traffic light color as \"blue\" in Japanese, demonstrating its accurate understanding of this cultural and linguistic nuance. LLaVA-1.6-Mistral-7B consistently but incorrectly labels the light as \"green\" in Japanese, revealing its lack of specific cultural knowledge. Japanese-Stable-VLM shows inconsistent responses, alternating between \"green,\" \"green and white,\" and \"white,\" indicating confusion. These results highlight EvoVLM-JP's superior grasp of Japanese language conventions and cultural context in this specific scenario.\n\nTable 7: Case Study of EvoLLM-JP-v1-7B. This study presents the responses of the LLMs to a problem that demands both Japanese-specific knowledge and mathematical reasoning.\n\n<span id=\"page-24-0\"></span>\n\n|                                      | Problem                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |  |\n|--------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|\n| JA Instruction                       | 去年の立春は2月3日でした。今年の立春は2月4日です。去年の節分の月、去年の節分の<br>日、今年の節分の月、今年の節分の日を足し合わせると、いくつになりますか?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |  |\n| EN Instruction                       | Last year, Risshun was on February 3rd. This year, it is on February 4th. What is the sum of the month<br>and day of last year's Setsubun, and this year's Setsubun?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |  |\n| Ground Truth Answer                  | 9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |  |\n| Note                                 | Setsubun, the eve of the first day of spring, always falls on the day before Risshun, which marks the<br>start of spring according to the lunar calendar. The date of Risshun can vary annually.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |  |\n|                                      | Answers                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |  |\n| EvoLLM-JP-v1-7B<br>JA Instruction    | 去年の立春は2月3日で、今年の立春は2月4日です。 去年の節分は立春の前日、つま<br>り2月2日でした。 今年の節分は立春の前日、つまり2月3日です。 去年の節分の月は2月、<br>去年の節分の日は2日で、合計は2 + 2 = 4です。 今年の節分の月は2月、今年の節分の日<br>は3日で、合計は2 + 3 = 5です。 足し合わせると、4 + 5 = 9になります。<br>#### 9<br>The answer is: 9.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |  |\n| shisa-gamma-7b<br>JA Instruction     | まず、去年の立春の日付を計算してみましょう。<br>去年の立春の日付 = 2021年の2月3日<br>次に、去年の節分の日付を計算してみましょう。<br>去年の節分の日付 = 2021年の2月2日<br>次に、今年の立春の日付を計算してみましょう。<br>今年の立春の日付 = 2022年の2月4日<br>次に、今年の節分の日付を計算してみましょう。<br>今年の節分の日付 = 2022年の2月1日<br>最後に、去年の節分の月と日、今年の節分の月と日を足し合わせてみましょう。<br>去年の節分の月と日 = 2021年の2月2日 今年の節分の月と日 = 2022年の2月1日<br>これで、足し合わせることができます。<br>足し合わせた月と日 = 2021年の2月2日 + 2022年の2月1日 = 2023年の2月3日<br>つまり、去年の節分の月と日、今年の節分の月と日を足し合わせると、2023年の2月3日に<br>なります。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |  |\n| WizardMath-7B-V1.1<br>JA Instruction | Step 1: Identify the dates for the given events.<br>- Last year's Setsubun (February 3)<br>- This year's Setsubun (February 4)<br>Step 2: Determine the dates for the corresponding events in the previous and next years.<br>- Last year's Setsubun (February 3) falls on a Monday.<br>- This year's Setsubun (February 4) falls on a Tuesday.<br>Step 3: Calculate the number of days between the corresponding events.<br>- The number of days between last year's and this year's Setsubun is 1 day (Tuesday - Monday = 1 day).<br>Step 4: Determine the dates for the corresponding events in the lunar calendar.<br>- Last year's Setsubun was on the 1st day of the 2nd month.<br>- This year's Setsubun will be on the 2nd day of the 2nd month.<br>Step 5: Calculate the number of days between the corresponding events in the lunar calendar.<br>- The number of days between last year's and this year's Setsubun in the lunar calendar is 1 day (2nd<br>day - 1st day = 1 day).<br>Step 6: Add the number of days between the corresponding events in both calendars.<br>- The total number of days between last year's and this year's Setsubun is 1 (from the solar calendar) + 1<br>(from the lunar calendar) = 2 days.<br>Step 7: Write the final answer.<br>- The sum of the dates for the corresponding events is 2.<br>The answer is: 2. |  |\n| WizardMath-7B-V1.1<br>EN Instruction | We are given that last year, Risshun (the beginning of spring according to the lunisolar Japanese<br>calendar) was on February 3rd, and this year it is on February 4th. We need to find the sum of the month<br>and day of last year's Setsubun, and this year's Setsubun.<br>Step 2: Identify the month and day of last year's Setsubun. Last year, Risshun was on February 3rd. So,<br>the month is February (2) and the day is 3.<br>Step 3: Identify the month and day of this year's Setsubun. This year, Risshun is on February 4th. So,<br>the month is February (2) and the day is 4.<br>Step 4: Calculate the sum of the month and day of last year's and this year's Setsubun. The sum of<br>the month and day of last year's Setsubun is 2 + 3 = 5. The sum of the month and day of this year's<br>Setsubun is 2 + 4 = 6.<br>Step 5: Add the two sums together. The total sum of the month and day of last year's and this year's<br>Setsubun is 5 + 6 = 11.<br>So, the sum of the month and day of last year's Setsubun, and this year's Setsubun is 11.<br>The answer is: 11.                                                                                                                                                                                                                                                                  |  |\n| Problem                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |  |\n| JA Instruction                       | ジョンは3箱持っています。それぞれの箱は5インチ×6インチ×4インチです。壁は1インチ<br>の暑さです。3箱全部の内容量はどれくらいですか?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |  |\n| EN Instruction                       | John has 3 boxes. Each box is 5 inches by 6 inches by 4 inches. The walls are 1 inch thick. What is the<br>total inner volume of all 3 boxes?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |  |\n| Ground Truth Answer                  | 72                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |  |\n| Answers                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |  |\n| EvoLLM-JP-v1-7B<br>JA Instruction    | 1箱の内容量は、長さ×幅×高さ = 5インチ×6インチ×4インチ = 120立方インチです。 ジョ<br>ンは3箱持っているので、3箱の内容量は、3×120立方インチ = 360立方インチです。 しか<br>し、壁の厚みは1インチなので、実際の内容量は、360立方インチ - 3箱×1インチ×3面 =<br>360立方インチ - 9立方インチ = 351立方インチです。<br>#### 351<br>指示: 351                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |  |\n| shisa-gamma-7b<br>JA Instruction     | ジョンは3箱持っています。それぞれの箱は5インチ×6インチ×4インチです。壁は1インチ<br>の暑さです。3箱全部の内容量はどれくらいですか? [/INST] 3つの箱の体積の合計を求める<br>には、それぞれの箱の体積を計算してからそれらを合計する必要があります。<br>箱1の体積 = 長さ * 幅 * 高さ = 5インチ * 6インチ * 4インチ = 120立方インチ<br>箱2の体積 = 長さ * 幅 * 高さ = 5インチ * 6インチ * 4インチ = 120立方インチ<br>箱3の体積 = 長さ * 幅 * 高さ = 5インチ * 6インチ * 4インチ = 120立方インチ<br>これで、3つの箱の体積を合計することができます。<br>合計体積 = 箱1の体積 + 箱2の体積 + 箱3の体積 = 120立方インチ + 120立方インチ + 120立方<br>インチ = 360立方インチ<br>つまり、3つの箱の合計体積は360立方インチです。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |  |\n| WizardMath-7B-V1.1<br>JA Instruction | Let's think step by step.<br>Step 1: Calculate the volume of one box. The volume of a box is calculated by multiplying its length,<br>width, and height. In this case, the dimensions of the box are 5 inches by 6 inches by 4 inches. So, the<br>volume of one box is 5 inches * 6 inches * 4 inches = 120 cubic inches.<br>Step 2: Calculate the total volume of all boxes. John has 3 boxes, so the total volume of all boxes is 3<br>boxes * 120 cubic inches/box = 360 cubic inches.<br>Step 3: Adjust for the thickness of the walls. The walls of the boxes add an extra inch to each dimension,<br>so we need to subtract the volume of the walls from the total volume of the boxes. The volume of the<br>walls of one box is 1 inch * 6 inches * 4 inches = 24 cubic inches. Since there are 3 boxes, the total<br>volume of the walls is 3 boxes * 24 cubic inches/box = 72 cubic inches.<br>Step 4: Subtract the volume of the walls from the total volume of the boxes. The adjusted total volume<br>of the boxes is 360 cubic inches - 72 cubic inches = 288 cubic inches.<br>So, the total volume of the contents of the 3 boxes is 288 cubic inches.<br>The answer is: 288 cubic inches.                                                                                                                                                     |  |\n| WizardMath-7B-V1.1<br>EN Instruction | John has 3 boxes. Each box is 5 inches by 6 inches by 4 inches. The walls are 1 inch thick. What is the<br>total inner volume of all 3 boxes?<br>### Response: Let's think step by step.First, we need to find the inner dimensions of one box. Since the<br>walls are 1 inch thick, we need to subtract 2 inches from each dimension (1 inch from each side of the<br>wall).<br>The original dimensions of the box are 5 inches by 6 inches by 4 inches. So, the inner dimensions will<br>be:<br>- Length: 5 inches - 2 inches = 3 inches - Width: 6 inches - 2 inches = 4 inches - Height: 4 inches - 2<br>inches = 2 inches<br>Now, we can calculate the inner volume of one box by multiplying the inner dimensions:<br>Inner Volume = Length × Width × Height Inner Volume = 3 inches × 4 inches × 2 inches Inner<br>Volume = 24 cubic inches<br>Since John has 3 boxes, we need to find the total inner volume of all 3 boxes:<br>Total Inner Volume = Inner Volume of one box × Number of boxes Total Inner Volume = 24 cubic<br>inches × 3 Total Inner Volume = 72 cubic inches<br>So, the total inner volume of all 3 boxes is 72 cubic inches.<br>The answer is: 72.                                                                                                                                                                               |  |\n\nTable 8: Case Study of EvoLLM-JP-v1-7B. This study presents a case where LLMs were unable to solve a Japanese math problem.\n\n<span id=\"page-25-0\"></span>\n\n#### Table 9: Case Study of EvoVLM-JP. This study presents the responses of the VLMs to questions that demand both Japanese-specific knowledge and VQA abilities.\n\n<span id=\"page-26-0\"></span>\n\n|                      | Example 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n|----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| User                 | この画像に映っている魚の形をした飾りは、日本では何と呼ばれていますか?<br>(What is the fish-shaped decoration in this image called in Japan?)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n| LLaVA-1.6-Mistral-7B | この画像に映っている魚の形をした飾りは、日本では「花魚」と呼ばれています。<br>(The fish-shaped decoration in this image is called \"hana-gyo\" in Japan.)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Japanese Stable VLM  | こいのぼり (Koi-nobori)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| EvoVLM-JP            | この画像に映っている魚の形をした飾りは、日本では「鯉のぼり」と呼ばれています。<br>(The fish-shaped decoration in this image is called \"Koi-nobori\" in Japan.)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n|                      | Example 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n| User                 | 写真を説明してください。<br>(Please explain this photo.)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n| LLaVA-1.6-Mistral-7B | この写真では、自然の中で小さな動物が歩いています。その動物は、犬のような外貌を持っていますが、実<br>際には犬ではありません。この動物は、野生動物として知られていますが、具体的な種類はわかりません。<br>背景には、秋の季節になっていることがわかります。多くの樹木には、黄色い葉が落ちています。このよう<br>な環境で、動物が歩いていることを見ることができます。 この写真では、動物が自然の中で活動しているこ<br>とを表現しています。同時に、秋の季節になっていることを伝えることもできます。<br>(In this photo, a small animal is walking in nature. The animal has a canine-like appearance but is not actually a dog. This<br>creature is known as a wild animal, but the specific species is unknown. In the background, it is evident that it is autumn.<br>Many trees have yellow leaves falling. In such an environment, the animal can be seen walking. The photo depicts the<br>animal being active in nature while also conveying that it is autumn.) |\n| Japanese Stable VLM  | 紅葉と鹿の写真です。<br>(It's a photo of autumn leaves and deer.)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n| EvoVLM-JP            | この写真は、一匹の小さな鹿が落ち葉の上に立っている様子を捉えています。鹿は警戒しているようで、周<br>囲を注意深く見回しています。背景には紅葉した木々が見え、秋の雰囲気が感じられます。<br>(This photo captures a small deer standing on fallen leaves. The deer appears to be alert, carefully scanning its surroundings.<br>In the background, you can see trees with autumn foliage, creating an atmosphere of autumn.)                                                                                                                                                                                                                                                                                                                                                                                          |\n\n|                      | Example 3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |  |  |  |\n|----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|--|--|\n| User                 | この信号機の色は何色ですか?<br>(What color is this traffic light?)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |  |  |  |\n| LLaVA-1.6-Mistral-7B | (A) この信号機の色は緑色です。 (The color of this traffic light is green.)<br>(B) この信号機の色は緑色です。 (The color of this traffic light is green.)<br>(C) この信号機の色は緑色です。 (The color of this traffic light is green.)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |  |  |  |\n| Japanese Stable VLM  | (A) 緑色です。 (It is green.)<br>(B) 緑と白です。 (It is green and white.)<br>(C) 白色 (White)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |  |  |  |\n| EvoVLM-JP            | (A) この信号機の色は青です。 (The color of this traffic light is blue.)<br>(B) この信号機の色は青です。 (The color of this traffic light is blue.)<br>(C) この信号機の色は青です。 (The color of this traffic light is blue.)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |  |  |  |\n|                      | Example 4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |  |  |  |\n| User                 | この建物について教えてください。<br>(Please explain this building.)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |  |  |  |\n| LLaVA-1.6-Mistral-7B | この建物は、日本で位置しています。具体的には、東京都渋谷区にあります。この建物は、1945年8月15日<br>に、第二次世界大戦中に日本軍によって攻撃され、完全に破壊されました。その後、1964年に再構築されま<br>した。 この建物は、「平和の塔」(Peace Tower)と呼ばれています。その名前は、この建物が再構築される<br>際に決められた名前です。この建物は、平和を祈るために作られたものであり、国内外から多くの人々が訪<br>れる場所です。<br>(This building is located in Japan, specifically in Shibuya Ward, Tokyo. It was completely destroyed by the Japanese military<br>during World War II on August 15, 1945. Subsequently, it was reconstructed in 1964. This building is called the \"Peace<br>Tower,\" a name decided upon during its reconstruction. It was created as a symbol of peace, and it is a place visited by many<br>people from both within and outside the country to pray for peace.)                       |  |  |  |\n| Japanese Stable VLM  | 原爆ドーム<br>(Atomic Bomb Dome)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |  |  |  |\n| EvoVLM-JP            | この建物は、日本の広島市にある「原爆ドーム」です。 1945年8月6日、広島市に原子爆弾が投下されまし<br>た。 この原子爆弾は、広島市の中心部にある「広島県産業奨励館」に直撃しました。 この奨励館は、現在は<br>「原爆ドーム」と呼ばれています。 この原爆ドームは、原子爆弾の被害を受けた建物の中で、唯一残ってい<br>る建物です。 この原爆ドームは、1996年に世界遺産に登録されました。<br>(This building is the \"Atomic Bomb Dome\" located in Hiroshima City, Japan. On August 6, 1945, an atomic bomb was<br>dropped on Hiroshima City. The bomb directly hit the \"Hiroshima Prefectural Industrial Promotion Hall,\" located in the<br>central part of Hiroshima City. This promotion hall is now known as the \"Atomic Bomb Dome.\" It is the only building that<br>remains standing among those damaged by the atomic bomb. The Atomic Bomb Dome was registered as a UNESCO World<br>Heritage Site in 1996.) |  |  |  |",
    "content_summary": "# Evolutionary Optimization of Model Merging Recipes\n\nTakuya Akiba, Makoto Shing, Yujin Tang, Qi Sun, David Ha Sakana AI Tokyo, Japan {takiba,mkshing,yujintang,qisun,hadavid}@sakana.ai\n\n## Abstract\n\nLarge language models (LLMs) have become increasing...",
    "content_length": 172403,
    "created_at": "2025-04-07T14:02:28.952807",
    "updated_at": "2025-04-07T14:07:46.792741",
    "file_path": "evolutionary_optimization_model_merging.txt"
  },
  "doc-785448a3ad051dd8ec23a0a536bb3823": {
    "status": "processed",
    "chunks_count": 20,
    "content": "# Mixup Model Merge: Enhancing Model Merging Performance through Randomized Linear Interpolation\n\nYue Zhou<sup>1</sup> Yi Chang<sup>1</sup>,2,<sup>3</sup> Yuan Wu<sup>1</sup>\\*\n\n<sup>1</sup>School of Artificial Intelligence, Jilin University\n\n<sup>2</sup>Engineering Research Center of Knowledge-Driven Human-Machine Intelligence, MOE, China <sup>3</sup>International Center of Future Science, Jilin University\n\nInternational Center of Future Science, Jilin University\n1. 24Q-Valley International Conference\n\nyuezhou24@mails.jlu.edu.cn, yichang@jlu.edu.cn, yuanwu@jlu.edu.cn\n\n### Abstract\n\nModel merging integrates the parameters of multiple models into a unified model, combining their diverse capabilities. Existing model merging methods are often constrained by fixed parameter merging ratios. In this study, we propose Mixup Model Merge (M<sup>3</sup>), an innovative approach inspired by the Mixup data augmentation technique. This method merges the parameters of two large language models (LLMs) by randomly generating linear interpolation ratios, allowing for a more flexible and comprehensive exploration of the parameter space. Extensive experiments demonstrate the superiority of our proposed M<sup>3</sup> method in merging fine-tuned LLMs: (1) it significantly improves performance across multiple tasks, (2) it enhances LLMs' out-of-distribution (OOD) robustness and adversarial robustness, (3) it achieves superior results when combined with sparsification techniques such as DARE, and (4) it offers a simple yet efficient solution that does not require additional computational resources. In conclusion, M<sup>3</sup> is a simple yet effective model merging method that significantly enhances the performance of the merged model by randomly generating contribution ratios for two fine-tuned LLMs. The code is available at [https://github.com/](https://github.com/MLGroupJLU/MixupModelMerge) [MLGroupJLU/MixupModelMerge](https://github.com/MLGroupJLU/MixupModelMerge).\n\n### 1 Introduction\n\nIn the field of Natural Language Processing (NLP), the emergence of large language models (LLMs) [\\(Brown et al.,](#page-8-0) [2020;](#page-8-0) [Touvron et al.,](#page-9-0) [2023;](#page-9-0) [OpenAI,](#page-9-1) [2023;](#page-9-1) [Chowdhery et al.,](#page-8-1) [2023\\)](#page-8-1) represents a revolutionary breakthrough. With their remarkable capabilities, these models have demonstrated outstanding performance across various tasks [\\(Jiao](#page-9-2) [et al.,](#page-9-2) [2023;](#page-9-2) [Chang et al.,](#page-8-2) [2024b;](#page-8-2) [Nam et al.,](#page-9-3) [2024;](#page-9-3) [Xing,](#page-10-0) [2024;](#page-10-0) [Guo et al.,](#page-8-3) [2024\\)](#page-8-3), significantly advancing NLP technologies.\n\n<span id=\"page-0-0\"></span>![](_page_0_Figure_11.jpeg)\n\n(a) Performance of the merged models obtained through Task Arithmetic and TIES-Merging with/without M<sup>3</sup>.\n\n![](_page_0_Figure_13.jpeg)\n\n(b) (Left) Performance of merged models with/without M<sup>3</sup> on OOD datasets. (Right) Adversarial robustness (PDR) of merged models with/without M<sup>3</sup>.\n\n![](_page_0_Figure_15.jpeg)\n\n(c) The performance trend of the merged models obtained through the following methods: TIES-Merging without M<sup>3</sup> and DARE, TIES-Merging with DARE, TIES-Merging with M<sup>3</sup>, and TIES-Merging with both M<sup>3</sup> and DARE.\n\nFigure 1: (a) M<sup>3</sup> significantly boosts the model merging performance. (b) OOD robustness, and adversarial robustness of the merged models. (c) Combined with DARE, M<sup>3</sup> delivers even better results. LM, MATH, and Code refer to the WizardLM13B, WizardMath-13B, and llama-2-13b-code-alpaca models. TA stands for Task Arithmetic and TIES stands for TIES-Merging.\n\nSupervised Fine-Tuning (SFT) is a crucial technique for adapting LLMs to specific tasks, refining their performance by training on domain-specific data [\\(Hu et al.,](#page-8-4) [2021;](#page-8-4) [Ding et al.,](#page-8-5) [2023;](#page-8-5) [Xia et al.,](#page-10-1) [2024\\)](#page-10-1). However, SFT requires substantial computational resources and long training times [\\(Brown](#page-8-0) [et al.,](#page-8-0) [2020;](#page-8-0) [Chang et al.,](#page-8-6) [2024a\\)](#page-8-6). To address this\n\n<sup>\\*</sup>Corresponding authors\n\nchallenge, Model Merging has emerged as an efficient solution, fusing the parameters of multiple fine-tuned LLMs into a unified model with diverse capabilities, without the need for additional training or computational costs [\\(Yang et al.,](#page-10-2) [2024;](#page-10-2) [Ak](#page-8-7)[iba et al.,](#page-8-7) [2024\\)](#page-8-7). It effectively reduces the resourceintensive demands of SFT while preserving and even enhancing model performance.\n\nA simple analogy for model merging is the Super Mario game, where the protagonist gains special abilities by absorbing power-up items. Similarly, merging model parameters integrates the strengths of different models, enabling more effective multi-task learning [\\(Yu et al.,](#page-10-3) [2024\\)](#page-10-3). However, existing model merging methods have some limitations, these methods heavily rely on predefined or heuristic parameter fusion strategies [\\(Wortsman](#page-9-4) [et al.,](#page-9-4) [2022;](#page-9-4) [Ilharco et al.,](#page-9-5) [2022;](#page-9-5) [Matena and Raf](#page-9-6)[fel,](#page-9-6) [2022;](#page-9-6) [Jin et al.,](#page-9-7) [2022;](#page-9-7) [Yadav et al.,](#page-10-4) [2023;](#page-10-4) [Yu](#page-10-3) [et al.,](#page-10-3) [2024\\)](#page-10-3) such that they fail to fully explore the parameter space, thereby restricting the potential of the merged model.\n\nTo address this issue, we draw inspiration from Mixup [\\(Zhang,](#page-10-5) [2017\\)](#page-10-5) and propose a novel technique called Mixup Model Merge (M<sup>3</sup>). This method introduces randomness by dynamically adjusting the contribution ratios between models, making the model merging process more flexible and enabling a thorough exploration of the parameter space. M<sup>3</sup> further unlocks the potential of model merging, significantly enhancing the generalization performance of the merged model while improving its out-of-distribution (OOD) and adversarial robustness [\\(Wang et al.,](#page-9-8) [2023;](#page-9-8) [Zhu et al.,](#page-10-6) [2023\\)](#page-10-6). As shown in Figure [2,](#page-2-0) the implementation of M<sup>3</sup> is similar to the process of proportionally mixing magical potions in *Harry Potter*. Specifically, this method dynamically controls the parameter fusion ratio between two fine-tuned LLMs by randomly generating a linear interpolation ratio  $\\lambda\\_m$ , where  $\\lambda\\_m \\in (0, 1)$  and  $\\lambda\\_m \\sim Beta(\\alpha, \\alpha)$ . By adjusting the parameter  $\\alpha$ , we can precisely control the distribution of  $\\lambda\\_m$ , allowing M<sup>3</sup> to explore the parameter space of model merging more deeply and thus fully unleash the potential of the models, leading to improved overall performance.\n\nWe conducted extensive experiments with three fine-tuned LLMs: WizardLM-13B [\\(Xu et al.,](#page-10-7) [2024\\)](#page-10-7), WizardMath-13B [\\(Luo et al.,](#page-9-9) [2023\\)](#page-9-9), and llama-2-13b-code-alpaca [\\(Chaudhary,](#page-8-8) [2023\\)](#page-8-8), which specialize in instruction following, mathematical reasoning, and code generation, respec-\n\ntively. Inspired by Mixup's effectiveness in enhancing the robustness of neural networks when handling corrupted labels or adversarial examples [\\(Zhang,](#page-10-5) [2017\\)](#page-10-5), we further performed comprehensive evaluations on LiveBench [\\(White et al.,](#page-9-10) [2024\\)](#page-9-10) and PromptBench [\\(Zhu et al.,](#page-10-8) [2024\\)](#page-10-8) to validate the potential of M<sup>3</sup> in improving the OOD robustness and adversarial robustness of merged models. The experimental results demonstrate that our proposed M<sup>3</sup> method can significantly improve merged models' performance across various tasks (as shown in Figure [1a\\)](#page-0-0), enhance the OOD and adversarial robustness of the merged models (as shown in Figure [1b\\)](#page-0-0), and boost model merging when combined with sparsification techniques like DARE [\\(Yu et al.,](#page-10-3) [2024\\)](#page-10-3) (as shown in Figure [1c\\)](#page-0-0).\n\n### 2 Related Works\n\nModel Merging Model merging is a technique that integrates the parameters of multiple models to create a unified model with enhanced or diverse capabilities [\\(Wortsman et al.,](#page-9-4) [2022;](#page-9-4) [Ilharco et al.,](#page-9-5) [2022;](#page-9-5) [Matena and Raffel,](#page-9-6) [2022;](#page-9-6) [Jin et al.,](#page-9-7) [2022;](#page-9-7) [Yadav et al.,](#page-10-4) [2023;](#page-10-4) [Yu et al.,](#page-10-3) [2024;](#page-10-3) [Lin et al.,](#page-9-11) [2024\\)](#page-9-11). Task arithmetic [\\(Ilharco et al.,](#page-9-5) [2022\\)](#page-9-5) leverages task vectors for model merging through arithmetic operations, incorporating a predefined scaling term to weight the contribution of different models. Fisher Merging [\\(Matena and Raffel,](#page-9-6) [2022\\)](#page-9-6) performs parameter fusion by applying weights derived from the Fisher information matrix [\\(Fisher,](#page-8-9) [1922\\)](#page-8-9), resulting in more precise parameter integration. TIES-Merging [\\(Yadav et al.,](#page-10-4) [2023\\)](#page-10-4) addresses task conflicts by removing low-magnitude parameters, resolving sign disagreements, and merging only the parameters that align with the final agreed-upon sign. In [\\(Yu et al.,](#page-10-3) [2024\\)](#page-10-3), it is found that LLMs can enhance their capabilities through model merging. Additionally, it introduces DARE, a method for sparsifying the delta parameters of the model [\\(Ilharco et al.,](#page-9-5) [2022\\)](#page-9-5), significantly improving the performance of various model merging techniques.\n\nMixup Mixup is proposed to enhance the generalization ability of deep learning models by surpassing traditional Empirical Risk Minimization (ERM) [\\(Zhang,](#page-10-5) [2017\\)](#page-10-5). It is a simple, data-agnostic augmentation technique that trains models using virtual examples created by linearly interpolating pairs of random examples and their corresponding labels. Rooted in the Vicinal Risk Minimization (VRM) principle [\\(Chapelle et al.,](#page-8-10) [2000\\)](#page-8-10), this ap-\n\n<span id=\"page-2-0\"></span>![](_page_2_Figure_0.jpeg)\n\nFigure 2: Implementation of m<sup>3</sup>: A process analogous to proportionally mixing magical potions in harry potter. The proposed method controls the contribution ratio between two fine-tuned llms by randomly generating a linear interpolation ratio  $\\lambda\\_m$ , where  $\\lambda\\_m \\in (0, 1)$  and  $\\lambda\\_m \\sim Beta(\\alpha, \\alpha)$ . The distribution of  $\\lambda\\_m$  is controlled by adjusting α.\n\nproach improves generalization across a variety of datasets [\\(Russakovsky et al.,](#page-9-12) [2015;](#page-9-12) [Krizhevsky](#page-9-13) [et al.,](#page-9-13) [2009;](#page-9-13) [Warden,](#page-9-14) [2017;](#page-9-14) [Asuncion et al.,](#page-8-11) [2007\\)](#page-8-11), and helps reduce overfitting, sensitivity to adversarial examples, and training instability, all with minimal computational cost. Given two samples  $(xi, yi)$  and  $(xj, yj)$ , Mixup generates a new sample using the following formulas:\n\n$$\\begin{aligned} \\check{x} &= \\lambda x\\_i + (1 - \\lambda) x\\_j \\\\ \\check{y} &= \\lambda y\\_i + (1 - \\lambda) y\\_j \\end{aligned} \\tag{1}$$\n\n $\\tilde{x}$  and  $\\tilde{y}$  represent the generated synthetic sample and its label, respectively, with  $\\lambda$  determining their interpolation ratio, typically ranging from 0 to 1. Here,  $\\lambda$  is a hyperparameter sampled from a Beta distribution, i.e.,  $\\lambda \\sim Beta(\\alpha, \\alpha)$ , where  $\\alpha$  controls the strength of the interpolation between featuretarget pairs. Inspired by Mixup, we propose a novel model merging method, M<sup>3</sup>, which generates the parameters of the merged model by performing linear interpolation between the parameters of two fine-tuned LLMs, with the interpolation ratio being random.\n\n### 3 Methodology\n\n#### 3.1 Model Merging Problem\n\nFollowing [Yu et al.](#page-10-3) [\\(2024\\)](#page-10-3), we focus on merging fine-tuned LLMs that have been optimized from the same pre-trained backbone [\\(Touvron et al.,](#page-9-0) [2023\\)](#page-9-0). Specifically, we aim to fuse the parameters of these LLMs to create a unified model capable of handling multiple tasks. In this context, we restrict our attention to the merging of two models, as the mixup theory, which forms the basis of our approach, is generally applied to two entities.\n\nGiven two tasks  $t1$  and  $t2$  with the corresponding fine-tuned LLMs having parameters  $\\theta$ <sup>t1</sup>SFT and  $\\theta$ <sup>t2</sup>SFT, model merging aims to combine the parameters of these two models into a single model with parameters  $\\theta$ M. The resulting model should be able to effectively perform both tasks simultaneously, leveraging the knowledge learned from each individual model.\n\n#### 3.2 Mixup Model Merge\n\nInspired by Mixup [\\(Zhang,](#page-10-5) [2017\\)](#page-10-5), we propose a simple yet effective model merging method called Mixup Model Merge (M<sup>3</sup>). Unlike existing methods that use fixed merging ratios, M<sup>3</sup> generates the model contribution ratio randomly, harnessing randomness to inject fresh vitality into the model merging process. Specifically, M<sup>3</sup> further explores the parameter space of the merged model to unlock the potential of model merging.\n\nTo further elaborate, given two fine-tuned LLMs with parameters { $\\theta^{t\\_1}\\_{SFT}$ ,  $\\theta^{t\\_2}\\_{SFT}$ }, we combine M<sup>3</sup> with established model merging methods to fuse these parameters and obtain a single merged model with parameters  $\\theta\\_M$ . As illustrative examples, we consider two widely used merging methods: Average Merging [\\(Wortsman et al.,](#page-9-4) [2022\\)](#page-9-4) and Task Arithmetic [\\(Ilharco et al.,](#page-9-5) [2022\\)](#page-9-5).\n\nThe official computation process for Average Merging is described as follows:\n\n$$\\theta\\_M = \\frac{1}{2} \\left( \\theta\\_{\\text{SFT}}^{t\\_1} + \\theta\\_{\\text{SFT}}^{t\\_2} \\right) \\tag{2}$$\n\nThe official computation process for Task Arith-\n\nmetic is:\n\n$$\\begin{split} \\theta\\_M &= \\theta\\_{\\text{PRE}} + \\lambda \\cdot (\\delta^{t\\_1} + \\delta^{t\\_2}) \\\\ &= \\theta\\_{\\text{PRE}} + \\lambda \\cdot \\sum\\_{i=1}^{2} (\\theta\\_{\\text{SFT}}^{t\\_i} - \\theta\\_{\\text{PRE}}), \\end{split} \\tag{3}$$\n\nwhere  $\\theta\\_{PRE} \\in \\mathbb{R}^d$  represents the parameters of the pre-trained language model (PLM), such as Llama 2 [\\(Touvron et al.,](#page-9-0) [2023\\)](#page-9-0). λ is a scaling factor that weights the contribution of each model during the merging process.  $\\delta^{t\\_i}$  denotes the delta parameter [\\(Ilharco et al.,](#page-9-5) [2022\\)](#page-9-5), which is defined as the difference between the parameters of the language models (LMs) before and after SFT, i.e.,  $\\delta^t = \\theta^t\\_{SFT} - \\theta\\_{PRE} \\in \\mathbb{R}^d$ , where t refers to task t.\n\nWhen introducing M<sup>3</sup>, the process for Average Merging is reformulated as:\n\n$$\n\\theta\\_M = \\lambda\\_m \\theta\\_{\\rm SFT}^{t\\_1} + (1 - \\lambda\\_m) \\theta\\_{\\rm SFT}^{t\\_2}, \\qquad (4)\n$$\n\nwhile the process for Task Arithmetic is reformulated as:\n\n$$\n\\theta\\_M = \\theta\\_{\\text{PRE}} + \\lambda\\_m \\delta^{t\\_1} + (1 - \\lambda\\_m) \\delta^{t\\_2}, \\quad (5)\n$$\n\nwhere  $\\lambda\\_m$  determines the linear interpolation ratio between the two fine-tuned LLMs, and is generally a value between 0 and 1.  $\\lambda\\_m$  is sampled from a Beta distribution, typically  $\\lambda\\_m \\sim Beta(\\alpha, \\alpha)$ , where  $\\alpha$  controls the shape of the Beta distribution.\n\nThe hyperparameter  $\\alpha$  for M<sup>3</sup> is selected from the range [0.2, 0.4, 0.5, 1, 2, 3, 5]. As shown in Figure [3:](#page-3-0) (1) When  $\\alpha = 1$ ,  $\\lambda$  follows a uniform distribution, meaning all values within the range (0, 1) are equally likely to be sampled. (2) When  $\\alpha < 1$ , the distribution of λ exhibits a bimodal shape, with higher probabilities near the extremes (0 and 1). This indicates that the merged model is more likely to be dominated by one of the two models. (3) When  $\\alpha > 1$ , the distribution of  $\\lambda$  becomes concentrated around the middle (e.g., 0.5), resulting in more balanced contributions from both models.\n\n#### 3.3 Theoretical Analysis\n\nMixup performs linear interpolations between data samples and their labels in the data space [\\(Zhang,](#page-10-5) [2017\\)](#page-10-5). Similarly, M<sup>3</sup> can be viewed as applying random linear interpolation in the parameter space, where the interpolation occurs between the parameters of two fine-tuned models trained on different tasks. M<sup>3</sup> represents a natural extension of the Vicinal Risk Minimization (VRM) principle [\\(Chapelle et al.,](#page-8-10) [2000\\)](#page-8-10) into the model parameter\n\n<span id=\"page-3-0\"></span>![](_page_3_Figure_11.jpeg)\n\nFigure 3: The Beta distribution visualization for different α values.\n\nspace. In data space, VRM introduces a vicinal distribution to simulate the true data distribution, thereby increasing the diversity of training data and enhancing the model's generalization ability. Similarly, M<sup>3</sup> extends this principle by constructing a virtual neighborhood in the model parameter space between two task-specific models, combining their knowledge in a way that is more natural and balanced.\n\nIn the context of model merging, interpolating between two sets of model parameters,  $\\theta^{t\\_1}\\_{SFT}$  and  $\\theta^{t\\_2}\\_{SFT}$ , defines a new neighborhood distribution in the parameter space, which can be expressed as:\n\n$$\\begin{split} P\\_{\\nu}(\\theta\\_M) &= \\int \\nu(\\theta\\_M \\mid \\theta\\_{\\text{SFT}}^{t\\_1}, \\theta\\_{\\text{SFT}}^{t\\_2}) \\, d\\theta\\_M \\\\ &\\times P(\\theta\\_{\\text{SFT}}^{t\\_1}) P(\\theta\\_{\\text{SFT}}^{t\\_2}), \\end{split} \\tag{6}$$\n\nwhere  $P\\_{\\nu}(\\theta\\_M)$  represents the probability distribution of the new model parameters  $\\theta\\_M$  generated through interpolation. The function  $\\nu(\\theta\\_M |$   $\\theta^{t\\_1}\\_{SFT}, \\theta^{t\\_2}\\_{SFT})$  defines the range and behavior of  $\\theta\\_M$ , depending on the interpolation strategy (such as linear interpolation). Additionally,  $P(\\theta^{t\\_1}\\_{SFT})$  and  $P(\\theta^{t\\_2}\\_{SFT})$  represent the prior distributions of the parameters of the two models, respectively.\n\nUnder the linear interpolation strategy, the function  $\\nu(\\theta\\_M | \\theta\\_{SFT}^{t\\_1}, \\theta\\_{SFT}^{t\\_2})$  is defined as:\n\n$$\n\\theta\\_M = \\lambda\\_m \\cdot \\theta\\_{\\text{SFT}}^{t\\_1} + (1 - \\lambda\\_m) \\cdot \\theta\\_{\\text{SFT}}^{t\\_2}, \\qquad (7)\n$$\n\nwhere  $\\lambda\\_m \\in (0, 1)$  is the interpolation ratio. The corresponding distribution  $P\\_{\\nu}(\\theta\\_M)$  is expressed as:\n\n$$P\\_{\\nu}(\\theta\\_M) = \\int \\delta\\left(\\theta\\_M - \\left(\\lambda\\_m \\cdot \\theta\\_{\\rm SFT}^{t\\_1} + (1 - \\lambda\\_m) \\cdot \\theta\\_{\\rm SFT}^{t\\_2}\\right)\\right) P(\\theta\\_{\\rm SFT}^{t\\_1}) P(\\theta\\_{\\rm SFT}^{t\\_2}) d\\theta\\_{\\rm SFT}^{t\\_1} d\\theta\\_{\\rm SFT}^{t\\_2},$$\n\n$$(8)$$\n\nwhere  $\\delta(\\cdot)$  is the Dirac delta function, ensuring that  $\\theta\\_M$  satisfies the linear interpolation rule. By using this linear interpolation method, the parameters  $\\theta\\_{SFT}^{t\\_1}$  and  $\\theta\\_{SFT}^{t\\_2}$  are combined in the parameter space, forming a new neighborhood distribution  $P\\_{\\nu}(\\theta\\_M)$ . This process effectively merges the knowledge of both models into a unified parameter space, thus achieving robust performance across different tasks while maintaining the original strengths of the models.\n\nBy interpolating the parameters, M<sup>3</sup> encourages the model to learn smoother decision boundaries. In this context, smoother decision boundaries can be understood as the boundaries between different tasks, such as instruction following, mathematical reasoning, and code generation, where the model must understand and adapt to each task differently. In tasks  $t\\_1$  and  $t\\_2$ , M<sup>3</sup> creates a virtual neighborhood that seamlessly integrates knowledge from both tasks. This approach prevents the merged model from overfitting task-specific details, ensuring a balanced and effective performance across all tasks.\n\nSecondly, M<sup>3</sup> introduces a linear inductive bias in the parameter space, encouraging the merged model parameters to lie on a linear manifold between the two source models. This linear structure offers significant advantages in terms of simplicity and generalization. According to Occam's Razor, simpler solutions tend to generalize better. By performing linear interpolation between two sets of parameters, M<sup>3</sup> avoids unnecessary complexity in the model merging process, leading to a more straightforward and efficient solution.\n\nThirdly, M<sup>3</sup> can improve the performance of the merged model across multiple tasks by mitigating task conflicts. Different tasks may require conflicting parameter values, which can lead to performance degradation on one task while optimizing for another. Linear interpolation helps balance these conflicts, resulting in a model that performs well among all tasks.\n\n### 4 Experiments\n\n#### 4.1 Experimental Setup\n\nTask-Specific Fine-Tuned LLMs and Datasets Following the experimental setup given in [Yu](#page-10-3) [et al.](#page-10-3) [\\(2024\\)](#page-10-3), we select three task-specific finetuned LLMs: WizardLM-13B [\\(Xu et al.,](#page-10-7) [2024\\)](#page-10-7), WizardMath-13B [\\(Luo et al.,](#page-9-9) [2023\\)](#page-9-9), and llama-2- 13b-code-alpaca [\\(Chaudhary,](#page-8-8) [2023\\)](#page-8-8), all of which\n\nuse Llama-2-13b [\\(Touvron et al.,](#page-9-0) [2023\\)](#page-9-0) as the pretrained backbone. These models are respectively designed for instruction-following, mathematical reasoning, and code generation tasks. To evaluate the instruction-following task we use AlpacaEval [\\(Li et al.,](#page-9-15) [2023\\)](#page-9-15). For testing mathematical reasoning task, we employ GSM8K [\\(Cobbe et al.,](#page-8-12) [2021\\)](#page-8-12) and MATH [\\(Hendrycks et al.,](#page-8-13) [2021\\)](#page-8-13). For estimating the performance of code-generating task, we use HumanEval [\\(Chen et al.,](#page-8-14) [2021\\)](#page-8-14) and MBPP [\\(Austin et al.,](#page-8-15) [2021\\)](#page-8-15). More details of these LLMs and datasets can be found in Appendix [A.1.](#page-10-9)\n\nThe Benchmarks for evaluating Out-of-Distribution and Adversarial Robustness To assess OOD robustness, we evaluate math & code, LM & math, and LM & code models using instruction following (LiveBench-Instruction), coding (LiveBench-Coding), and language comprehension (LiveBench-TypoFixing) category in LiveBench [\\(White et al.,](#page-9-10) [2024\\)](#page-9-10), respectively. More details on OOD benchmarks are given in Appendix [A.3.](#page-11-0)\n\nWe utilize the Adversarial Prompt Attacks module in PromptBench [\\(Zhu et al.,](#page-10-8) [2024\\)](#page-10-8) to assess the robustness of LLMs against adversarial prompts. Specifically, we employ three attack methods: DeepWordBug (character-level) [\\(Gao](#page-8-16) [et al.,](#page-8-16) [2018\\)](#page-8-16), BERTAttack (word-level) [\\(Li et al.,](#page-9-16) [2020\\)](#page-9-16), and StressTest (sentence-level) [\\(Naik et al.,](#page-9-17) [2018\\)](#page-9-17). The evaluation is conducted on two datasets supported by PromptBench: SST2 (sentiment analysis) [\\(Socher et al.,](#page-9-18) [2013\\)](#page-9-18) and CoLA (grammatical correctness) [\\(Warstadt,](#page-9-19) [2019\\)](#page-9-19). For more details on PromptBench and attack methods, please refer to Appendix [A.4.](#page-12-0)\n\nEvaluation Metrics We calculate win rate for AlpacaEval and LiveBench-Instruction, zeroshot accuracy for GSM8K and MATH, pass@1 for HumanEval, MBPP and LiveBench-Coding, Matthews correlation coefficient (MCC) for CoLA, accuracy for SST2, and zero-shot accuracy for LiveBench-TypoFixing.\n\nImplementation Details Unless otherwise specified, the details of the model merging experiments are consistent with [Yu et al.](#page-10-3) [\\(2024\\)](#page-10-3). The hyperparameter  $\\alpha$  for M<sup>3</sup> is chosen from the range [0.2, 0.4, 0.5, 1, 2, 3, 5]. For a detailed description of the hyperparameter settings in model merging methods, please refer to Appendix [A.2.](#page-11-1) Additionally, all experiments are conducted on NVIDIA GeForce RTX 4090 GPUs.\n\n#### 4.2 Merging Task-Specific Fine-Tuned LLMs\n\nWe integrate M<sup>3</sup> into three prominent model merging techniques: Average Merging, Task Arithmetic, and TIES-Merging. The performance of merging task-specific fine-tuned LLMs is presented in Table [1.](#page-6-0)\n\nFrom Table [1,](#page-6-0) we obtain the following observations: 1) M<sup>3</sup> generally enhances Average Merging, Task Arithmetic, and TIES-Merging when merging fine-tuned LLMs. For example, the improvements achieved by Average Merging with M<sup>3</sup> for Math & Code are 7.43% on GSM8K, 3.74% on Math, and 11.0% on MBPP. For LM & Code, Average Merging with M<sup>3</sup> shows improvements of 7.31% on AlpacaEval, 7.32% on HumanEval, and 2.4% on MBPP. Task Arithmetic with M<sup>3</sup> results in improvements of 2.0% on AlpacaEval and 2.44% on HumanEval for LM & Code, and 10.4% on MBPP for Math & Code. TIES-Merging with M<sup>3</sup> achieves an improvement of 4.01% for LM & Math on GSM8K. For LM & Code, TIES-Merging with M<sup>3</sup> shows significant improvements of 3.11% on AlpacaEval, 25.61% on HumanEval, and 30.8% on MBPP. 2) Compared to Task Arithmetic, Average Merging and TIES-Merging tend to benefit more from M<sup>3</sup>. This is because both Average Merging and TIES-Merging use a fixed merging ratio of 1/2, whereas Task Arithmetic allows the merging ratio to vary within the range [0.5, 1.0]. Consequently, the randomness introduced by M<sup>3</sup> in the merging ratio has a more pronounced impact on Average Merging. This further highlights the critical role of an effective merging ratio in determining the performance of the merged model. 3) [Yu et al.](#page-10-3) [\\(2024\\)](#page-10-3) has indicated that the suboptimal results of merging WizardMath-13B with llama-2-13b-codealpaca are due to llama-2-13b-code-alpaca not being well fine-tuned for code generation. In this context, the proposed M<sup>3</sup> approach improves the pass@1 score on MBPP by 10.4% for the merged model of WizardMath-13B and llama-2-13b-codealpaca. The improvement demonstrates that when one of the fine-tuned models to be merged is not well fine-tuned for the specific task, M<sup>3</sup> can effectively unlock the potential of both models, maximizing the performance of the merged model. The M<sup>3</sup> approach helps mitigate the impact of suboptimal fine-tuning on model merging performance.\n\n#### 4.3 Model Robustness\n\n<span id=\"page-5-0\"></span>![](_page_5_Figure_4.jpeg)\n\nFigure 4: Performance of merged models (Math & Code, LM & Math, and LM & Code) using three model merging methods (Average Merging, Task Arithmetic, and TIES-Merging) on OOD datasets.\n\nOut-of-distribution robustness To ensure that the evaluation datasets are as representative as possible of OOD data, we select datasets with sufficiently recent release dates and ensure they cover domains that fine-tuned LLMs have not been specifically trained on. Consequently, Math & Code is evaluated on LiveBench-Instruction, LM & Math on LiveBench-Coding, and LM & Code on LiveBench-TypoFixing. The performance of the merged LLMs is shown in Figure [4.](#page-5-0) As illustrated in Figure [4,](#page-5-0) M<sup>3</sup> consistently enhances the performance of merged models—Math & Code, LM & Math, and LM & Code—on OOD datasets. Specifically, when Task Arithmetic is combined with M<sup>3</sup>, the Math & Code model demonstrates a 1.9% improvement in win rate on LiveBench-Instruction, the LM & Math model achieves a 1.6% increase in pass@1 on LiveBench-Coding, and the LM & Code model shows a significant 6% boost in accuracy on LiveBench-TypoFixing. Similarly, when Average Merging is combined with M<sup>3</sup>, the Math & Code model attains a 1.5% improvement in win rate on LiveBench-Instruction, the LM & Math model achieves a 0.7% increase\n\n<span id=\"page-6-0\"></span>\n\n| Merging<br>Methods | Models             | Use Model<br>Mixup | Use<br>DARE | Instruction<br>Following | Mathematical<br>Reasoning |       | Code Generating |       |\n|--------------------|--------------------|--------------------|-------------|--------------------------|---------------------------|-------|-----------------|-------|\n|                    |                    |                    |             | AlpacaEval               | GSM8K                     | MATH  | HumanEval       | MBPP  |\n| /                  | LM                 | No                 | No          | 45.14                    | 2.20                      | 0.04  | 36.59           | 34.00 |\n|                    | Math               | No                 | No          | /                        | 64.22                     | 14.02 | /               | /     |\n|                    | Code               | No                 | No          | /                        | /                         | /     | 23.78           | 27.60 |\n|                    |                    | No                 | No          | 45.28                    | 66.34                     | 13.40 | /               | /     |\n|                    | LM<br>& Math       | Yes                | No          | 44.40                    | 66.26                     | 13.80 | /               | /     |\n|                    |                    | No                 | Yes         | 44.22                    | 66.57                     | 12.96 | /               | /     |\n|                    |                    | Yes                | Yes         | 43.53                    | 66.57                     | 14.12 | /               | /     |\n|                    | Average<br>Merging |                    | No          | No                       | 36.60                     | /     | /               | 29.88 |\n|                    |                    | LM<br>& Code       | Yes         | No                       | 43.91                     | /     | /               | 37.20 |\n|                    |                    |                    | No          | Yes                      | 38.81                     | /     | /               | 31.71 |\n|                    |                    | Yes                | Yes         | 40.31                    | /                         | /     | 36.59           | 37.00 |\n|                    |                    | No                 | No          | /                        | 56.17                     | 10.28 | 8.53            | 8.20  |\n|                    | Math<br>& Code     | Yes                | No          | /                        | 63.61                     | 14.02 | 8.54            | 19.20 |\n|                    |                    | No                 | Yes         | /                        | 56.18                     | 10.28 | 6.10            | 7.80  |\n|                    |                    | Yes                | Yes         | /                        | 64.97                     | 13.54 | 9.76            | 21.20 |\n|                    |                    | No                 | No          | 45.78                    | 66.34                     | 13.40 | /               | /     |\n|                    | LM<br>& Math       | Yes                | No          | 41.65                    | 66.34                     | 13.74 | /               | /     |\n|                    |                    | No                 | Yes         | 49.00                    | 66.64                     | 13.02 | /               | /     |\n|                    |                    | Yes                | Yes         | 44.90                    | 67.32                     | 13.74 | /               | /     |\n| Task<br>Arithmetic |                    | No                 | No          | 44.64                    | /                         | /     | 32.93           | 33.60 |\n|                    | LM<br>& Code       | Yes                | No          | 46.64                    | /                         | /     | 35.37           | 33.80 |\n|                    |                    | No                 | Yes         | 41.47                    | /                         | /     | 35.98           | 33.00 |\n|                    |                    | Yes                | Yes         | 45.20                    | /                         | /     | 35.98           | 35.20 |\n|                    |                    | No                 | No          | /                        | 64.67                     | 13.98 | 8.54            | 8.60  |\n|                    | Math<br>& Code     | Yes                | No          | /                        | 63.53                     | 13.94 | 7.93            | 19.00 |\n|                    |                    | No                 | Yes         | /                        | 65.05                     | 13.96 | 10.37           | 9.80  |\n|                    |                    | Yes                | Yes         | /                        | 65.13                     | 14.32 | 8.54            | 18.00 |\n|                    |                    | No                 | No          | 38.63                    | 14.56                     | 2.12  | /               | /     |\n|                    | LM<br>& Math       | Yes                | No          | 38.73                    | 18.57                     | 2.48  | /               | /     |\n|                    |                    | No                 | Yes         | 37.92                    | 18.04                     | 2.34  | /               | /     |\n|                    |                    | Yes                | Yes         | 39.93                    | 19.26                     | 2.82  | /               | /     |\n| TIES-<br>Merging   |                    | No                 | No          | 41.85                    | /                         | /     | 0.0             | 0.0   |\n|                    | LM<br>& Code       | Yes                | No          | 44.96                    | /                         | /     | 25.61           | 30.80 |\n|                    |                    | No                 | Yes         | 43.13                    | /                         | /     | 0.0             | 0.0   |\n|                    |                    | Yes                | Yes         | 45.65                    | /                         | /     | 26.83           | 33.20 |\n|                    |                    | No                 | No          | /                        | 64.67                     | 13.68 | 9.15            | 22.60 |\n|                    | Math<br>& Code     | Yes                | No          | /                        | 64.75                     | 14.16 | 9.76            | 21.4  |\n|                    |                    | No                 | Yes         | /                        | 64.82                     | 13.88 | 10.37           | 23.60 |\n|                    |                    | Yes                | Yes         | /                        | 64.75                     | 14.78 | 9.15            | 19.60 |\n\nTable 1: Performance of merging task-specific LLMs WizardLM-13B (LM), WizardMath-13B (Math), and llama-2- 13b-codealpaca (Code) on all the datasets. The best and second-best results are marked in bold and underlined fonts.\n\nin pass@1 on LiveBench-Coding, and the LM & Code model exhibits a 4% enhancement in accuracy on LiveBench-TypoFixing. Finally, when TIES-Merging is applied alongside M<sup>3</sup>, the Math & Code model achieves a 1.1% improvement in win rate on LiveBench-Instruction, the LM & Math model records a 0.6% increase in pass@1 on LiveBench-Coding, and the LM & Code model demonstrates a remarkable 14% improvement in accuracy on LiveBench-TypoFixing. These results underscore the robustness and versatility of M<sup>3</sup> in enhancing model performance across diverse merging strategies and OOD tasks.\n\nAdversarial robustness We employ three Prompt Attack Methods supported by the promptbench codebase (DeepWordBug, BERTAttack, and StressTest) [\\(Zhu et al.,](#page-10-8) [2024\\)](#page-10-8) to evaluate the adversarial robustness of three merged models\n\n(Math & Code, LM & Math, and LM & Code) obtained through the task arithmetic method. To balance experimental effectiveness with computational efficiency, we randomly selected the positions of the three attacked words in the prompts when executing the DeepWordBug and BERTAttack attacks. Adversarial robustness is assessed using the Performance Drop Rate (PDR) [\\(Zhu et al.,](#page-10-6) [2023\\)](#page-10-6), where a lower PDR indicates stronger robustness. Further details on PDR can be found in Appendix [D.](#page-14-0) The performance of the merged LLMs is shown in Table [2.](#page-7-0)\n\nAs shown in Table [2,](#page-7-0) M<sup>3</sup> improves the adversarial robustness of the merged models in most cases with the StressTest Prompt Attack Method. For example, with M<sup>3</sup>, the PDR of Math & Code decreased by 3.2% on the SST2 dataset and by 92.12% on the CoLA dataset, while the PDR of LM & Code decreased by 30.36% on SST2 and\n\n<span id=\"page-7-0\"></span>\n\n| Model             | Dataset | Use<br>Mixup    | Use<br>Attack | Metric<br>(%) | PDR<br>(%)   |\n|-------------------|---------|-----------------|---------------|---------------|--------------|\n| Math<br>&<br>Code | SST2    | No              | No            | 57.68         | 38.97        |\n|                   | SST2    | Yes             | Yes           | 35.21         |              |\n| Math<br>&<br>Code | Yes     | No              | 86.24         | <b>35.77</b>  |              |\n| Math<br>&<br>Code | CoLA    | No              | No            | 45.54         | 98.53        |\n| Math<br>&<br>Code |         | No              | Yes           | 0.67          |              |\n| Math<br>&<br>Code | CoLA    | Yes             | No            | 71.72         | <b>6.42</b>  |\n| Math<br>&<br>Code | CoLA    | Yes             | Yes           | 67.11         |              |\n| LM<br>&<br>Math   | SST2    | No              | No            | 92.78         | <b>29.05</b> |\n|                   |         | LM<br>&<br>Math | No            | Yes           | 65.83        |\n|                   | SST2    | Yes             | No            | 91.28         | 34.55        |\n|                   | SST2    | Yes             | Yes           | 59.75         |              |\n| LM<br>&<br>Math   | CoLA    | No              | No            | 79.19         | 8.84         |\n| LM<br>&<br>Math   |         | No              | Yes           | 72.20         |              |\n| LM<br>&<br>Math   | CoLA    | Yes             | No            | 80.54         | <b>4.52</b>  |\n| LM<br>&<br>Math   | CoLA    | Yes             | Yes           | 76.89         |              |\n| LM<br>&<br>Code   | SST2    | No              | No            | 10.55         | 38.04        |\n|                   |         | LM<br>&<br>Code | No            | Yes           | 6.54         |\n|                   | SST2    | Yes             | No            | 73.17         | <b>7.68</b>  |\n|                   | SST2    | Yes             | Yes           | 67.55         |              |\n| LM<br>&<br>Code   | CoLA    | No              | No            | 74.21         | 47.42        |\n| LM<br>&<br>Code   |         | No              | Yes           | 39.02         |              |\n| LM<br>&<br>Code   | CoLA    | Yes             | No            | 74.78         | <b>31.67</b> |\n| LM<br>&<br>Code   | CoLA    | Yes             | Yes           | 51.10         |              |\n\nTable 2: Adversarial Robustness of Merged Models on the SST2 and CoLA Datasets when Executing StressTest prompt attack method. The best and secondbest results are marked in bold and underlined fonts.\n\nby 15.75% on CoLA. Furthermore, in most cases, M<sup>3</sup> not only improves the adversarial robustness of the merged models but also enhances their performance metrics (accuracy and MCC) on the SST2 and CoLA datasets. Specifically, with M<sup>3</sup>, Math & Code demonstrates a 28.56% increase in accuracy on SST2 and a 26.18% increase in MCC on CoLA, while LM & Code achieves a 62.62% increase in accuracy on SST2. These results show that M<sup>3</sup> effectively enhances both the adversarial robustness and the performance of the merged models in sentiment analysis and grammar correctness tasks. Detailed experimental results for the remaining Prompt Attack Methods (DeepWordBug and BERTAttack) are presented in Appendix [B.](#page-12-1)\n\n#### 4.4 Mixup Model Merge with DARE\n\nDARE is a model sparsification method proposed by [\\(Yu et al.,](#page-10-3) [2024\\)](#page-10-3), with a more detailed introduction provided in Appendix [E.](#page-14-1) We combine M<sup>3</sup> and DARE with three model merging techniques, including Average Merging, Task Arithmetic, and TIES-Merging, to compare the effects of M<sup>3</sup> and DARE individually and explore their combined impact. The experimental results are presented in Table [1.](#page-6-0) Additionally, in the DARE method, the drop rate hyperparameter is set to 0.2.\n\nFrom Table [1,](#page-6-0) we conclude that: 1) In most cases,\n\nM<sup>3</sup> outperforms DARE, with a particularly significant advantage on certain datasets. For instance, the Math & Code model achieves a pass@1 score of 9.8% on the MBPP dataset when combined with DARE, while this score increases to 19% when combined with M<sup>3</sup>. This demonstrates that M<sup>3</sup> unlocks new potential in model merging by randomly generating merging ratios, leading to performance improvements that surpass those achieved by DARE. 2) Combining DARE and M<sup>3</sup> generally results in better model merging performance. For example, the LM & Math and LM & Code models, enhanced by TIES-Merging with M<sup>3</sup> and DARE, achieve the best performance on the test datasets. While only incorporating TIES-Merging with M<sup>3</sup> to these models, the enhanced models achieve the second best performance. This suggests that M<sup>3</sup> and DARE can complement each other. Moreover, in some cases, M<sup>3</sup> alone can deliver the best results, while DARE alone only achieves the best performance in very few cases, further demonstrating the superiority of M<sup>3</sup>.\n\n### 5 Conclusion\n\nInspired by the mixup method and the Vicinal Risk Minimization (VRM) principle, we propose Mixup Model Merge (M<sup>3</sup>), a novel approach for merging fine-tuned LLMs by introducing randomness into the parameters linear interpolation process. Unlike traditional methods such as average merging and task arithmetic, M<sup>3</sup> leverages a Beta distribution to dynamically adjust the merging ratio, enabling more flexible exploration of the parameter space. Experimental results demonstrate that M<sup>3</sup> not only significantly enhances the performance of the merged model across various tasks but also improves its OOD and adversarial robustness. Furthermore, when combined with sparsification techniques such as DARE, our approach achieves even more favorable model merging outcomes. In summary, M<sup>3</sup> is a simple yet powerful technique that requires minimal computational resources. By merely adjusting the merging ratio, it produces a merged model with enhanced task-specific capabilities and robustness. This exciting discovery paves the way for further research into optimizing merging ratio selection in model merging processes.\n\n### 6 Limitations\n\nThere are several limitations of the M<sup>3</sup> method: While it performs well for merging two models, (1) its scalability when merging a larger number of models, especially those with significant differences, remains uncertain. Additionally, (2) due to the inherent randomness in the merging process, multiple attempts may be required to achieve a merged model that meets expectations. This unpredictability can lead to increased computational costs, particularly in large-scale applications, resulting in a significant rise in resource consumption. Finally, (3) Our method may also be extended to a wider range of applications, such as merging fine-tuned models with Reinforcement Learning with Human Feedback (RLHF) [\\(Ouyang et al.,](#page-9-20) [2022\\)](#page-9-20) models to reduce the alignment tax [\\(FINE-](#page-8-17)[TUNING\\)](#page-8-17).\n\n### References\n\n- <span id=\"page-8-7\"></span>Takuya Akiba, Makoto Shing, Yujin Tang, Qi Sun, and David Ha. 2024. Evolutionary optimization of model merging recipes. *arXiv preprint arXiv:2403.13187*.\n- <span id=\"page-8-11\"></span>Arthur Asuncion, David Newman, et al. 2007. Uci machine learning repository.\n- <span id=\"page-8-15\"></span>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. *arXiv preprint arXiv:2108.07732*.\n- <span id=\"page-8-19\"></span>Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. 2023. Open llm leaderboard. *Hugging Face*.\n- <span id=\"page-8-0\"></span>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. *Advances in neural information processing systems*, 33:1877–1901.\n- <span id=\"page-8-6\"></span>Yupeng Chang, Yi Chang, and Yuan Wu. 2024a. Balora: Bias-alleviating low-rank adaptation to mitigate catastrophic inheritance in large language models. *arXiv preprint arXiv:2408.04556*.\n- <span id=\"page-8-2\"></span>Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2024b. A survey on evaluation of large language models. *ACM Transactions on Intelligent Systems and Technology*, 15(3):1–45.\n- <span id=\"page-8-10\"></span>Olivier Chapelle, Jason Weston, Léon Bottou, and Vladimir Vapnik. 2000. Vicinal risk minimization. *Advances in neural information processing systems*, 13.\n- <span id=\"page-8-8\"></span>Sahil Chaudhary. 2023. Code alpaca: An instructionfollowing llama model for code generation. *GitHub repository*.\n- <span id=\"page-8-14\"></span>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. *arXiv preprint arXiv:2107.03374*.\n- <span id=\"page-8-1\"></span>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways. *Journal of Machine Learning Research*, 24(240):1–113.\n- <span id=\"page-8-12\"></span>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. *arXiv preprint arXiv:2110.14168*.\n- <span id=\"page-8-5\"></span>Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. 2023. Parameter-efficient fine-tuning of large-scale pretrained language models. *Nature Machine Intelligence*, 5(3):220–235.\n- <span id=\"page-8-18\"></span>Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori B Hashimoto. 2024. Length-controlled alpacaeval: A simple way to debias automatic evaluators. *arXiv preprint arXiv:2404.04475*.\n- <span id=\"page-8-17\"></span>TIVE LLM FINE-TUNING. Paft: Aparallel training paradigm for effec-tive llm fine-tuning.\n- <span id=\"page-8-9\"></span>Ronald A Fisher. 1922. On the mathematical foundations of theoretical statistics. *Philosophical transactions of the Royal Society of London. Series A, containing papers of a mathematical or physical character*, 222(594-604):309–368.\n- <span id=\"page-8-16\"></span>Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. 2018. Black-box generation of adversarial text sequences to evade deep learning classifiers. In *2018 IEEE Security and Privacy Workshops (SPW)*, pages 50–56. IEEE.\n- <span id=\"page-8-3\"></span>Chenlu Guo, Nuo Xu, Yi Chang, and Yuan Wu. 2024. Chbench: A chinese dataset for evaluating health in large language models. *arXiv preprint arXiv:2409.15766*.\n- <span id=\"page-8-13\"></span>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. *arXiv preprint arXiv:2103.03874*.\n- <span id=\"page-8-4\"></span>Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. *arXiv preprint arXiv:2106.09685*.\n- <span id=\"page-9-5\"></span>Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. 2022. Editing models with task arithmetic. *arXiv preprint arXiv:2212.04089*.\n- <span id=\"page-9-2\"></span>Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, and Zhaopeng Tu. 2023. Is chatgpt a good translator? a preliminary study. *arXiv preprint arXiv:2301.08745*, 1(10).\n- <span id=\"page-9-7\"></span>Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, and Pengxiang Cheng. 2022. Dataless knowledge fusion by merging weights of language models. *arXiv preprint arXiv:2212.09849*.\n- <span id=\"page-9-13\"></span>Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features from tiny images.\n- <span id=\"page-9-16\"></span>Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, and Xipeng Qiu. 2020. Bert-attack: Adversarial attack against bert using bert. *arXiv preprint arXiv:2004.09984*.\n- <span id=\"page-9-15\"></span>Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Alpacaeval: An automatic evaluator of instruction-following models.\n- <span id=\"page-9-11\"></span>Yong Lin, Hangyu Lin, Wei Xiong, Shizhe Diao, Jianmeng Liu, Jipeng Zhang, Rui Pan, Haoxiang Wang, Wenbin Hu, Hanning Zhang, et al. 2024. Mitigating the alignment tax of rlhf. In *Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing*, pages 580–606.\n- <span id=\"page-9-9\"></span>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. *arXiv preprint arXiv:2308.09583*.\n- <span id=\"page-9-6\"></span>Michael S Matena and Colin A Raffel. 2022. Merging models with fisher-weighted averaging. *Advances in Neural Information Processing Systems*, 35:17703– 17716.\n- <span id=\"page-9-17\"></span>Aakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose, and Graham Neubig. 2018. Stress test evaluation for natural language inference. *arXiv preprint arXiv:1806.00692*.\n- <span id=\"page-9-3\"></span>Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, and Brad Myers. 2024. Using an llm to help with code understanding. In *Proceedings of the IEEE/ACM 46th International Conference on Software Engineering*, pages 1–13.\n- <span id=\"page-9-1\"></span>R OpenAI. 2023. Gpt-4 technical report. arxiv 2303.08774. *View in Article*, 2(5).\n- <span id=\"page-9-20\"></span>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. *Advances in neural information processing systems*, 35:27730–27744.\n- <span id=\"page-9-12\"></span>Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. 2015. Imagenet large scale visual recognition challenge. *International journal of computer vision*, 115:211–252.\n- <span id=\"page-9-18\"></span>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In *Proceedings of the 2013 conference on empirical methods in natural language processing*, pages 1631–1642.\n- <span id=\"page-9-22\"></span>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. [https://](https://github.com/tatsu-lab/stanford_alpaca) [github.com/tatsu-lab/stanford\\\\_alpaca](https://github.com/tatsu-lab/stanford_alpaca).\n- <span id=\"page-9-0\"></span>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*.\n- <span id=\"page-9-8\"></span>Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun Huang, Wei Ye, Xiubo Geng, et al. 2023. On the robustness of chatgpt: An adversarial and out-of-distribution perspective. *arXiv preprint arXiv:2302.12095*.\n- <span id=\"page-9-23\"></span>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language models with self-generated instructions. *arXiv preprint arXiv:2212.10560*.\n- <span id=\"page-9-14\"></span>Pete Warden. 2017. Launching the speech commands dataset. *Google Research Blog*.\n- <span id=\"page-9-19\"></span>A Warstadt. 2019. Neural network acceptability judgments. *arXiv preprint arXiv:1805.12471*.\n- <span id=\"page-9-21\"></span>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. *Advances in neural information processing systems*, 35:24824–24837.\n- <span id=\"page-9-10\"></span>Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, et al. 2024. Livebench: A challenging, contamination-free llm benchmark. *arXiv preprint arXiv:2406.19314*.\n- <span id=\"page-9-4\"></span>Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. 2022. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In *In ternational conference on machine learning*, pages 23965–23998. PMLR.\n- <span id=\"page-10-1\"></span>Tingyu Xia, Bowen Yu, Kai Dang, An Yang, Yuan Wu, Yuan Tian, Yi Chang, and Junyang Lin. 2024. Rethinking data selection at scale: Random selection is almost all you need. *arXiv preprint arXiv:2410.09335*.\n- <span id=\"page-10-0\"></span>Frank Xing. 2024. Designing heterogeneous llm agents for financial sentiment analysis. *ACM Transactions on Management Information Systems*.\n- <span id=\"page-10-7\"></span>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. 2024. Wizardlm: Empowering large pre-trained language models to follow complex instructions. In *The Twelfth International Conference on Learning Representations*.\n- <span id=\"page-10-4\"></span>Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. 2023. Resolving interference when merging models. *arXiv preprint arXiv:2306.01708*, 1.\n- <span id=\"page-10-2\"></span>Enneng Yang, Li Shen, Guibing Guo, Xingwei Wang, Xiaochun Cao, Jie Zhang, and Dacheng Tao. 2024. Model merging in llms, mllms, and beyond: Methods, theories, applications and opportunities. *arXiv preprint arXiv:2408.07666*.\n- <span id=\"page-10-3\"></span>Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. 2024. Language models are super mario: Absorbing abilities from homologous models as a free lunch. In *Forty-first International Conference on Machine Learning*.\n- <span id=\"page-10-5\"></span>Hongyi Zhang. 2017. mixup: Beyond empirical risk minimization. *arXiv preprint arXiv:1710.09412*.\n- <span id=\"page-10-6\"></span>Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Yue Zhang, Neil Gong, et al. 2023. Promptrobust: Towards evaluating the robustness of large language models on adversarial prompts. In *Proceedings of the 1st ACM Workshop on Large AI Systems and Models with Privacy and Safety Analysis*, pages 57–68.\n- <span id=\"page-10-8\"></span>Kaijie Zhu, Qinlin Zhao, Hao Chen, Jindong Wang, and Xing Xie. 2024. Promptbench: A unified library for evaluation of large language models. *Journal of Machine Learning Research*, 25(254):1–22.\n\n## A Detailed Experimental Settings\n\n## <span id=\"page-10-9\"></span>A.1 Task-Specific Fine-Tuned LLMs and Datasets Details\n\nWe conduct model merging experiments using three task-specific LLMs fine-tuned from Llama-2- 13b:\n\n• WizardLM-13B is an instruction-following model based on Llama-2-13b, designed to improve open-domain instruction-following. Using the Evol-Instruct method [\\(Xu et al.,](#page-10-7)\n\n[2024\\)](#page-10-7), it generates high-complexity instruction data to reduce human annotation and enhance generalization. The model undergoes supervised fine-tuning with AI-generated data, followed by refinement via RLHF. Evaluation results show that Evol-Instruct-generated instructions outperform human-written ones, and WizardLM-13B surpasses ChatGPT in high-complexity tasks. In GPT-4 automated evaluation, it achieves over 90% of ChatGPT's performance in 17 out of 29 tasks, demonstrating the effectiveness of AI-evolved instruction fine-tuning [\\(Xu et al.,](#page-10-7) [2024\\)](#page-10-7).\n\n- WizardMath-13B, optimized from Llama-2- 13b, is designed for mathematical reasoning and enhances Chain-of-Thought (CoT) [\\(Wei](#page-9-21) [et al.,](#page-9-21) [2022\\)](#page-9-21) capabilities. It uses Reinforcement Learning from Evol-Instruct Feedback to evolve math tasks and improve reasoning. Trained on GSM8K and MATH datasets, it excels in both basic and advanced math problems. In evaluations, WizardMath-Mistral 7B outperforms all open-source models with fewer training data, while WizardMath 70B surpasses GPT-3.5-Turbo, Claude 2, and even early GPT-4 versions in mathematical reasoning tasks.\n- llama-2-13b-code-alpaca is a code generation model fine-tuned from Llama-2-13b, designed to enhance code understanding and generation. It follows the same training approach as Stanford Alpaca [\\(Taori et al.,](#page-9-22) [2023\\)](#page-9-22) but focuses on code-related tasks. The model is fine-tuned with 20K instruction-following code samples generated using the Self-Instruct method [\\(Wang et al.,](#page-9-23) [2022\\)](#page-9-23). However, as it has not undergone safety fine-tuning, caution is required when using it in production environments.\n\nWe use one dataset to evaluate the instructionfollowing task:\n\n• AlpacaEval [\\(Li et al.,](#page-9-15) [2023\\)](#page-9-15) is an LLMbased automated evaluation metric that assesses model performance by testing on a fixed set of 805 instructions and computing the win rate of the evaluated model against a baseline. The evaluation process involves an LLM-based evaluator that compares the responses and determines the probability of\n\npreferring the evaluated model. In this paper, we use AlpacaEval 2.0 [\\(Dubois et al.,](#page-8-18) [2024\\)](#page-8-18). To reduce costs, we use chatgpt\\_fn for evaluation.\n\nWe use two dataset to evaluate the mathematical reasoning task:\n\n- GSM8K is a dataset of 8.5K high-quality, linguistically diverse grade school math word problems, designed to evaluate the multi-step mathematical reasoning abilities of large language models. It consists of 7.5K training problems and 1K test problems. In this paper, we use the 1K test set for evaluation [\\(Cobbe](#page-8-12) [et al.,](#page-8-12) [2021\\)](#page-8-12).\n- MATH is a dataset containing 12,500 competition-level math problems, designed to evaluate and enhance the problem-solving abilities of machine learning models. It consists of 7,500 training problems and 5,000 test problems. We use the 5,000 test set for evaluation [\\(Hendrycks et al.,](#page-8-13) [2021\\)](#page-8-13).\n\nWe used two dataset to evaluate the code generation task:\n\n- HumanEval is a dataset consisting of 164 hand-written programming problems, designed to evaluate the functional correctness of code generation models. Each problem includes a function signature, docstring, function body, and unit tests. The dataset tests models' language comprehension, reasoning, and algorithmic abilities [\\(Chen et al.,](#page-8-14) [2021\\)](#page-8-14).\n- MBPP is a dataset containing 974 programming problems designed to evaluate a model's ability to synthesize Python programs from natural language descriptions. The problems range from basic numerical operations to more complex tasks involving list and string processing. The test set consists of 500 problems, which are used for evaluation in this paper [\\(Austin et al.,](#page-8-15) [2021\\)](#page-8-15).\n\n### <span id=\"page-11-1\"></span>A.2 Hyperparameter Setting Details in Model Merging Methods\n\nTable [3](#page-11-2) presents the hyperparameter search ranges for the model merging methods. For Task Arithmetic and TIES-Merging, the scaling terms are selected from [0.5, 1.0], while in TIES-Merging, the retain ratio for the largest-magnitude parameters is chosen from [0.5, 0.7, 0.9]. In contrast, the\n\nAverage Merging method does not require any hyperparameters.\n\n<span id=\"page-11-2\"></span>\n\n| Merging Methods | Search Ranges of Hyperparameters                                                                                                                                     |\n|-----------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Task Arithmetic | Scaling term for merging model parameters:<br>[0.5, 0.6, 0.7, 0.8, 0.9, 1.0]                                                                                         |\n| TIES-Merging    | Scaling term for merging model parameters:<br>[0.5, 0.6, 0.7, 0.8, 0.9, 1.0]<br>Ratio for retaining parameters with the<br>largest-magnitude values: [0.5, 0.7, 0.9] |\n\nTable 3: Hyperparameter search ranges for model merging methods.\n\nTable [4](#page-11-3) presents the optimal hyperparameter settings for the TIES-Merging model merging method obtained through searching. These settings are further applied to model merging experiments involving M<sup>3</sup> and DARE.\n\n<span id=\"page-11-3\"></span>\n\n| Merging Method | Model       | Hyperparameter Values              |\n|----------------|-------------|------------------------------------|\n| TIES-Merging   | LM & Math   | scaling_term=0.5, retain_ratio=0.5 |\n|                | LM & Code   | scaling_term=1.0, retain_ratio=0.7 |\n|                | Math & Code | scaling_term=1.0, retain_ratio=0.5 |\n\nTable 4: Hyperparameter settings in TIES-Merging.\n\n## <span id=\"page-11-0\"></span>A.3 Out-of-Distribution Dataset Selection Details\n\nLiveBench [\\(White et al.,](#page-9-10) [2024\\)](#page-9-10) is a dynamic benchmark for large language models, featuring frequently updated questions and diverse tasks. To assess OOD robustness, we evaluate math & code, LM & math, and LM & code models using instruction following (LiveBench-Instruction), coding (LiveBench-Coding), and language comprehension (LiveBench-TypoFixing) category in LiveBench, respectively, deliberately avoiding the fine-tuning domains of the merged fine-tuned models. These tasks were released after November 2023, whereas WizardLM-13B, WizardMath-13B, and llama-2- 13b-code-alpaca were all introduced earlier. Furthermore, their shared Llama-2-13b backbone was trained on data only up to July 2023. Consequently, these factors collectively ensure that the evaluation remains OOD in the temporal dimension.\n\nWhen assessing the OOD robustness of LM & Code using the Language Comprehension category in LiveBench, only the typo-fixing task is considered. This decision is based on the fact that LiveBench is highly challenging, and the merged model performs poorly on other tasks in this category, with accuracy close to zero, rendering the evaluation results inconclusive and uninformative.\n\nFinally, we acknowledge the limitations of these datasets. For large models like Llama-2-13b, identifying truly OOD datasets is difficult, as their training data likely covers similar distributions. These datasets are better described as \"out-of-example\", representing instances not explicitly seen during training. As discussed in [\\(Wang et al.,](#page-9-8) [2023\\)](#page-9-8), distribution shifts can occur across domains and time. While Llama-2-13b may have been trained on datasets for tasks like instruction-following, coding, and language comprehension, the datasets we selected remain valuable for OOD evaluation by capturing temporal shifts, providing insights into robustness over time.\n\n### <span id=\"page-12-0\"></span>A.4 Adversarial Robustness Evaluation Experiments Setting Details\n\nPromptBench [\\(Zhu et al.,](#page-10-8) [2024\\)](#page-10-8) is a unified library designed for evaluating LLMs, providing a standardized and extensible framework. It includes several key components such as prompt construction, prompt engineering, dataset and model loading, adversarial prompt attacks, dynamic evaluation protocols, and analysis tools.\n\nWe use the Adversarial Prompt Attacks module in PromptBench aims to evaluate the robustness of LLMs against adversarial prompts. We employ three methods to perform adversarial attacks on prompts to evaluate the adversarial robustness of the merged models: DeepWordBug [\\(Gao et al.,](#page-8-16) [2018\\)](#page-8-16), BERTAttack [\\(Li et al.,](#page-9-16) [2020\\)](#page-9-16), and StressTest [\\(Naik et al.,](#page-9-17) [2018\\)](#page-9-17), representing Character-level, Word-level, and Sentence-level attacks, respectively.\n\n- DeepWordBug introduces subtle characterlevel perturbations (e.g., adding, deleting, or replacing characters) to words in text to deceive language models. It aims to evaluate a model's robustness against small typographical errors that may alter the model's performance without being easily detected.\n- BERTAttack manipulates text at the word level by replacing words with contextually similar synonyms to mislead large language models. This method tests the model's ability to maintain accuracy despite small lexical changes that might alter the meaning of the input.\n- StressTest appends irrelevant or redundant sentences to the end of a prompt to distract\n\n<span id=\"page-12-2\"></span>![](_page_12_Figure_7.jpeg)\n\nMerged Task Vector For task vectors within the dashed box, apply randomized linear interpolation to determine the final parameter value.\n\nTask Vector of Merged LLM\n\nTask Vector of LLM 1 Parameter\n\nInfluential values\n\nTask Vector of LLM 2\n\nThe values that align with the elected sign\n\nTask Vector of LLM 1 Parameter Task Vector of LLM 2 Task Vector of Merged LLM Influential values (b) After introducing M<sup>3</sup>, the Disjoint Merge step in the TIES-\n\nThe values that align with the elected sign\n\nMerging procedure.\n\n2.Elect the sign for 3.Disjoint Merge Merged\nnal TIES-Merging is that, in the Disjoint Merge step, Take their mean Figure 5: The difference between M<sup>3</sup> and the origiwhen two task vectors are retained for a given parameter, the mean of the task vectors is replaced by a random linear interpolation, while the other operations remain unchanged.\n\n> and confuse language models. It assesses the model's ability to handle extraneous information and maintain accuracy when faced with unnecessary distractions.\n\nThe values that align with the elected sign Merged Task Vector\n\nparameter value\n\nas the final\n\nThe evaluation is conducted on the Sentiment Analysis dataset (SST2 [\\(Socher et al.,](#page-9-18) [2013\\)](#page-9-18)) and the Grammar Correctness dataset (CoLA [\\(Warstadt,](#page-9-19) [2019\\)](#page-9-19)):\n\n- SST2 [\\(Socher et al.,](#page-9-18) [2013\\)](#page-9-18): A sentiment analysis dataset designed to assess whether a given sentence conveys a positive or negative sentiment.\n- CoLA [\\(Warstadt,](#page-9-19) [2019\\)](#page-9-19): A dataset for grammar correctness, where the model must determine whether a sentence is grammatically acceptable.\n\n## <span id=\"page-12-1\"></span>B Additional Experimental Results on Adversarial Robustness\n\nAll the merged models are obtained using the Task Arithmetic method. Table [5](#page-13-0) presents the detailed experimental results of the adversarial robustness of merged models on the SST2 and CoLA datasets applying the DeepWordBug prompt attack method. Table [6](#page-13-1) presents the detailed experimental results of the adversarial robustness of merged models on the\n\n<span id=\"page-13-0\"></span>\n\n| Model          | Dataset | Use Mixup | Use Attack | Metric (%) | PDR (%) |\n|----------------|---------|-----------|------------|------------|---------|\n| Math<br>& Code | SST2    | No        | No         | 57.68      | 11.73   |\n|                |         | No        | Yes        | 50.92      |         |\n|                | CoLA    | Yes       | No         | 78.21      | 37.10   |\n|                |         | Yes       | Yes        | 49.20      |         |\n| LM<br>& Math   | SST2    | No        | No         | 72.87      | 56.97   |\n|                |         | No        | Yes        | 31.35      |         |\n|                | CoLA    | Yes       | No         | 74.02      | 58.94   |\n|                |         | Yes       | Yes        | 30.39      |         |\n| LM<br>& Math   | SST2    | No        | No         | 92.78      | 2.60    |\n|                |         | No        | Yes        | 90.37      |         |\n|                | CoLA    | Yes       | No         | 91.28      | 3.77    |\n|                |         | Yes       | Yes        | 87.84      |         |\n| LM<br>& Code   | SST2    | No        | No         | 79.19      | 4.96    |\n|                |         | No        | Yes        | 75.26      |         |\n|                | CoLA    | Yes       | No         | 80.54      | 1.07    |\n|                |         | Yes       | Yes        | 79.67      |         |\n| LM<br>& Code   | SST2    | No        | No         | 10.55      | 98.91   |\n|                |         | No        | Yes        | 0.11       |         |\n|                | CoLA    | Yes       | No         | 73.17      | 97.65   |\n|                |         | Yes       | Yes        | 1.72       |         |\n| LM<br>& Code   | SST2    | No        | No         | 74.21      | 8.79    |\n|                |         | No        | Yes        | 67.69      |         |\n|                | CoLA    | Yes       | No         | 73.922     | 11.15   |\n|                |         | Yes       | Yes        | 65.68      |         |\n\n<span id=\"page-13-1\"></span>Table 5: Adversarial robustness of merged models on the SST2 and CoLA datasets when executing the DeepWord-Bug prompt attack method.\n\n| Model          | Dataset | Use Mixup | Use Attack | Metric (%) | PDR (%) |\n|----------------|---------|-----------|------------|------------|---------|\n| Math<br>& Code | SST2    | No        | No         | 57.68      | 13.92   |\n|                |         | No        | Yes        | 49.66      |         |\n|                | CoLA    | Yes       | No         | 78.21      | 4.11    |\n|                |         | Yes       | Yes        | 75.00      |         |\n| LM<br>& Math   | SST2    | No        | No         | 45.54      | 13.47   |\n|                |         | No        | Yes        | 39.41      |         |\n|                | CoLA    | Yes       | No         | 71.72      | 17.25   |\n|                |         | Yes       | Yes        | 59.35      |         |\n| LM<br>& Math   | SST2    | No        | No         | 92.78      | 2.22    |\n|                |         | No        | Yes        | 90.71      |         |\n|                | CoLA    | Yes       | No         | 91.28      | 0.0     |\n|                |         | Yes       | Yes        | 91.28      |         |\n| LM<br>& Math   | SST2    | No        | No         | 79.19      | 12.00   |\n|                |         | No        | Yes        | 69.70      |         |\n|                | CoLA    | Yes       | No         | 80.54      | 5.83    |\n|                |         | Yes       | Yes        | 75.84      |         |\n| LM<br>& Code   | SST2    | No        | No         | 10.55      | 95.65   |\n|                |         | No        | Yes        | 0.46       |         |\n|                | CoLA    | Yes       | No         | 73.17      | 55.02   |\n|                |         | Yes       | Yes        | 32.91      |         |\n| LM<br>& Code   | SST2    | No        | No         | 74.21      | 7.24    |\n|                |         | No        | Yes        | 68.84      |         |\n|                | CoLA    | Yes       | No         | 73.92      | 7.52    |\n|                |         | Yes       | Yes        | 68.36      |         |\n\nTable 6: Adversarial robustness of merged models on the SST2 and CoLA datasets when executing the Bertattack prompt attack method.\n\nprompt attack method.\n\n## SST2 and CoLA datasets applying the BERTAttack\nC Integrating M<sup>3</sup> into the TIES-Merging Model Merging Method\n\nFigure [5](#page-12-2) shows the specific implementation approach to incorporating M<sup>3</sup> into TIES-Merging. After the steps of trimming parameters with lower magnitudes and resolving sign disagreements, the\n\ntwo models to be merged are denoted as M<sup>1</sup> and M2. During the M<sup>3</sup> process, only the parameters that are preserved in both M<sup>1</sup> and M2 are interpolated according to the model merging hyperparameter  $\\lambda\\_m$  to obtain the merged parameters. For parameters that are preserved in only one of the models, no interpolation is performed, and the original value from the preserved model is retained in the merged model.\n\n## <span id=\"page-14-0\"></span>D Performance Drop Rate (PDR) for Adversarial Robustness\n\nThe adversarial robustness is evaluated using the Performance Drop Rate (PDR) [\\(Zhu et al.,](#page-10-6) [2023\\)](#page-10-6), which is defined as follows:\n\n$$\\text{PDR} = \\frac{\\text{Metric}\\_{\\text{no attack}} - \\text{Metric}\\_{\\text{attack}}}{\\text{Metric}\\_{\\text{no attack}}},\\quad(9)$$\n\nwhere Metricno attack denotes the performance metric without any prompt attack, and Metricattack represents the performance metric under the prompt attack. A smaller PDR indicates stronger adversarial defense against prompt attacks, implying better adversarial robustness.\n\n### <span id=\"page-14-1\"></span>E Detailed Introduction to DARE\n\nDARE (Drop And REscale) [\\(Yu et al.,](#page-10-3) [2024\\)](#page-10-3) is a model sparsification method designed to reduce the redundancy of delta parameters in fine-tuned models while preserving their task-specific capabilities. In SFT, model parameters are optimized to unlock abilities for specific tasks, with the difference between fine-tuned and pre-trained parameters referred to as delta parameters.\n\nHowever, studies have shown that delta parameters are often highly redundant. DARE addresses this redundancy by randomly dropping a proportion  $p$  of delta parameters (referred to as the drop rate) and rescaling the remaining ones by a factor of  $1/(1 - p)$ . This simple yet effective approach enables DARE to eliminate up to 99% of delta parameters with minimal impact on model performance, particularly in large-scale models, and it can be applied using only CPUs.\n\nBeyond sparsification, DARE serves as a versatile plug-in for merging multiple homologous fine-tuned models (fine-tuned from the same base model) by reducing parameter interference. When combined with existing model merging techniques such as Average Merging, Task Arithmetic, and\n\nTIES-Merging, DARE facilitates the fusion of models while retaining or even enhancing task performance across multiple benchmarks. This effect is especially pronounced in decoder-based LMs, where DARE boosts task generalization.\n\nExperiments on AlpacaEval, GSM8K, and MBPP reveal that the merged LM has the potential to outperform any individual source LM, presenting a significant new discovery. Notably, the 7B model obtained through DARE merging, Super-Mario v2, ranks first among models of the same scale on the Open LLM Leaderboard [\\(Beeching](#page-8-19) [et al.,](#page-8-19) [2023\\)](#page-8-19). These improvements were achieved without the need for retraining, positioning DARE as an efficient and resource-friendly solution for model merging.\n\n\n\n# THE RELATIONSHIP BETWEEN REASONING AND PERFORMANCE IN LARGE LANGUAGE MODELS—O3 (MINI) THINKS HARDER, NOT LONGER.\n\nMarthe Ballon1, [0009-0000-4586-234X](https://orcid.org/0009-0000-4586-234X)\n\nAndres Algaba<sup>1</sup> [0000-0002-0532-3066](https://orcid.org/0000-0002-0532-3066)\n\nVincent Ginis1,2 [0000-0003-0063-9608](https://orcid.org/0000-0003-0063-9608)\n\n<sup>1</sup>Data Analytics Lab, Vrije Universiteit Brussel, 1050 Brussel, Belgium <sup>2</sup>School of Engineering and Applied Sciences, Harvard University, Cambridge, Massachusetts 02138, USA",
    "content_summary": "# Mixup Model Merge: Enhancing Model Merging Performance through Randomized Linear Interpolation\n\nYue Zhou<sup>1</sup> Yi Chang<sup>1</sup>,2,<sup>3</sup> Yuan Wu<sup>1</sup>\\*\n\n<sup>1</sup>School of Artificial Intelligence, Jilin University\n\n<sup>2<...",
    "content_length": 77935,
    "created_at": "2025-04-07T14:02:28.955892",
    "updated_at": "2025-04-07T14:09:27.426389",
    "file_path": "Mixup_model_merging.txt"
  }
}