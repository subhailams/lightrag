### This is sample file of .env

### Server Configuration
# HOST=0.0.0.0
# PORT=9621
# WORKERS=2
### separating data from difference Lightrag instances
# NAMESPACE_PREFIX=lightrag
### Max nodes return from grap retrieval
# MAX_GRAPH_NODES=1000
# CORS_ORIGINS=http://localhost:3000,http://localhost:8080

### Optional SSL Configuration
# SSL=true
# SSL_CERTFILE=/path/to/cert.pem
# SSL_KEYFILE=/path/to/key.pem

### Directory Configuration (defaults to current working directory)
WORKING_DIR=/Users/subhailamathy/Documents/MS/Sp25/CS532/Project/LightRAG/new_working_dir
# INPUT_DIR=<absolute_path_for_doc_input_dir>

### Ollama Emulating Model Tag
# OLLAMA_EMULATING_MODEL_TAG=latest

### Logging level
# LOG_LEVEL=INFO
# VERBOSE=False
# LOG_MAX_BYTES=10485760
# LOG_BACKUP_COUNT=5
### Logfile location (defaults to current working directory)
# LOG_DIR=/path/to/log/directory

### Settings for RAG query
# HISTORY_TURNS=3
# COSINE_THRESHOLD=0.2
# TOP_K=60
# MAX_TOKEN_TEXT_CHUNK=4000
# MAX_TOKEN_RELATION_DESC=4000
# MAX_TOKEN_ENTITY_DESC=4000

### Settings for document indexing
ENABLE_LLM_CACHE_FOR_EXTRACT=true
SUMMARY_LANGUAGE=English
# CHUNK_SIZE=1200
# CHUNK_OVERLAP_SIZE=100
### Max tokens for entity or relations summary
# MAX_TOKEN_SUMMARY=500
### Number of parallel processing documents in one patch
# MAX_PARALLEL_INSERT=2

### Num of chunks send to Embedding in single request
# EMBEDDING_BATCH_NUM=32text_chunks_task
### Max concurrency requests for Embedding
# EMBEDDING_FUNC_MAX_ASYNC=16
# MAX_EMBED_TOKENS=8192

### LLM Configuration
### Time out in seconds for LLM, None for infinite timeout
TIMEOUT=150
### Some models like o1-mini require temperature to be set to 1
TEMPERATURE=0.5
### Max concurrency requests of LLM
MAX_ASYNC=4
### Max tokens send to LLM (less than context size of the model)
MAX_TOKENS=32768



### OpenAI alike example
LLM_BINDING=openai
LLM_MODEL=gpt-4o-mini
LLM_BINDING_HOST=https://api.openai.com/v1
LLM_BINDING_API_KEY=

EMBEDDING_BINDING=ollama
EMBEDDING_BINDING_HOST=https://4db6-34-148-187-197.ngrok-free.app
EMBEDDING_MODEL=nomic-embed-text
EMBEDDING_DIM=768
# EMBEDDING_BINDING_API_KEY=your_api_key




### MongoDB Configuration
MONGO_URI=mongodb://root:root@localhost:27017/
MONGO_DATABASE=LightRAG
### separating all data from difference Lightrag instances(deprecating, use NAMESPACE_PREFIX in future)
# MONGODB_GRAPH=false

### Milvus Configuration
MILVUS_URI=http://localhost:19530
MILVUS_DB_NAME=default
# MILVUS_USER=root
# MILVUS_PASSWORD=your_password
# MILVUS_TOKEN=your_token


### Data storage selection
<!-- LIGHTRAG_KV_STORAGE=JsonKVStorage -->
<!-- LIGHTRAG_VECTOR_STORAGE=MilvusVectorDBStorage -->
<!-- LIGHTRAG_GRAPH_STORAGE=Neo4JStorage -->
<!-- LIGHTRAG_DOC_STATUS_STORAGE=JsonDocStatusStorage -->


### Neo4j Configuration
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD='light_rag'


