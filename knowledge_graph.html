<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 100vh;
                 background-color: #ffffff;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"color": "#a7884d", "description": "Recurrent Neural Networks (RNNs) are a type of neural network architecture designed to handle sequential data by maintaining an internal state or memory.", "entity_id": "Recurrent Neural Networks", "entity_type": "organization", "file_path": "unknown_source", "id": "Recurrent Neural Networks", "label": "Recurrent Neural Networks", "shape": "dot", "size": 10, "source_id": "chunk-2105e0fac066cb4bca789ad1d39e48a6", "title": "Recurrent Neural Networks (RNNs) are a type of neural network architecture designed to handle sequential data by maintaining an internal state or memory."}, {"color": "#d0f95c", "description": "Self-attention is a more advanced form of computation that extends the capabilities of recurrent models by allowing them to focus on different parts of the sequence.", "entity_id": "Self-Attention Mechanism", "entity_type": "UNKNOWN", "file_path": "unknown_source", "id": "Self-Attention Mechanism", "label": "Self-Attention Mechanism", "shape": "dot", "size": 10, "source_id": "chunk-2105e0fac066cb4bca789ad1d39e48a6", "title": "Self-attention is a more advanced form of computation that extends the capabilities of recurrent models by allowing them to focus on different parts of the sequence."}, {"color": "#efcf38", "description": "Attention mechanisms allow models to focus on specific parts of the input sequence when generating output, which can improve performance in various tasks.", "entity_id": "Attention Mechanisms", "entity_type": "category", "file_path": "unknown_source", "id": "Attention Mechanisms", "label": "Attention Mechanisms", "shape": "dot", "size": 10, "source_id": "chunk-2105e0fac066cb4bca789ad1d39e48a6", "title": "Attention mechanisms allow models to focus on specific parts of the input sequence when generating output, which can improve performance in various tasks."}, {"color": "#2927f5", "description": "These techniques are used to optimize the computational efficiency of recurrent models by breaking down complex operations into simpler ones.", "entity_id": "Factorization Tricks [21] ", "entity_type": "organization", "file_path": "unknown_source", "id": "Factorization Tricks [21] ", "label": "Factorization Tricks [21] ", "shape": "dot", "size": 10, "source_id": "chunk-2105e0fac066cb4bca789ad1d39e48a6", "title": "These techniques are used to optimize the computational efficiency of recurrent models by breaking down complex operations into simpler ones."}, {"color": "#b58ced", "description": "The Transformer relies entirely on self-attention for computing input and output representations without using sequential recurrence.", "entity_id": "Transformer Model Architecture", "entity_type": "UNKNOWN", "file_path": "unknown_source", "id": "Transformer Model Architecture", "label": "Transformer Model Architecture", "shape": "dot", "size": 10, "source_id": "chunk-2105e0fac066cb4bca789ad1d39e48a6", "title": "The Transformer relies entirely on self-attention for computing input and output representations without using sequential recurrence."}, {"color": "#6463a5", "description": "Conditional computation is a method that allows only certain parts of a neural network to be activated based on conditions, thus saving computational resources.", "entity_id": "Conditional Computation [32] ", "entity_type": "organization", "file_path": "unknown_source", "id": "Conditional Computation [32] ", "label": "Conditional Computation [32] ", "shape": "dot", "size": 10, "source_id": "chunk-2105e0fac066cb4bca789ad1d39e48a6", "title": "Conditional computation is a method that allows only certain parts of a neural network to be activated based on conditions, thus saving computational resources."}, {"color": "#400333", "description": "LSTM is a specific type of RNN that addresses the vanishing gradient problem and has been widely used in sequence modeling tasks.", "entity_id": "Long Short-Term Memory [13] ", "entity_type": "organization", "file_path": "unknown_source", "id": "Long Short-Term Memory [13] ", "label": "Long Short-Term Memory [13] ", "shape": "dot", "size": 10, "source_id": "chunk-2105e0fac066cb4bca789ad1d39e48a6", "title": "LSTM is a specific type of RNN that addresses the vanishing gradient problem and has been widely used in sequence modeling tasks."}, {"color": "#0d43b7", "description": "GRUs are another variant of RNNs designed to manage the long-term dependencies more efficiently than traditional RNNs.", "entity_id": "Gated Recurrent Neural Networks [7] ", "entity_type": "organization", "file_path": "unknown_source", "id": "Gated Recurrent Neural Networks [7] ", "label": "Gated Recurrent Neural Networks [7] ", "shape": "dot", "size": 10, "source_id": "chunk-2105e0fac066cb4bca789ad1d39e48a6", "title": "GRUs are another variant of RNNs designed to manage the long-term dependencies more efficiently than traditional RNNs."}, {"color": "#65ab27", "description": "These approaches refer to the current leading methods in sequence modeling and transduction problems such as language modeling and machine translation.", "entity_id": "World State of the Art Approaches", "entity_type": "category", "file_path": "unknown_source", "id": "World State of the Art Approaches", "label": "World State of the Art Approaches", "shape": "dot", "size": 10, "source_id": "chunk-2105e0fac066cb4bca789ad1d39e48a6", "title": "These approaches refer to the current leading methods in sequence modeling and transduction problems such as language modeling and machine translation."}, {"color": "#44fa42", "description": "Language modeling is a task that involves predicting the probability distribution over words given the previous context.", "entity_id": "Language Modeling", "entity_type": "category", "file_path": "unknown_source", "id": "Language Modeling", "label": "Language Modeling", "shape": "dot", "size": 10, "source_id": "chunk-2105e0fac066cb4bca789ad1d39e48a6", "title": "Language modeling is a task that involves predicting the probability distribution over words given the previous context."}, {"color": "#9391cd", "description": "Machine translation is the process of converting text from one language to another automatically.", "entity_id": "Machine Translation", "entity_type": "category", "file_path": "unknown_source", "id": "Machine Translation", "label": "Machine Translation", "shape": "dot", "size": 10, "source_id": "chunk-2105e0fac066cb4bca789ad1d39e48a6", "title": "Machine translation is the process of converting text from one language to another automatically."}, {"color": "#558642", "description": "These architectures consist of an encoder that processes the input sequence and a decoder that generates the output sequence based on the encoded representation.", "entity_id": "Encoder-Decoder Architectures [38, 24, 15] ", "entity_type": "organization", "file_path": "unknown_source", "id": "Encoder-Decoder Architectures [38, 24, 15] ", "label": "Encoder-Decoder Architectures [38, 24, 15] ", "shape": "dot", "size": 10, "source_id": "chunk-2105e0fac066cb4bca789ad1d39e48a6", "title": "These architectures consist of an encoder that processes the input sequence and a decoder that generates the output sequence based on the encoded representation."}, {"color": "#8df62b", "description": "Hidden states are internal representations computed by neural networks at each time step during sequence processing.", "entity_id": "Hidden States", "entity_type": "category", "file_path": "unknown_source", "id": "Hidden States", "label": "Hidden States", "shape": "dot", "size": 10, "source_id": "chunk-2105e0fac066cb4bca789ad1d39e48a6", "title": "Hidden states are internal representations computed by neural networks at each time step during sequence processing."}, {"color": "#02e86f", "description": "Training examples refer to individual instances used for training machine learning models, where the goal is to minimize prediction errors.", "entity_id": "Training Examples", "entity_type": "category", "file_path": "unknown_source", "id": "Training Examples", "label": "Training Examples", "shape": "dot", "size": 10, "source_id": "chunk-2105e0fac066cb4bca789ad1d39e48a6", "title": "Training examples refer to individual instances used for training machine learning models, where the goal is to minimize prediction errors."}, {"color": "#89037d", "description": "Memory constraints limit the number of training examples that can be processed simultaneously in a batch during model training.", "entity_id": "Memory Constraints", "entity_type": "category", "file_path": "unknown_source", "id": "Memory Constraints", "label": "Memory Constraints", "shape": "dot", "size": 10, "source_id": "chunk-2105e0fac066cb4bca789ad1d39e48a6", "title": "Memory constraints limit the number of training examples that can be processed simultaneously in a batch during model training."}, {"color": "#eb5d9c", "description": "The Extended Neural GPU is a model architecture that uses convolutional neural networks for parallel computation across all positions in the sequence.", "entity_id": "Extended Neural GPU [16] ", "entity_type": "organization", "file_path": "unknown_source", "id": "Extended Neural GPU [16] ", "label": "Extended Neural GPU [16] ", "shape": "dot", "size": 10, "source_id": "chunk-2105e0fac066cb4bca789ad1d39e48a6", "title": "The Extended Neural GPU is a model architecture that uses convolutional neural networks for parallel computation across all positions in the sequence."}, {"color": "#3ea7b8", "description": "ByteNet is another model that employs convolutional neural networks to compute hidden representations for input and output sequences.", "entity_id": "ByteNet [18] ", "entity_type": "organization", "file_path": "unknown_source", "id": "ByteNet [18] ", "label": "ByteNet [18] ", "shape": "dot", "size": 10, "source_id": "chunk-2105e0fac066cb4bca789ad1d39e48a6", "title": "ByteNet is another model that employs convolutional neural networks to compute hidden representations for input and output sequences."}, {"color": "#a4ae1b", "description": "Convolutional Sequence-to-Sequence (ConvS2S) uses convolutional layers to process the sequence, with a cost of increasing complexity as the distance between positions increases.", "entity_id": "ConvS2S [9] ", "entity_type": "organization", "file_path": "unknown_source", "id": "ConvS2S [9] ", "label": "ConvS2S [9] ", "shape": "dot", "size": 10, "source_id": "chunk-2105e0fac066cb4bca789ad1d39e48a6", "title": "Convolutional Sequence-to-Sequence (ConvS2S) uses convolutional layers to process the sequence, with a cost of increasing complexity as the distance between positions increases."}, {"color": "#873a90", "description": "The Transformer is an innovative model that avoids recurrence and relies solely on attention mechanisms for efficient computation without sequential constraints.", "entity_id": "Transformer Model Architecture ", "entity_type": "organization", "file_path": "unknown_source", "id": "Transformer Model Architecture ", "label": "Transformer Model Architecture ", "shape": "dot", "size": 10, "source_id": "chunk-2105e0fac066cb4bca789ad1d39e48a6", "title": "The Transformer is an innovative model that avoids recurrence and relies solely on attention mechanisms for efficient computation without sequential constraints."}, {"color": "#6d667b", "description": "Self-attention allows the model to weigh different parts of a sequence based on their relevance, which is crucial for tasks like reading comprehension and summarization.", "entity_id": "Self-Attention Mechanism [4, 27, 28, 22] ", "entity_type": "category", "file_path": "unknown_source", "id": "Self-Attention Mechanism [4, 27, 28, 22] ", "label": "Self-Attention Mechanism [4, 27, 28, 22] ", "shape": "dot", "size": 10, "source_id": "chunk-2105e0fac066cb4bca789ad1d39e48a6", "title": "Self-attention allows the model to weigh different parts of a sequence based on their relevance, which is crucial for tasks like reading comprehension and summarization."}, {"color": "#7b9c33", "description": "These networks use an attention mechanism instead of recurrence and have shown success in simple language question answering and modeling tasks.", "entity_id": "End-to-End Memory Networks [34] ", "entity_type": "organization", "file_path": "unknown_source", "id": "End-to-End Memory Networks [34] ", "label": "End-to-End Memory Networks [34] ", "shape": "dot", "size": 10, "source_id": "chunk-2105e0fac066cb4bca789ad1d39e48a6", "title": "These networks use an attention mechanism instead of recurrence and have shown success in simple language question answering and modeling tasks."}, {"color": "#81fe5a", "description": "These approaches refer to the current leading methods in sequence modeling and transduction problems such as language modeling and machine translation.", "entity_id": "State of the Art Approaches", "entity_type": "category", "file_path": "unknown_source", "id": "State of the Art Approaches", "label": "State of the Art Approaches", "shape": "dot", "size": 10, "source_id": "chunk-2105e0fac066cb4bca789ad1d39e48a6", "title": "These approaches refer to the current leading methods in sequence modeling and transduction problems such as language modeling and machine translation."}]);
                  edges = new vis.DataSet([{"description": "Self-attention is a more advanced form of computation that extends the capabilities of recurrent models by allowing them to focus on different parts of the sequence.", "file_path": "unknown_source", "from": "Recurrent Neural Networks", "keywords": "recurrence vs. attention, model evolution", "source_id": "chunk-2105e0fac066cb4bca789ad1d39e48a6", "title": "Self-attention is a more advanced form of computation that extends the capabilities of recurrent models by allowing them to focus on different parts of the sequence.", "to": "Self-Attention Mechanism", "width": 6.0}, {"description": "Self-attention is a more advanced form of computation that extends the capabilities of recurrent models by allowing them to focus on different parts of the sequence.", "file_path": "unknown_source", "from": "Recurrent Neural Networks", "keywords": "recurrence vs. attention, model evolution", "source_id": "chunk-2105e0fac066cb4bca789ad1d39e48a6", "title": "Self-attention is a more advanced form of computation that extends the capabilities of recurrent models by allowing them to focus on different parts of the sequence.", "to": "Attention Mechanisms", "width": 6.0}, {"description": "While the Transformer avoids sequence recurrence, some techniques like factorization tricks are still used to optimize computation efficiency in other models.", "file_path": "unknown_source", "from": "Factorization Tricks [21] ", "keywords": "computational optimization, alternative approaches", "source_id": "chunk-2105e0fac066cb4bca789ad1d39e48a6", "title": "While the Transformer avoids sequence recurrence, some techniques like factorization tricks are still used to optimize computation efficiency in other models.", "to": "Transformer Model Architecture", "width": 10.0}, {"description": "The concept of conditional computation might indirectly influence the design choices for the Transformer, but it is not explicitly utilized there.", "file_path": "unknown_source", "from": "Conditional Computation [32] ", "keywords": "conditional computation, indirect influence", "source_id": "chunk-2105e0fac066cb4bca789ad1d39e48a6", "title": "The concept of conditional computation might indirectly influence the design choices for the Transformer, but it is not explicitly utilized there.", "to": "Transformer Model Architecture", "width": 8.0}, {"description": "The Transformer relies entirely on self-attention for computing input and output representations without using sequential recurrence.", "file_path": "unknown_source", "from": "Self-Attention Mechanism", "keywords": "architecture innovation, attention mechanism", "source_id": "chunk-2105e0fac066cb4bca789ad1d39e48a6", "title": "The Transformer relies entirely on self-attention for computing input and output representations without using sequential recurrence.", "to": "Transformer Model Architecture", "width": 18.0}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {
    "configure": {
        "enabled": false
    },
    "edges": {
        "color": {
            "inherit": true
        },
        "smooth": {
            "enabled": true,
            "type": "dynamic"
        }
    },
    "interaction": {
        "dragNodes": true,
        "hideEdgesOnDrag": false,
        "hideNodesOnDrag": false
    },
    "physics": {
        "enabled": true,
        "stabilization": {
            "enabled": true,
            "fit": true,
            "iterations": 1000,
            "onlyDynamicEdges": false,
            "updateInterval": 50
        }
    }
};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>